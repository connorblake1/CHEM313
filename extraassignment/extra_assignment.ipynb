{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Assignment: Transformers and SSMs\n",
    "Connor Blake, 5/23/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers, processors\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optax\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Transfomer\n",
    "Explain, in as much detail as possible, how a Transformer works from the paper [Attention is All You Need](https://arxiv.org/pdf/1706.03762). Be explicit in the use of function signatures, tensor dimensionalities, function definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 1:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1 Notation and Shapes  \n",
    "\n",
    "* B — batch size  \n",
    "* T — source-sequence length  \n",
    "* T′ — target-sequence length  \n",
    "* H — number of attention heads  \n",
    "* $d_{\\text{model}}$ — model width  \n",
    "* $d_k = d_{\\text{model}} ⁄ H$ — key/query sub-dimension  \n",
    "* $d_v = d_{\\text{model}} ⁄ H$ — value sub-dimension  \n",
    "* $d_{FF} ≈ 4 d_{\\text{model}}$ — hidden width of the feed-forward layer  \n",
    "\n",
    "Indices  \n",
    "\n",
    "* b ∈ {0,…,B−1} — sequence in batch  \n",
    "* t ∈ {0,…,T−1} — source token position  \n",
    "* t′ ∈ {0,…,T′−1} — target token position  \n",
    "* h ∈ {0,…,H−1} — head index  \n",
    "* m ∈ {0,…,d_{\\text{model}}−1} — model feature  \n",
    "* i ∈ {0,…,d_k−1} — key/query feature  \n",
    "* j ∈ {0,…,d_v−1} — value feature  \n",
    "* r ∈ {0,…,$d_{\\text{FF}}$}-1$ — feed-forward feature  \n",
    "\n",
    "A tensor element $X_{b t m}$ contains feature $m$ of token $t$ in sequence $b$.\n",
    "\n",
    "### 2 Primitive Operations  \n",
    "\n",
    "#### 2.1 Softmax  \n",
    "\n",
    "$$\n",
    "\\text{softmax}(Z)_{b n} = \\frac{e^{Z_{b n}}}{\\sum_{n′} e^{Z_{b n′}}}, \\quad\n",
    "Z \\in \\mathbb R^{B \\times N}.\n",
    "$$\n",
    "\n",
    "#### 2.2 Layer Normalization  \n",
    "\n",
    "Given $X_{b t m} ∈ ℝ^{B×T×d}  $\n",
    "\n",
    "$$\n",
    "\\mu_{b t} = \\frac{1}{d} \\sum_{m} X_{b t m}, \\qquad\n",
    "\\sigma_{b t} = \\sqrt{\\frac{1}{d} \\sum_{m} (X_{b t m} - \\mu_{b t})^{2} + \\varepsilon},\n",
    "$$\n",
    "$$\n",
    "\\text{LayerNorm}(X)_{b t m} =\n",
    "\\frac{X_{b t m} - \\mu_{b t}}{\\sigma_{b t}} \\, \\gamma_{m} + \\beta_{m},\n",
    "$$\n",
    "\n",
    "with learned gain $γ_{m}$ and bias $β_{m}$.\n",
    "\n",
    "### 3 Scaled Dot-Product Attention  \n",
    "\n",
    "#### 3.1 Linear projections  \n",
    "\n",
    "Learn weight matrices  \n",
    "\n",
    "* $W^{Q}_{m i} ∈ ℝ^{d_{\\text{model}}×d_k}$  \n",
    "* $W^{K}_{m i} ∈ ℝ^{d_{\\text{model}}×d_k}  $\n",
    "* $W^{V}_{m j} ∈ ℝ^{d_{\\text{model}}×d_v}$\n",
    "\n",
    "and form  \n",
    "\n",
    "$$\n",
    "Q_{b t i} = X_{b t m} \\, W^{Q}_{m i}, \\quad\n",
    "K_{b t i} = X_{b t m} \\, W^{K}_{m i}, \\quad\n",
    "V_{b t j} = X_{b t m} \\, W^{V}_{m j}.\n",
    "$$\n",
    "\n",
    "Split the last index into H heads:  \n",
    "$Q_{b t h i}, K_{b t h i} ∈ ℝ^{B×T×H×d_k}; V_{b t h j} ∈ ℝ^{B×T×H×d_v}$.\n",
    "\n",
    "#### 3.2 Attention kernel and weights  \n",
    "\n",
    "$$\n",
    "S_{b h t_q t_k} = \\frac{1}{\\sqrt{d_k}}\n",
    "\\, Q_{b t_q h i} \\, K_{b t_k h i}.\n",
    "$$\n",
    "\n",
    "For decoder self-attention, entries with $t_k > t_q$ are set to $-\\infty$.\n",
    "\n",
    "$$\n",
    "\\alpha_{b h t_q t_k} = \\text{softmax}_{t_k} \\bigl( S_{b h t_q t_k} \\bigr).\n",
    "$$\n",
    "\n",
    "#### 3.3 Head output and combination  \n",
    "\n",
    "$$\n",
    "Z_{b t_q h j} = \\alpha_{b h t_q t_k} \\, V_{b t_k h j}.\n",
    "$$\n",
    "\n",
    "Concatenate heads as $Z_{b t n}$ with $n = h d_v + j$ and project\n",
    "\n",
    "$$\n",
    "\\tilde Z_{b t m} = Z_{b t n} \\, W^{O}_{n m},\n",
    "\\qquad W^{O} \\in \\mathbb R^{H d_v \\times d_{\\text{model}}}.\n",
    "$$\n",
    "\n",
    "\n",
    "### 4 Position-wise Feed-Forward Network  \n",
    "\n",
    "$$\n",
    "\\tilde X_{b t r} = \\max \\bigl( 0, \\, X_{b t m} W^{(1)}_{m r} + b^{(1)}_{r} \\bigr),\n",
    "$$\n",
    "$$\n",
    "Y_{b t m} = \\tilde X_{b t r} \\, W^{(2)}_{r m} + b^{(2)}_{m},\n",
    "\\qquad\n",
    "W^{(1)} \\in \\mathbb R^{d_{\\text{model}} \\times d_{ff}},\n",
    "\\; W^{(2)} \\in \\mathbb R^{d_{ff} \\times d_{\\text{model}}}.\n",
    "$$\n",
    "\n",
    "\n",
    "### 5 Positional Encoding  \n",
    "\n",
    "For position $t$ and feature $m$,\n",
    "\n",
    "$$\n",
    "\\text{PE}_{t m} =\n",
    "\\begin{cases}\n",
    "\\sin \\bigl( t / 10000^{\\,2m / d_{\\text{model}}} \\bigr) & m \\text{ even}, \\\\[6pt]\n",
    "\\cos \\bigl( t / 10000^{\\,2m / d_{\\text{model}}} \\bigr) & m \\text{ odd}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Add $PE_{t m}$ to token embeddings $E_{t m}$.\n",
    "\n",
    "### 6 Encoder Layer  \n",
    "\n",
    "With input $X^{(l)}_{b t m}$\n",
    "\n",
    "1. Multi-head self-attention  \n",
    "   $$ \\hat Z_{b t m} = \\text{MHA} \\bigl( \\text{LayerNorm}(X^{(l)}) \\bigr)_{b t m}. $$\n",
    "2. Residual add $Z_{b t m} = X^{(l)}_{b t m} + \\hat Z_{b t m}$.  \n",
    "3. Feed-forward  \n",
    "   $$ \\hat Y_{b t m} = \\text{FFN} \\bigl( \\text{LayerNorm}(Z) \\bigr)_{b t m}. $$\n",
    "4. Residual add $X^{(l+1)}_{b t m} = Z_{b t m} + \\hat Y_{b t m}$.\n",
    "\n",
    "Repeat for $l = 0,…,L_E−1$; final output is memory $M_{b t m}$.\n",
    "\n",
    "\n",
    "### 7 Decoder Layer  \n",
    "\n",
    "With decoder input $Y^{(l)}_{b t′ m}$ and memory $M_{b t m}$\n",
    "\n",
    "1. Masked self-attention  \n",
    "   $$ \\hat U = \\text{MHA}_{\\text{masked}} \\bigl( \\text{LayerNorm}(Y^{(l)}) \\bigr). $$\n",
    "2. $U = Y^{(l)} + \\hat U$.  \n",
    "3. Cross-attention  \n",
    "   $$ \\hat V = \\text{MHA} \\bigl( Q = \\text{LayerNorm}(U),\\, K = M,\\, V = M \\bigr). $$\n",
    "4. $V = U + \\hat V$.  \n",
    "5. Feed-forward  \n",
    "   $$ \\hat Y = \\text{FFN} \\bigl( \\text{LayerNorm}(V) \\bigr). $$\n",
    "6. $Y^{(l+1)} = V + \\hat Y$.\n",
    "\n",
    "Repeat for $l = 0,\\ldots,L_D−1$.\n",
    "\n",
    "### 8 End-to-End Forward Pass  \n",
    "\n",
    "1. Encode the source to obtain memory $M_{b t m}$.  \n",
    "2. Decode shifted-right targets with causal masking while attending to $M_{b t m}$.  \n",
    "3. Project decoder states with $W^{\\text{vocab}}_{m v}$ and apply softmax to obtain  \n",
    "   $$ p(y_{t′} \\mid y_{<t′}, x). $$\n",
    "\n",
    "\n",
    "### 9 Training Objective  \n",
    "\n",
    "$$\n",
    "\\mathcal L = -\\sum_{b,\\,t′} \\log p \\bigl( y_{t′}^{(b)} \\mid y_{<t′}^{(b)},\\, x^{(b)} \\bigr),\n",
    "$$\n",
    "\n",
    "minimised with Adam using learning-rate warm-up and inverse-square-root decay.\n",
    "\n",
    "### 10 Parameter and Complexity Summary  \n",
    "\n",
    "| sub-unit | parameters | time per layer | space per layer |\n",
    "|----------|------------|----------------|-----------------|\n",
    "| multi-head attention | $3 d_{\\text{model}} d_k H + d_{\\text{model}} d_v H$ | $O(B T^{2} d_k)$ | $O(B T d_{\\text{model}})$ |\n",
    "| feed-forward | $2 d_{\\text{model}} d_{\\text{FF}}$ | $O(B T d_{\\text{FF}})$ | $O(B T d_{\\text{FF}})$ |\n",
    "| layer norm | $2 d_{\\text{model}}$ | negligible | negligible |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 ChemBERTa:\n",
    "This problem is based on the [ChemBERTa](https://arxiv.org/pdf/2010.09885) paper.\n",
    "### a)\n",
    "Load the file `250k_rndm_zinc_drugs_clean_3.csv` and set up a word-level tokenizer with a byte-level pre tokenizer. Explain what these mean and why they are needed.\n",
    "### b)\n",
    "Using this tokenizer, pick a nontrivial SMILES string. Print out the string, the tokenization, and use rdkit to display it.\n",
    "### c)\n",
    "Set up a train/test loader using BERT-style masking. Explain why there must be collation.\n",
    "### d)\n",
    "Set up and define a Transformer class, a Masked Transformer class, masking function and optimizers. Explain why a Masked Transformer is needed. Choose a loss function and justify it.\n",
    "### e) \n",
    "Train the BERT-style model for a number of epochs until the loss is consistently less than ~3. Use the model to predict a previously-unseen SMILES string, and print out both the model, the masked, and the predicted. Qualitatively describe what's happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_csv('250k_rndm_zinc_drugs_clean_3.csv')\n",
    "df = df_full.iloc[:10000]\n",
    "smiles = df['smiles'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.WordLevel(unk_token='[UNK]'))\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "trainer = trainers.WordLevelTrainer(vocab_size=1000,\n",
    "    special_tokens=['[PAD]','[UNK]','[CLS]','[SEP]','[MASK]'])\n",
    "tokenizer.train_from_iterator(smiles, trainer=trainer)\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    '[CLS] $A [SEP]', special_tokens=[('[CLS]',1),('[SEP]',2)])\n",
    "tokenizer.enable_truncation(max_length=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Byte-level guarantees that every character is representable. Word-level allows the model to parse more complex strings like \"Cl\",\"=O\" as a single item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cc1occc1C(=O)NC1CCN(C(=O)C(=O)Nc2ccc(F)cc2F)CC1\n",
      "\n",
      "['[CLS]', 'ĠCc', '1', 'occc', '1', 'C', '(=', 'O', ')', 'NC', '1', 'CCN', '(', 'C', '(=', 'O', ')', 'C', '(=', 'O', ')', 'Nc', '2', 'ccc', '(', 'F', ')', 'cc', '2', 'F', ')', 'CC', '1', 'Ċ', '[SEP]']\n",
      "[1, 32, 7, 216, 7, 6, 12, 10, 5, 29, 7, 52, 8, 6, 12, 10, 5, 6, 12, 10, 5, 35, 9, 18, 8, 26, 5, 17, 9, 26, 5, 23, 7, 13, 2]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=300x300>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles0 = smiles[2100]\n",
    "print(smiles0)\n",
    "encoding0 = tokenizer.encode(smiles0)\n",
    "print(encoding0.tokens)\n",
    "print(encoding0.ids)\n",
    "print(encoding0.attention_mask)\n",
    "mol=Chem.MolFromSmiles(smiles0)\n",
    "Draw.MolToImage(mol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = {'input_ids':[], 'attention_mask':[]}\n",
    "for e in tokenizer.encode_batch(smiles): # tokenizes each\n",
    "    enc['input_ids'].append(e.ids)\n",
    "    enc['attention_mask'].append(e.attention_mask)\n",
    "ids = enc['input_ids']\n",
    "masks = enc['attention_mask']\n",
    "# train test split\n",
    "ids_train, ids_test, masks_train, masks_test = train_test_split(\n",
    "    ids, masks, test_size=0.1, random_state=42\n",
    ")\n",
    "# packs into dicts\n",
    "train_enc = {'input_ids': ids_train, 'attention_mask': masks_train}\n",
    "test_enc  = {'input_ids': ids_test,  'attention_mask': masks_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset custom class\n",
    "class SMILESDataset(Dataset):\n",
    "    def __init__(self, enc):\n",
    "        self.ids = enc['input_ids']\n",
    "        self.mask = enc['attention_mask']\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.ids[i], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.mask[i], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # this must exist to pad them all to the same length so they can be stacked and passed into transfomer\n",
    "    ids = [item['input_ids'] for item in batch]\n",
    "    masks = [item['attention_mask'] for item in batch]\n",
    "    pad_id = tokenizer.token_to_id('[PAD]')\n",
    "    ids_padded = pad_sequence(ids, batch_first=True, padding_value=pad_id)\n",
    "    masks_padded = pad_sequence(masks, batch_first=True, padding_value=0)\n",
    "    return {'input_ids': ids_padded, 'attention_mask': masks_padded}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all SMILES strings are the same length, so they must be padded to use standord uniform size tensor frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    SMILESDataset(train_enc),\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    SMILESDataset(test_enc),\n",
    "    batch_size=32,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, mlp_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(d_model, num_heads)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(nn.Linear(d_model, mlp_dim),\n",
    "                                 nn.GELU(),\n",
    "                                 nn.Linear(mlp_dim, d_model))\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "    def forward(self, x, mask):\n",
    "        attn_out, _ = self.attn(x, x, x, key_padding_mask=mask==0)\n",
    "        x = x + attn_out\n",
    "        y = self.ln1(x)\n",
    "        x = x + self.mlp(y)\n",
    "        return self.ln2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=64, num_heads=4,\n",
    "                 mlp_dim=128, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, mlp_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "    def forward(self, input_ids, mask):\n",
    "        x = self.embed(input_ids).transpose(0, 1)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.fc(x).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention mask is a way to tell the Transformer which positions in the input are “real” tokens and which are just padding or otherwise shouldn’t contribute to attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_inputs(ids, mask_token_id, mask_prob=0.15):\n",
    "    labels = ids.clone()\n",
    "    rand = torch.rand(ids.shape)\n",
    "    mask_positions = rand < mask_prob\n",
    "    labels[~mask_positions] = -100\n",
    "    ids[mask_positions] = mask_token_id\n",
    "    return ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.get_vocab_size()\n",
    "model = MaskedTransformer(vocab_size)\n",
    "opt = optim.Adam(model.parameters(), lr=1e-4)\n",
    "crit = nn.CrossEntropyLoss(ignore_index=-100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross entropy is a good choice for multiclass classification where there are probabilistic predictions. It is always positive and is uniquely minimized when fully confident correct answers are given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t batch, loss = 6.604\n",
      "\t batch, loss = 6.601\n",
      "\t batch, loss = 6.576\n",
      "\t batch, loss = 6.525\n",
      "\t batch, loss = 6.443\n",
      "\t batch, loss = 6.402\n",
      "\t batch, loss = 6.405\n",
      "\t batch, loss = 6.319\n",
      "\t batch, loss = 6.276\n",
      "\t batch, loss = 6.280\n",
      "\t batch, loss = 6.198\n",
      "\t batch, loss = 6.273\n",
      "\t batch, loss = 6.195\n",
      "\t batch, loss = 6.073\n",
      "\t batch, loss = 6.038\n",
      "\t batch, loss = 6.067\n",
      "\t batch, loss = 6.057\n",
      "\t batch, loss = 5.842\n",
      "\t batch, loss = 6.023\n",
      "\t batch, loss = 5.902\n",
      "\t batch, loss = 5.951\n",
      "\t batch, loss = 5.935\n",
      "\t batch, loss = 5.783\n",
      "\t batch, loss = 5.842\n",
      "\t batch, loss = 5.779\n",
      "\t batch, loss = 5.724\n",
      "\t batch, loss = 5.604\n",
      "\t batch, loss = 5.780\n",
      "\t batch, loss = 5.871\n",
      "\t batch, loss = 5.274\n",
      "\t batch, loss = 5.747\n",
      "\t batch, loss = 5.668\n",
      "\t batch, loss = 5.705\n",
      "\t batch, loss = 5.406\n",
      "\t batch, loss = 5.473\n",
      "\t batch, loss = 5.162\n",
      "\t batch, loss = 5.528\n",
      "\t batch, loss = 5.342\n",
      "\t batch, loss = 5.199\n",
      "\t batch, loss = 5.176\n",
      "\t batch, loss = 5.117\n",
      "\t batch, loss = 5.092\n",
      "\t batch, loss = 5.262\n",
      "\t batch, loss = 5.274\n",
      "\t batch, loss = 4.990\n",
      "\t batch, loss = 5.146\n",
      "\t batch, loss = 5.214\n",
      "\t batch, loss = 5.312\n",
      "\t batch, loss = 5.056\n",
      "\t batch, loss = 5.127\n",
      "\t batch, loss = 5.001\n",
      "\t batch, loss = 4.616\n",
      "\t batch, loss = 5.041\n",
      "\t batch, loss = 5.214\n",
      "\t batch, loss = 4.577\n",
      "\t batch, loss = 5.048\n",
      "\t batch, loss = 4.845\n",
      "\t batch, loss = 4.953\n",
      "\t batch, loss = 4.799\n",
      "\t batch, loss = 4.860\n",
      "\t batch, loss = 4.992\n",
      "\t batch, loss = 5.318\n",
      "\t batch, loss = 4.656\n",
      "\t batch, loss = 4.630\n",
      "\t batch, loss = 4.762\n",
      "\t batch, loss = 5.016\n",
      "\t batch, loss = 4.624\n",
      "\t batch, loss = 4.916\n",
      "\t batch, loss = 4.805\n",
      "\t batch, loss = 4.709\n",
      "\t batch, loss = 5.011\n",
      "\t batch, loss = 4.909\n",
      "\t batch, loss = 4.870\n",
      "\t batch, loss = 4.330\n",
      "\t batch, loss = 4.752\n",
      "\t batch, loss = 4.655\n",
      "\t batch, loss = 4.902\n",
      "\t batch, loss = 4.485\n",
      "\t batch, loss = 4.549\n",
      "\t batch, loss = 4.691\n",
      "\t batch, loss = 4.504\n",
      "\t batch, loss = 4.165\n",
      "\t batch, loss = 4.658\n",
      "\t batch, loss = 4.715\n",
      "\t batch, loss = 4.733\n",
      "\t batch, loss = 4.026\n",
      "\t batch, loss = 4.382\n",
      "\t batch, loss = 4.648\n",
      "\t batch, loss = 4.486\n",
      "\t batch, loss = 4.755\n",
      "\t batch, loss = 4.630\n",
      "\t batch, loss = 4.444\n",
      "\t batch, loss = 4.359\n",
      "\t batch, loss = 4.754\n",
      "\t batch, loss = 4.488\n",
      "\t batch, loss = 4.317\n",
      "\t batch, loss = 4.205\n",
      "\t batch, loss = 3.910\n",
      "\t batch, loss = 4.287\n",
      "\t batch, loss = 4.274\n",
      "\t batch, loss = 3.834\n",
      "\t batch, loss = 4.549\n",
      "\t batch, loss = 4.787\n",
      "\t batch, loss = 3.986\n",
      "\t batch, loss = 4.475\n",
      "\t batch, loss = 4.234\n",
      "\t batch, loss = 4.360\n",
      "\t batch, loss = 4.321\n",
      "\t batch, loss = 4.073\n",
      "\t batch, loss = 4.631\n",
      "\t batch, loss = 4.048\n",
      "\t batch, loss = 3.811\n",
      "\t batch, loss = 4.227\n",
      "\t batch, loss = 3.845\n",
      "\t batch, loss = 4.297\n",
      "\t batch, loss = 3.883\n",
      "\t batch, loss = 4.829\n",
      "\t batch, loss = 4.476\n",
      "\t batch, loss = 4.457\n",
      "\t batch, loss = 4.051\n",
      "\t batch, loss = 4.268\n",
      "\t batch, loss = 4.430\n",
      "\t batch, loss = 4.822\n",
      "\t batch, loss = 4.004\n",
      "\t batch, loss = 4.542\n",
      "\t batch, loss = 4.298\n",
      "\t batch, loss = 4.391\n",
      "\t batch, loss = 4.252\n",
      "\t batch, loss = 3.960\n",
      "\t batch, loss = 4.033\n",
      "\t batch, loss = 4.474\n",
      "\t batch, loss = 4.094\n",
      "\t batch, loss = 3.729\n",
      "\t batch, loss = 4.068\n",
      "\t batch, loss = 4.039\n",
      "\t batch, loss = 4.323\n",
      "\t batch, loss = 4.139\n",
      "\t batch, loss = 4.279\n",
      "\t batch, loss = 4.067\n",
      "\t batch, loss = 3.983\n",
      "\t batch, loss = 4.479\n",
      "\t batch, loss = 4.141\n",
      "\t batch, loss = 4.357\n",
      "\t batch, loss = 4.076\n",
      "\t batch, loss = 4.072\n",
      "\t batch, loss = 4.224\n",
      "\t batch, loss = 4.062\n",
      "\t batch, loss = 4.330\n",
      "\t batch, loss = 4.110\n",
      "\t batch, loss = 3.748\n",
      "\t batch, loss = 4.145\n",
      "\t batch, loss = 3.495\n",
      "\t batch, loss = 3.766\n",
      "\t batch, loss = 4.099\n",
      "\t batch, loss = 4.057\n",
      "\t batch, loss = 4.167\n",
      "\t batch, loss = 3.787\n",
      "\t batch, loss = 3.920\n",
      "\t batch, loss = 4.073\n",
      "\t batch, loss = 3.835\n",
      "\t batch, loss = 4.484\n",
      "\t batch, loss = 3.972\n",
      "\t batch, loss = 4.266\n",
      "\t batch, loss = 4.050\n",
      "\t batch, loss = 4.026\n",
      "\t batch, loss = 3.670\n",
      "\t batch, loss = 3.877\n",
      "\t batch, loss = 3.925\n",
      "\t batch, loss = 3.723\n",
      "\t batch, loss = 4.126\n",
      "\t batch, loss = 3.871\n",
      "\t batch, loss = 4.215\n",
      "\t batch, loss = 4.043\n",
      "\t batch, loss = 4.133\n",
      "\t batch, loss = 4.179\n",
      "\t batch, loss = 3.612\n",
      "\t batch, loss = 4.017\n",
      "\t batch, loss = 3.404\n",
      "\t batch, loss = 3.341\n",
      "\t batch, loss = 3.928\n",
      "\t batch, loss = 4.322\n",
      "\t batch, loss = 3.908\n",
      "\t batch, loss = 4.110\n",
      "\t batch, loss = 3.832\n",
      "\t batch, loss = 3.972\n",
      "\t batch, loss = 3.743\n",
      "\t batch, loss = 3.628\n",
      "\t batch, loss = 3.586\n",
      "\t batch, loss = 3.949\n",
      "\t batch, loss = 3.602\n",
      "\t batch, loss = 4.006\n",
      "\t batch, loss = 3.628\n",
      "\t batch, loss = 3.890\n",
      "\t batch, loss = 3.819\n",
      "\t batch, loss = 3.579\n",
      "\t batch, loss = 3.760\n",
      "\t batch, loss = 3.589\n",
      "\t batch, loss = 3.876\n",
      "\t batch, loss = 3.792\n",
      "\t batch, loss = 3.886\n",
      "\t batch, loss = 3.994\n",
      "\t batch, loss = 3.961\n",
      "\t batch, loss = 3.995\n",
      "\t batch, loss = 4.270\n",
      "\t batch, loss = 3.681\n",
      "\t batch, loss = 3.888\n",
      "\t batch, loss = 3.856\n",
      "\t batch, loss = 3.703\n",
      "\t batch, loss = 3.927\n",
      "\t batch, loss = 3.841\n",
      "\t batch, loss = 4.036\n",
      "\t batch, loss = 4.043\n",
      "\t batch, loss = 3.548\n",
      "\t batch, loss = 3.761\n",
      "\t batch, loss = 3.602\n",
      "\t batch, loss = 3.636\n",
      "\t batch, loss = 3.862\n",
      "\t batch, loss = 3.872\n",
      "\t batch, loss = 3.695\n",
      "\t batch, loss = 3.977\n",
      "\t batch, loss = 3.293\n",
      "\t batch, loss = 4.106\n",
      "\t batch, loss = 3.497\n",
      "\t batch, loss = 3.548\n",
      "\t batch, loss = 3.366\n",
      "\t batch, loss = 3.917\n",
      "\t batch, loss = 3.441\n",
      "\t batch, loss = 3.926\n",
      "\t batch, loss = 3.326\n",
      "\t batch, loss = 3.822\n",
      "\t batch, loss = 3.283\n",
      "\t batch, loss = 3.372\n",
      "\t batch, loss = 3.665\n",
      "\t batch, loss = 3.792\n",
      "\t batch, loss = 3.882\n",
      "\t batch, loss = 4.001\n",
      "\t batch, loss = 3.464\n",
      "\t batch, loss = 3.853\n",
      "\t batch, loss = 3.784\n",
      "\t batch, loss = 3.477\n",
      "\t batch, loss = 3.523\n",
      "\t batch, loss = 3.717\n",
      "\t batch, loss = 3.809\n",
      "\t batch, loss = 3.571\n",
      "\t batch, loss = 3.695\n",
      "\t batch, loss = 3.796\n",
      "\t batch, loss = 3.401\n",
      "\t batch, loss = 3.475\n",
      "\t batch, loss = 3.550\n",
      "\t batch, loss = 3.387\n",
      "\t batch, loss = 3.714\n",
      "\t batch, loss = 3.492\n",
      "\t batch, loss = 3.598\n",
      "\t batch, loss = 3.607\n",
      "\t batch, loss = 3.607\n",
      "\t batch, loss = 3.683\n",
      "\t batch, loss = 3.439\n",
      "\t batch, loss = 3.043\n",
      "\t batch, loss = 3.303\n",
      "\t batch, loss = 3.687\n",
      "\t batch, loss = 3.390\n",
      "\t batch, loss = 3.669\n",
      "\t batch, loss = 3.077\n",
      "\t batch, loss = 3.608\n",
      "\t batch, loss = 3.750\n",
      "\t batch, loss = 3.466\n",
      "\t batch, loss = 3.395\n",
      "\t batch, loss = 3.795\n",
      "\t batch, loss = 3.293\n",
      "\t batch, loss = 3.587\n",
      "\t batch, loss = 3.707\n",
      "\t batch, loss = 3.763\n",
      "\t batch, loss = 3.615\n",
      "\t batch, loss = 3.904\n",
      "\t batch, loss = 3.546\n",
      "\t batch, loss = 3.570\n",
      "\t batch, loss = 3.216\n",
      "\t batch, loss = 3.668\n",
      "\t batch, loss = 3.435\n",
      "\t batch, loss = 3.613\n",
      "\t batch, loss = 3.310\n",
      "\t batch, loss = 3.603\n",
      "Epoch = 0\n",
      "\t batch, loss = 3.479\n",
      "\t batch, loss = 3.473\n",
      "\t batch, loss = 3.568\n",
      "\t batch, loss = 3.318\n",
      "\t batch, loss = 3.149\n",
      "\t batch, loss = 4.068\n",
      "\t batch, loss = 3.534\n",
      "\t batch, loss = 3.147\n",
      "\t batch, loss = 3.336\n",
      "\t batch, loss = 3.504\n",
      "\t batch, loss = 3.542\n",
      "\t batch, loss = 3.478\n",
      "\t batch, loss = 3.229\n",
      "\t batch, loss = 3.586\n",
      "\t batch, loss = 3.525\n",
      "\t batch, loss = 3.216\n",
      "\t batch, loss = 3.169\n",
      "\t batch, loss = 3.692\n",
      "\t batch, loss = 3.695\n",
      "\t batch, loss = 3.428\n",
      "\t batch, loss = 3.666\n",
      "\t batch, loss = 3.810\n",
      "\t batch, loss = 3.483\n",
      "\t batch, loss = 3.522\n",
      "\t batch, loss = 3.570\n",
      "\t batch, loss = 3.019\n",
      "\t batch, loss = 3.423\n",
      "\t batch, loss = 3.929\n",
      "\t batch, loss = 3.256\n",
      "\t batch, loss = 3.519\n",
      "\t batch, loss = 3.359\n",
      "\t batch, loss = 3.035\n",
      "\t batch, loss = 3.099\n",
      "\t batch, loss = 2.976\n",
      "\t batch, loss = 3.626\n",
      "\t batch, loss = 3.793\n",
      "\t batch, loss = 3.133\n",
      "\t batch, loss = 3.149\n",
      "\t batch, loss = 3.425\n",
      "\t batch, loss = 3.351\n",
      "\t batch, loss = 3.558\n",
      "\t batch, loss = 3.383\n",
      "\t batch, loss = 3.259\n",
      "\t batch, loss = 3.653\n",
      "\t batch, loss = 3.095\n",
      "\t batch, loss = 3.385\n",
      "\t batch, loss = 3.555\n",
      "\t batch, loss = 3.708\n",
      "\t batch, loss = 3.212\n",
      "\t batch, loss = 3.601\n",
      "\t batch, loss = 3.183\n",
      "\t batch, loss = 3.476\n",
      "\t batch, loss = 2.922\n",
      "\t batch, loss = 3.586\n",
      "\t batch, loss = 3.174\n",
      "\t batch, loss = 3.808\n",
      "\t batch, loss = 3.363\n",
      "\t batch, loss = 3.280\n",
      "\t batch, loss = 3.013\n",
      "\t batch, loss = 3.373\n",
      "\t batch, loss = 3.818\n",
      "\t batch, loss = 3.159\n",
      "\t batch, loss = 3.426\n",
      "\t batch, loss = 3.368\n",
      "\t batch, loss = 3.224\n",
      "\t batch, loss = 3.125\n",
      "\t batch, loss = 3.174\n",
      "\t batch, loss = 3.287\n",
      "\t batch, loss = 3.326\n",
      "\t batch, loss = 3.356\n",
      "\t batch, loss = 3.380\n",
      "\t batch, loss = 3.329\n",
      "\t batch, loss = 3.604\n",
      "\t batch, loss = 3.611\n",
      "\t batch, loss = 3.178\n",
      "\t batch, loss = 3.093\n",
      "\t batch, loss = 3.513\n",
      "\t batch, loss = 3.618\n",
      "\t batch, loss = 3.231\n",
      "\t batch, loss = 3.586\n",
      "\t batch, loss = 3.696\n",
      "\t batch, loss = 3.120\n",
      "\t batch, loss = 3.244\n",
      "\t batch, loss = 3.036\n",
      "\t batch, loss = 3.729\n",
      "\t batch, loss = 3.310\n",
      "\t batch, loss = 3.425\n",
      "\t batch, loss = 2.865\n",
      "\t batch, loss = 3.442\n",
      "\t batch, loss = 3.318\n",
      "\t batch, loss = 3.607\n",
      "\t batch, loss = 2.966\n",
      "\t batch, loss = 3.282\n",
      "\t batch, loss = 3.470\n",
      "\t batch, loss = 2.996\n",
      "\t batch, loss = 3.443\n",
      "\t batch, loss = 3.350\n",
      "\t batch, loss = 3.278\n",
      "\t batch, loss = 3.263\n",
      "\t batch, loss = 3.376\n",
      "\t batch, loss = 3.293\n",
      "\t batch, loss = 3.190\n",
      "\t batch, loss = 3.153\n",
      "\t batch, loss = 2.976\n",
      "\t batch, loss = 3.393\n",
      "\t batch, loss = 3.676\n",
      "\t batch, loss = 3.581\n",
      "\t batch, loss = 3.396\n",
      "\t batch, loss = 3.561\n",
      "\t batch, loss = 3.384\n",
      "\t batch, loss = 2.930\n",
      "\t batch, loss = 3.666\n",
      "\t batch, loss = 3.268\n",
      "\t batch, loss = 3.034\n",
      "\t batch, loss = 3.280\n",
      "\t batch, loss = 2.953\n",
      "\t batch, loss = 3.346\n",
      "\t batch, loss = 3.380\n",
      "\t batch, loss = 3.531\n",
      "\t batch, loss = 3.229\n",
      "\t batch, loss = 3.540\n",
      "\t batch, loss = 3.332\n",
      "\t batch, loss = 3.381\n",
      "\t batch, loss = 3.624\n",
      "\t batch, loss = 3.085\n",
      "\t batch, loss = 3.281\n",
      "\t batch, loss = 2.966\n",
      "\t batch, loss = 3.432\n",
      "\t batch, loss = 3.081\n",
      "\t batch, loss = 3.412\n",
      "\t batch, loss = 3.267\n",
      "\t batch, loss = 3.560\n",
      "\t batch, loss = 3.343\n",
      "\t batch, loss = 3.458\n",
      "\t batch, loss = 3.124\n",
      "\t batch, loss = 3.492\n",
      "\t batch, loss = 3.385\n",
      "\t batch, loss = 3.089\n",
      "\t batch, loss = 2.959\n",
      "\t batch, loss = 3.124\n",
      "\t batch, loss = 3.410\n",
      "\t batch, loss = 3.366\n",
      "\t batch, loss = 2.755\n",
      "\t batch, loss = 3.460\n",
      "\t batch, loss = 3.194\n",
      "\t batch, loss = 2.884\n",
      "\t batch, loss = 3.665\n",
      "\t batch, loss = 3.048\n",
      "\t batch, loss = 3.501\n",
      "\t batch, loss = 3.225\n",
      "\t batch, loss = 3.319\n",
      "\t batch, loss = 3.215\n",
      "\t batch, loss = 3.429\n",
      "\t batch, loss = 3.044\n",
      "\t batch, loss = 3.172\n",
      "\t batch, loss = 3.122\n",
      "\t batch, loss = 3.353\n",
      "\t batch, loss = 2.841\n",
      "\t batch, loss = 3.033\n",
      "\t batch, loss = 3.281\n",
      "\t batch, loss = 2.958\n",
      "\t batch, loss = 3.283\n",
      "\t batch, loss = 2.985\n",
      "\t batch, loss = 3.098\n",
      "\t batch, loss = 3.144\n",
      "\t batch, loss = 3.043\n",
      "\t batch, loss = 3.234\n",
      "\t batch, loss = 3.053\n",
      "\t batch, loss = 3.073\n",
      "\t batch, loss = 3.144\n",
      "\t batch, loss = 3.045\n",
      "\t batch, loss = 2.651\n",
      "\t batch, loss = 2.995\n",
      "\t batch, loss = 3.142\n",
      "\t batch, loss = 3.303\n",
      "\t batch, loss = 3.259\n",
      "\t batch, loss = 2.884\n",
      "\t batch, loss = 3.251\n",
      "\t batch, loss = 3.101\n",
      "\t batch, loss = 3.029\n",
      "\t batch, loss = 3.168\n",
      "\t batch, loss = 2.884\n",
      "\t batch, loss = 3.168\n",
      "\t batch, loss = 3.630\n",
      "\t batch, loss = 3.306\n",
      "\t batch, loss = 3.206\n",
      "\t batch, loss = 3.287\n",
      "\t batch, loss = 3.057\n",
      "\t batch, loss = 2.959\n",
      "\t batch, loss = 3.200\n",
      "\t batch, loss = 3.530\n",
      "\t batch, loss = 2.764\n",
      "\t batch, loss = 2.873\n",
      "\t batch, loss = 3.286\n",
      "\t batch, loss = 3.044\n",
      "\t batch, loss = 2.982\n",
      "\t batch, loss = 3.671\n",
      "\t batch, loss = 3.314\n",
      "\t batch, loss = 3.193\n",
      "\t batch, loss = 3.301\n",
      "\t batch, loss = 3.474\n",
      "\t batch, loss = 3.235\n",
      "\t batch, loss = 3.248\n",
      "\t batch, loss = 3.340\n",
      "\t batch, loss = 3.310\n",
      "\t batch, loss = 3.034\n",
      "\t batch, loss = 3.573\n",
      "\t batch, loss = 2.992\n",
      "\t batch, loss = 3.159\n",
      "\t batch, loss = 3.217\n",
      "\t batch, loss = 3.159\n",
      "\t batch, loss = 2.862\n",
      "\t batch, loss = 2.902\n",
      "\t batch, loss = 3.267\n",
      "\t batch, loss = 3.140\n",
      "\t batch, loss = 2.888\n",
      "\t batch, loss = 3.238\n",
      "\t batch, loss = 3.610\n",
      "\t batch, loss = 3.255\n",
      "\t batch, loss = 2.865\n",
      "\t batch, loss = 3.065\n",
      "\t batch, loss = 3.077\n",
      "\t batch, loss = 3.297\n",
      "\t batch, loss = 3.606\n",
      "\t batch, loss = 3.285\n",
      "\t batch, loss = 3.543\n",
      "\t batch, loss = 2.960\n",
      "\t batch, loss = 2.580\n",
      "\t batch, loss = 3.304\n",
      "\t batch, loss = 2.831\n",
      "\t batch, loss = 3.257\n",
      "\t batch, loss = 3.421\n",
      "\t batch, loss = 3.083\n",
      "\t batch, loss = 3.336\n",
      "\t batch, loss = 3.540\n",
      "\t batch, loss = 3.250\n",
      "\t batch, loss = 3.090\n",
      "\t batch, loss = 3.552\n",
      "\t batch, loss = 2.994\n",
      "\t batch, loss = 3.153\n",
      "\t batch, loss = 3.169\n",
      "\t batch, loss = 3.229\n",
      "\t batch, loss = 3.326\n",
      "\t batch, loss = 3.071\n",
      "\t batch, loss = 3.335\n",
      "\t batch, loss = 3.322\n",
      "\t batch, loss = 3.478\n",
      "\t batch, loss = 2.777\n",
      "\t batch, loss = 3.011\n",
      "\t batch, loss = 3.342\n",
      "\t batch, loss = 3.286\n",
      "\t batch, loss = 2.902\n",
      "\t batch, loss = 2.631\n",
      "\t batch, loss = 2.978\n",
      "\t batch, loss = 3.251\n",
      "\t batch, loss = 3.203\n",
      "\t batch, loss = 3.095\n",
      "\t batch, loss = 3.278\n",
      "\t batch, loss = 3.397\n",
      "\t batch, loss = 3.270\n",
      "\t batch, loss = 3.157\n",
      "\t batch, loss = 3.081\n",
      "\t batch, loss = 2.999\n",
      "\t batch, loss = 3.040\n",
      "\t batch, loss = 3.281\n",
      "\t batch, loss = 3.145\n",
      "\t batch, loss = 2.996\n",
      "\t batch, loss = 2.913\n",
      "\t batch, loss = 3.039\n",
      "\t batch, loss = 3.122\n",
      "\t batch, loss = 3.147\n",
      "\t batch, loss = 3.508\n",
      "\t batch, loss = 3.118\n",
      "\t batch, loss = 2.950\n",
      "\t batch, loss = 3.246\n",
      "\t batch, loss = 3.009\n",
      "\t batch, loss = 2.995\n",
      "\t batch, loss = 3.146\n",
      "\t batch, loss = 3.132\n",
      "\t batch, loss = 3.190\n",
      "\t batch, loss = 3.149\n",
      "\t batch, loss = 3.427\n",
      "Epoch = 1\n",
      "\t batch, loss = 2.704\n",
      "\t batch, loss = 3.318\n",
      "\t batch, loss = 3.049\n",
      "\t batch, loss = 3.244\n",
      "\t batch, loss = 3.248\n",
      "\t batch, loss = 3.237\n",
      "\t batch, loss = 3.273\n",
      "\t batch, loss = 3.112\n",
      "\t batch, loss = 3.444\n",
      "\t batch, loss = 3.237\n",
      "\t batch, loss = 3.421\n",
      "\t batch, loss = 3.219\n",
      "\t batch, loss = 3.099\n",
      "\t batch, loss = 3.245\n",
      "\t batch, loss = 3.210\n",
      "\t batch, loss = 2.949\n",
      "\t batch, loss = 3.115\n",
      "\t batch, loss = 3.107\n",
      "\t batch, loss = 3.343\n",
      "\t batch, loss = 3.307\n",
      "\t batch, loss = 3.217\n",
      "\t batch, loss = 3.198\n",
      "\t batch, loss = 2.991\n",
      "\t batch, loss = 3.046\n",
      "\t batch, loss = 3.254\n",
      "\t batch, loss = 2.822\n",
      "\t batch, loss = 3.448\n",
      "\t batch, loss = 3.273\n",
      "\t batch, loss = 2.709\n",
      "\t batch, loss = 3.155\n",
      "\t batch, loss = 3.039\n",
      "\t batch, loss = 3.213\n",
      "\t batch, loss = 3.145\n",
      "\t batch, loss = 3.057\n",
      "\t batch, loss = 3.027\n",
      "\t batch, loss = 3.119\n",
      "\t batch, loss = 3.318\n",
      "\t batch, loss = 3.285\n",
      "\t batch, loss = 3.011\n",
      "\t batch, loss = 3.290\n",
      "\t batch, loss = 3.190\n",
      "\t batch, loss = 3.219\n",
      "\t batch, loss = 3.039\n",
      "\t batch, loss = 3.207\n",
      "\t batch, loss = 3.093\n",
      "\t batch, loss = 3.068\n",
      "\t batch, loss = 2.918\n",
      "\t batch, loss = 2.961\n",
      "\t batch, loss = 3.078\n",
      "\t batch, loss = 3.036\n",
      "\t batch, loss = 3.182\n",
      "\t batch, loss = 3.142\n",
      "\t batch, loss = 2.811\n",
      "\t batch, loss = 2.908\n",
      "\t batch, loss = 3.446\n",
      "\t batch, loss = 2.947\n",
      "\t batch, loss = 2.860\n",
      "\t batch, loss = 3.107\n",
      "\t batch, loss = 3.336\n",
      "\t batch, loss = 3.500\n",
      "\t batch, loss = 3.400\n",
      "\t batch, loss = 3.199\n",
      "\t batch, loss = 3.187\n",
      "\t batch, loss = 3.197\n",
      "\t batch, loss = 2.686\n",
      "\t batch, loss = 3.282\n",
      "\t batch, loss = 3.626\n",
      "\t batch, loss = 3.505\n",
      "\t batch, loss = 2.572\n",
      "\t batch, loss = 2.753\n",
      "\t batch, loss = 3.051\n",
      "\t batch, loss = 3.489\n",
      "\t batch, loss = 3.198\n",
      "\t batch, loss = 2.813\n",
      "\t batch, loss = 2.942\n",
      "\t batch, loss = 3.312\n",
      "\t batch, loss = 2.824\n",
      "\t batch, loss = 3.254\n",
      "\t batch, loss = 3.242\n",
      "\t batch, loss = 3.427\n",
      "\t batch, loss = 3.232\n",
      "\t batch, loss = 2.891\n",
      "\t batch, loss = 3.168\n",
      "\t batch, loss = 2.919\n",
      "\t batch, loss = 3.286\n",
      "\t batch, loss = 2.989\n",
      "\t batch, loss = 2.976\n",
      "\t batch, loss = 3.140\n",
      "\t batch, loss = 3.059\n",
      "\t batch, loss = 2.903\n",
      "\t batch, loss = 2.906\n",
      "\t batch, loss = 3.282\n",
      "\t batch, loss = 3.521\n",
      "\t batch, loss = 3.413\n",
      "\t batch, loss = 3.142\n",
      "\t batch, loss = 3.424\n",
      "\t batch, loss = 3.149\n",
      "\t batch, loss = 3.020\n",
      "\t batch, loss = 2.939\n",
      "\t batch, loss = 3.253\n",
      "\t batch, loss = 3.001\n",
      "\t batch, loss = 2.921\n",
      "\t batch, loss = 2.993\n",
      "\t batch, loss = 3.379\n",
      "\t batch, loss = 3.037\n",
      "\t batch, loss = 3.042\n",
      "\t batch, loss = 2.902\n",
      "\t batch, loss = 3.108\n",
      "\t batch, loss = 3.105\n",
      "\t batch, loss = 3.294\n",
      "\t batch, loss = 2.932\n",
      "\t batch, loss = 3.204\n",
      "\t batch, loss = 2.986\n",
      "\t batch, loss = 3.100\n",
      "\t batch, loss = 3.200\n",
      "\t batch, loss = 2.815\n",
      "\t batch, loss = 2.855\n",
      "\t batch, loss = 2.826\n",
      "\t batch, loss = 3.179\n",
      "\t batch, loss = 3.010\n",
      "\t batch, loss = 2.770\n",
      "\t batch, loss = 2.862\n",
      "\t batch, loss = 3.075\n",
      "\t batch, loss = 3.051\n",
      "\t batch, loss = 3.142\n",
      "\t batch, loss = 3.015\n",
      "\t batch, loss = 2.975\n",
      "\t batch, loss = 3.133\n",
      "\t batch, loss = 3.040\n",
      "\t batch, loss = 3.359\n",
      "\t batch, loss = 3.294\n",
      "\t batch, loss = 2.989\n",
      "\t batch, loss = 3.036\n",
      "\t batch, loss = 2.690\n",
      "\t batch, loss = 3.505\n",
      "\t batch, loss = 3.445\n",
      "\t batch, loss = 3.431\n",
      "\t batch, loss = 2.708\n",
      "\t batch, loss = 3.195\n",
      "\t batch, loss = 3.171\n",
      "\t batch, loss = 2.987\n",
      "\t batch, loss = 3.058\n",
      "\t batch, loss = 3.221\n",
      "\t batch, loss = 3.115\n",
      "\t batch, loss = 3.270\n",
      "\t batch, loss = 3.343\n",
      "\t batch, loss = 3.082\n",
      "\t batch, loss = 3.240\n",
      "\t batch, loss = 3.117\n",
      "\t batch, loss = 2.963\n",
      "\t batch, loss = 2.765\n",
      "\t batch, loss = 2.822\n",
      "\t batch, loss = 2.980\n",
      "\t batch, loss = 3.300\n",
      "\t batch, loss = 3.182\n",
      "\t batch, loss = 3.243\n",
      "\t batch, loss = 3.301\n",
      "\t batch, loss = 3.199\n",
      "\t batch, loss = 2.894\n",
      "\t batch, loss = 3.154\n",
      "\t batch, loss = 2.784\n",
      "\t batch, loss = 2.782\n",
      "\t batch, loss = 3.268\n",
      "\t batch, loss = 3.205\n",
      "\t batch, loss = 3.213\n",
      "\t batch, loss = 2.644\n",
      "\t batch, loss = 3.073\n",
      "\t batch, loss = 3.384\n",
      "\t batch, loss = 3.351\n",
      "\t batch, loss = 3.480\n",
      "\t batch, loss = 2.900\n",
      "\t batch, loss = 3.217\n",
      "\t batch, loss = 2.915\n",
      "\t batch, loss = 3.092\n",
      "\t batch, loss = 3.433\n",
      "\t batch, loss = 2.724\n",
      "\t batch, loss = 2.982\n",
      "\t batch, loss = 2.800\n",
      "\t batch, loss = 3.157\n",
      "\t batch, loss = 3.345\n",
      "\t batch, loss = 3.066\n",
      "\t batch, loss = 2.943\n",
      "\t batch, loss = 3.380\n",
      "\t batch, loss = 3.046\n",
      "\t batch, loss = 2.731\n",
      "\t batch, loss = 2.950\n",
      "\t batch, loss = 2.818\n",
      "\t batch, loss = 3.242\n",
      "\t batch, loss = 2.815\n",
      "\t batch, loss = 3.145\n",
      "\t batch, loss = 2.839\n",
      "\t batch, loss = 3.025\n",
      "\t batch, loss = 3.056\n",
      "\t batch, loss = 3.107\n",
      "\t batch, loss = 2.685\n",
      "\t batch, loss = 3.102\n",
      "\t batch, loss = 3.043\n",
      "\t batch, loss = 3.255\n",
      "\t batch, loss = 3.146\n",
      "\t batch, loss = 3.079\n",
      "\t batch, loss = 3.193\n",
      "\t batch, loss = 3.095\n",
      "\t batch, loss = 3.150\n",
      "\t batch, loss = 2.610\n",
      "\t batch, loss = 2.919\n",
      "\t batch, loss = 2.940\n",
      "\t batch, loss = 2.793\n",
      "\t batch, loss = 3.032\n",
      "\t batch, loss = 3.167\n",
      "\t batch, loss = 3.293\n",
      "\t batch, loss = 3.144\n",
      "\t batch, loss = 3.041\n",
      "\t batch, loss = 3.089\n",
      "\t batch, loss = 3.455\n",
      "\t batch, loss = 3.086\n",
      "\t batch, loss = 3.040\n",
      "\t batch, loss = 2.788\n",
      "\t batch, loss = 3.277\n",
      "\t batch, loss = 3.117\n",
      "\t batch, loss = 3.114\n",
      "\t batch, loss = 2.983\n",
      "\t batch, loss = 2.912\n",
      "\t batch, loss = 2.955\n",
      "\t batch, loss = 2.927\n",
      "\t batch, loss = 3.321\n",
      "\t batch, loss = 2.776\n",
      "\t batch, loss = 3.381\n",
      "\t batch, loss = 2.983\n",
      "\t batch, loss = 3.130\n",
      "\t batch, loss = 2.726\n",
      "\t batch, loss = 2.759\n",
      "\t batch, loss = 3.203\n",
      "\t batch, loss = 2.813\n",
      "\t batch, loss = 3.264\n",
      "\t batch, loss = 3.168\n",
      "\t batch, loss = 3.496\n",
      "\t batch, loss = 3.134\n",
      "\t batch, loss = 2.888\n",
      "\t batch, loss = 3.148\n",
      "\t batch, loss = 2.620\n",
      "\t batch, loss = 2.612\n",
      "\t batch, loss = 3.390\n",
      "\t batch, loss = 3.194\n",
      "\t batch, loss = 3.039\n",
      "\t batch, loss = 2.544\n",
      "\t batch, loss = 3.037\n",
      "\t batch, loss = 2.963\n",
      "\t batch, loss = 3.309\n",
      "\t batch, loss = 3.305\n",
      "\t batch, loss = 2.834\n",
      "\t batch, loss = 3.014\n",
      "\t batch, loss = 3.366\n",
      "\t batch, loss = 3.062\n",
      "\t batch, loss = 3.179\n",
      "\t batch, loss = 3.155\n",
      "\t batch, loss = 2.745\n",
      "\t batch, loss = 3.372\n",
      "\t batch, loss = 3.012\n",
      "\t batch, loss = 3.248\n",
      "\t batch, loss = 3.201\n",
      "\t batch, loss = 3.156\n",
      "\t batch, loss = 3.162\n",
      "\t batch, loss = 3.090\n",
      "\t batch, loss = 3.044\n",
      "\t batch, loss = 2.887\n",
      "\t batch, loss = 2.741\n",
      "\t batch, loss = 3.198\n",
      "\t batch, loss = 2.943\n",
      "\t batch, loss = 3.095\n",
      "\t batch, loss = 2.916\n",
      "\t batch, loss = 2.994\n",
      "\t batch, loss = 3.177\n",
      "\t batch, loss = 2.789\n",
      "\t batch, loss = 3.003\n",
      "\t batch, loss = 2.670\n",
      "\t batch, loss = 2.539\n",
      "\t batch, loss = 3.193\n",
      "\t batch, loss = 2.768\n",
      "\t batch, loss = 2.990\n",
      "\t batch, loss = 3.231\n",
      "\t batch, loss = 3.015\n",
      "\t batch, loss = 3.688\n",
      "Epoch = 2\n",
      "\t batch, loss = 2.808\n",
      "\t batch, loss = 3.073\n",
      "\t batch, loss = 3.242\n",
      "\t batch, loss = 3.356\n",
      "\t batch, loss = 3.123\n",
      "\t batch, loss = 2.918\n",
      "\t batch, loss = 3.324\n",
      "\t batch, loss = 3.511\n",
      "\t batch, loss = 2.959\n",
      "\t batch, loss = 3.238\n",
      "\t batch, loss = 2.882\n",
      "\t batch, loss = 3.136\n",
      "\t batch, loss = 3.089\n",
      "\t batch, loss = 2.931\n",
      "\t batch, loss = 3.122\n",
      "\t batch, loss = 3.198\n",
      "\t batch, loss = 2.903\n",
      "\t batch, loss = 3.412\n",
      "\t batch, loss = 2.877\n",
      "\t batch, loss = 2.719\n",
      "\t batch, loss = 3.175\n",
      "\t batch, loss = 2.872\n",
      "\t batch, loss = 3.038\n",
      "\t batch, loss = 2.579\n",
      "\t batch, loss = 2.924\n",
      "\t batch, loss = 3.129\n",
      "\t batch, loss = 2.974\n",
      "\t batch, loss = 3.367\n",
      "\t batch, loss = 2.952\n",
      "\t batch, loss = 3.180\n",
      "\t batch, loss = 2.976\n",
      "\t batch, loss = 2.970\n",
      "\t batch, loss = 2.833\n",
      "\t batch, loss = 2.562\n",
      "\t batch, loss = 2.891\n",
      "\t batch, loss = 2.900\n",
      "\t batch, loss = 2.922\n",
      "\t batch, loss = 3.194\n",
      "\t batch, loss = 2.966\n",
      "\t batch, loss = 3.117\n",
      "\t batch, loss = 2.928\n",
      "\t batch, loss = 3.025\n",
      "\t batch, loss = 2.993\n",
      "\t batch, loss = 3.244\n",
      "\t batch, loss = 3.191\n",
      "\t batch, loss = 2.921\n",
      "\t batch, loss = 3.352\n",
      "\t batch, loss = 2.982\n",
      "\t batch, loss = 3.449\n",
      "\t batch, loss = 3.199\n",
      "\t batch, loss = 2.829\n",
      "\t batch, loss = 2.770\n",
      "\t batch, loss = 3.140\n",
      "\t batch, loss = 3.212\n",
      "\t batch, loss = 2.937\n",
      "\t batch, loss = 3.121\n",
      "\t batch, loss = 2.967\n",
      "\t batch, loss = 3.088\n",
      "\t batch, loss = 3.186\n",
      "\t batch, loss = 3.243\n",
      "\t batch, loss = 3.137\n",
      "\t batch, loss = 3.026\n",
      "\t batch, loss = 2.905\n",
      "\t batch, loss = 2.624\n",
      "\t batch, loss = 3.203\n",
      "\t batch, loss = 2.768\n",
      "\t batch, loss = 3.298\n",
      "\t batch, loss = 2.998\n",
      "\t batch, loss = 2.854\n",
      "\t batch, loss = 2.805\n",
      "\t batch, loss = 2.899\n",
      "\t batch, loss = 2.961\n",
      "\t batch, loss = 3.051\n",
      "\t batch, loss = 3.513\n",
      "\t batch, loss = 3.018\n",
      "\t batch, loss = 3.075\n",
      "\t batch, loss = 3.102\n",
      "\t batch, loss = 2.845\n",
      "\t batch, loss = 3.181\n",
      "\t batch, loss = 2.840\n",
      "\t batch, loss = 2.848\n",
      "\t batch, loss = 3.215\n",
      "\t batch, loss = 3.119\n",
      "\t batch, loss = 2.547\n",
      "\t batch, loss = 2.964\n",
      "\t batch, loss = 3.020\n",
      "\t batch, loss = 2.594\n",
      "\t batch, loss = 2.715\n",
      "\t batch, loss = 2.856\n",
      "\t batch, loss = 3.544\n",
      "\t batch, loss = 3.049\n",
      "\t batch, loss = 2.803\n",
      "\t batch, loss = 3.171\n",
      "\t batch, loss = 3.075\n",
      "\t batch, loss = 2.937\n",
      "\t batch, loss = 3.067\n",
      "\t batch, loss = 2.838\n",
      "\t batch, loss = 3.160\n",
      "\t batch, loss = 2.879\n",
      "\t batch, loss = 3.099\n",
      "\t batch, loss = 2.963\n",
      "\t batch, loss = 2.933\n",
      "\t batch, loss = 2.968\n",
      "\t batch, loss = 2.953\n",
      "\t batch, loss = 3.153\n",
      "\t batch, loss = 3.089\n",
      "\t batch, loss = 3.182\n",
      "\t batch, loss = 3.031\n",
      "\t batch, loss = 3.015\n",
      "\t batch, loss = 3.356\n",
      "\t batch, loss = 2.633\n",
      "\t batch, loss = 3.419\n",
      "\t batch, loss = 2.954\n",
      "\t batch, loss = 2.795\n",
      "\t batch, loss = 3.287\n",
      "\t batch, loss = 3.135\n",
      "\t batch, loss = 2.569\n",
      "\t batch, loss = 3.315\n",
      "\t batch, loss = 2.864\n",
      "\t batch, loss = 3.146\n",
      "\t batch, loss = 2.649\n",
      "\t batch, loss = 2.949\n",
      "\t batch, loss = 2.965\n",
      "\t batch, loss = 3.160\n",
      "\t batch, loss = 2.900\n",
      "\t batch, loss = 3.158\n",
      "\t batch, loss = 3.106\n",
      "\t batch, loss = 3.186\n",
      "\t batch, loss = 2.951\n",
      "\t batch, loss = 3.271\n",
      "\t batch, loss = 3.441\n",
      "\t batch, loss = 2.615\n",
      "\t batch, loss = 2.819\n",
      "\t batch, loss = 3.345\n",
      "\t batch, loss = 2.793\n",
      "\t batch, loss = 2.814\n",
      "\t batch, loss = 3.046\n",
      "\t batch, loss = 3.098\n",
      "\t batch, loss = 3.227\n",
      "\t batch, loss = 3.221\n",
      "\t batch, loss = 3.072\n",
      "\t batch, loss = 3.146\n",
      "\t batch, loss = 3.349\n",
      "\t batch, loss = 2.807\n",
      "\t batch, loss = 3.166\n",
      "\t batch, loss = 3.134\n",
      "\t batch, loss = 2.672\n",
      "\t batch, loss = 2.935\n",
      "\t batch, loss = 3.088\n",
      "\t batch, loss = 2.953\n",
      "\t batch, loss = 2.790\n",
      "\t batch, loss = 2.962\n",
      "\t batch, loss = 2.821\n",
      "\t batch, loss = 3.347\n",
      "\t batch, loss = 2.689\n",
      "\t batch, loss = 2.792\n",
      "\t batch, loss = 3.340\n",
      "\t batch, loss = 3.263\n",
      "\t batch, loss = 2.772\n",
      "\t batch, loss = 3.133\n",
      "\t batch, loss = 3.171\n",
      "\t batch, loss = 3.120\n",
      "\t batch, loss = 2.892\n",
      "\t batch, loss = 3.258\n",
      "\t batch, loss = 2.867\n",
      "\t batch, loss = 3.101\n",
      "\t batch, loss = 3.294\n",
      "\t batch, loss = 2.715\n",
      "\t batch, loss = 2.856\n",
      "\t batch, loss = 2.869\n",
      "\t batch, loss = 3.372\n",
      "\t batch, loss = 2.855\n",
      "\t batch, loss = 2.852\n",
      "\t batch, loss = 2.746\n",
      "\t batch, loss = 2.560\n",
      "\t batch, loss = 3.062\n",
      "\t batch, loss = 3.262\n",
      "\t batch, loss = 3.352\n",
      "\t batch, loss = 3.110\n",
      "\t batch, loss = 3.180\n",
      "\t batch, loss = 2.502\n",
      "\t batch, loss = 2.928\n",
      "\t batch, loss = 3.177\n",
      "\t batch, loss = 2.980\n",
      "\t batch, loss = 2.837\n",
      "\t batch, loss = 3.271\n",
      "\t batch, loss = 2.818\n",
      "\t batch, loss = 3.287\n",
      "\t batch, loss = 3.041\n",
      "\t batch, loss = 2.884\n",
      "\t batch, loss = 2.581\n",
      "\t batch, loss = 3.046\n",
      "\t batch, loss = 2.702\n",
      "\t batch, loss = 3.051\n",
      "\t batch, loss = 3.114\n",
      "\t batch, loss = 2.973\n",
      "\t batch, loss = 3.080\n",
      "\t batch, loss = 3.070\n",
      "\t batch, loss = 3.232\n",
      "\t batch, loss = 3.321\n",
      "\t batch, loss = 3.227\n",
      "\t batch, loss = 2.944\n",
      "\t batch, loss = 3.118\n",
      "\t batch, loss = 3.335\n",
      "\t batch, loss = 2.900\n",
      "\t batch, loss = 3.192\n",
      "\t batch, loss = 2.551\n",
      "\t batch, loss = 2.870\n",
      "\t batch, loss = 2.805\n",
      "\t batch, loss = 2.945\n",
      "\t batch, loss = 3.237\n",
      "\t batch, loss = 3.607\n",
      "\t batch, loss = 3.063\n",
      "\t batch, loss = 3.003\n",
      "\t batch, loss = 2.721\n",
      "\t batch, loss = 3.167\n",
      "\t batch, loss = 2.907\n",
      "\t batch, loss = 3.008\n",
      "\t batch, loss = 3.138\n",
      "\t batch, loss = 3.493\n",
      "\t batch, loss = 2.884\n",
      "\t batch, loss = 2.954\n",
      "\t batch, loss = 3.182\n",
      "\t batch, loss = 3.036\n",
      "\t batch, loss = 2.791\n",
      "\t batch, loss = 3.215\n",
      "\t batch, loss = 3.067\n",
      "\t batch, loss = 3.059\n",
      "\t batch, loss = 2.668\n",
      "\t batch, loss = 2.855\n",
      "\t batch, loss = 2.906\n",
      "\t batch, loss = 3.019\n",
      "\t batch, loss = 2.826\n",
      "\t batch, loss = 3.097\n",
      "\t batch, loss = 2.779\n",
      "\t batch, loss = 3.188\n",
      "\t batch, loss = 3.092\n",
      "\t batch, loss = 2.930\n",
      "\t batch, loss = 2.811\n",
      "\t batch, loss = 3.532\n",
      "\t batch, loss = 3.064\n",
      "\t batch, loss = 2.945\n",
      "\t batch, loss = 3.037\n",
      "\t batch, loss = 3.085\n",
      "\t batch, loss = 3.078\n",
      "\t batch, loss = 3.030\n",
      "\t batch, loss = 3.061\n",
      "\t batch, loss = 3.046\n",
      "\t batch, loss = 2.911\n",
      "\t batch, loss = 3.028\n",
      "\t batch, loss = 2.334\n",
      "\t batch, loss = 3.136\n",
      "\t batch, loss = 2.752\n",
      "\t batch, loss = 3.211\n",
      "\t batch, loss = 2.508\n",
      "\t batch, loss = 2.816\n",
      "\t batch, loss = 2.884\n",
      "\t batch, loss = 3.068\n",
      "\t batch, loss = 2.845\n",
      "\t batch, loss = 3.157\n",
      "\t batch, loss = 3.015\n",
      "\t batch, loss = 3.035\n",
      "\t batch, loss = 3.407\n",
      "\t batch, loss = 2.852\n",
      "\t batch, loss = 3.227\n",
      "\t batch, loss = 3.156\n",
      "\t batch, loss = 3.030\n",
      "\t batch, loss = 3.147\n",
      "\t batch, loss = 2.701\n",
      "\t batch, loss = 3.177\n",
      "\t batch, loss = 2.944\n",
      "\t batch, loss = 3.123\n",
      "\t batch, loss = 2.867\n",
      "\t batch, loss = 2.689\n",
      "\t batch, loss = 3.292\n",
      "\t batch, loss = 2.853\n",
      "\t batch, loss = 3.075\n",
      "\t batch, loss = 2.779\n",
      "\t batch, loss = 2.674\n",
      "\t batch, loss = 2.697\n",
      "\t batch, loss = 3.254\n",
      "\t batch, loss = 3.291\n",
      "Epoch = 3\n",
      "\t batch, loss = 3.365\n",
      "\t batch, loss = 3.194\n",
      "\t batch, loss = 3.138\n",
      "\t batch, loss = 2.829\n",
      "\t batch, loss = 3.036\n",
      "\t batch, loss = 2.909\n",
      "\t batch, loss = 3.296\n",
      "\t batch, loss = 2.974\n",
      "\t batch, loss = 3.172\n",
      "\t batch, loss = 3.122\n",
      "\t batch, loss = 2.952\n",
      "\t batch, loss = 3.002\n",
      "\t batch, loss = 3.134\n",
      "\t batch, loss = 3.322\n",
      "\t batch, loss = 2.893\n",
      "\t batch, loss = 2.946\n",
      "\t batch, loss = 2.920\n",
      "\t batch, loss = 3.089\n",
      "\t batch, loss = 3.098\n",
      "\t batch, loss = 2.891\n",
      "\t batch, loss = 3.108\n",
      "\t batch, loss = 3.032\n",
      "\t batch, loss = 3.303\n",
      "\t batch, loss = 3.177\n",
      "\t batch, loss = 2.901\n",
      "\t batch, loss = 2.785\n",
      "\t batch, loss = 2.773\n",
      "\t batch, loss = 3.163\n",
      "\t batch, loss = 3.168\n",
      "\t batch, loss = 2.816\n",
      "\t batch, loss = 2.995\n",
      "\t batch, loss = 2.874\n",
      "\t batch, loss = 3.014\n",
      "\t batch, loss = 3.009\n",
      "\t batch, loss = 3.176\n",
      "\t batch, loss = 3.073\n",
      "\t batch, loss = 3.116\n",
      "\t batch, loss = 2.647\n",
      "\t batch, loss = 2.791\n",
      "\t batch, loss = 3.060\n",
      "\t batch, loss = 2.981\n",
      "\t batch, loss = 2.872\n",
      "\t batch, loss = 2.966\n",
      "\t batch, loss = 3.263\n",
      "\t batch, loss = 2.859\n",
      "\t batch, loss = 3.399\n",
      "\t batch, loss = 2.928\n",
      "\t batch, loss = 3.032\n",
      "\t batch, loss = 2.916\n",
      "\t batch, loss = 2.733\n",
      "\t batch, loss = 3.213\n",
      "\t batch, loss = 2.639\n",
      "\t batch, loss = 3.193\n",
      "\t batch, loss = 2.731\n",
      "\t batch, loss = 2.898\n",
      "\t batch, loss = 2.976\n",
      "\t batch, loss = 3.061\n",
      "\t batch, loss = 3.167\n",
      "\t batch, loss = 2.987\n",
      "\t batch, loss = 2.984\n",
      "\t batch, loss = 2.754\n",
      "\t batch, loss = 3.319\n",
      "\t batch, loss = 3.050\n",
      "\t batch, loss = 3.078\n",
      "\t batch, loss = 2.875\n",
      "\t batch, loss = 3.066\n",
      "\t batch, loss = 3.152\n",
      "\t batch, loss = 2.894\n",
      "\t batch, loss = 2.875\n",
      "\t batch, loss = 3.219\n",
      "\t batch, loss = 2.607\n",
      "\t batch, loss = 2.847\n",
      "\t batch, loss = 2.999\n",
      "\t batch, loss = 2.964\n",
      "\t batch, loss = 3.042\n",
      "\t batch, loss = 3.300\n",
      "\t batch, loss = 3.143\n",
      "\t batch, loss = 2.813\n",
      "\t batch, loss = 2.790\n",
      "\t batch, loss = 2.931\n",
      "\t batch, loss = 3.123\n",
      "\t batch, loss = 2.840\n",
      "\t batch, loss = 3.232\n",
      "\t batch, loss = 3.026\n",
      "\t batch, loss = 2.996\n",
      "\t batch, loss = 3.189\n",
      "\t batch, loss = 2.751\n",
      "\t batch, loss = 3.187\n",
      "\t batch, loss = 2.685\n",
      "\t batch, loss = 3.020\n",
      "\t batch, loss = 2.809\n",
      "\t batch, loss = 2.980\n",
      "\t batch, loss = 2.847\n",
      "\t batch, loss = 3.172\n",
      "\t batch, loss = 3.270\n",
      "\t batch, loss = 2.904\n",
      "\t batch, loss = 3.011\n",
      "\t batch, loss = 2.926\n",
      "\t batch, loss = 2.808\n",
      "\t batch, loss = 3.262\n",
      "\t batch, loss = 2.744\n",
      "\t batch, loss = 3.276\n",
      "\t batch, loss = 2.682\n",
      "\t batch, loss = 2.878\n",
      "\t batch, loss = 2.917\n",
      "\t batch, loss = 2.927\n",
      "\t batch, loss = 3.178\n",
      "\t batch, loss = 2.989\n",
      "\t batch, loss = 3.066\n",
      "\t batch, loss = 2.971\n",
      "\t batch, loss = 3.078\n",
      "\t batch, loss = 3.055\n",
      "\t batch, loss = 3.129\n",
      "\t batch, loss = 3.279\n",
      "\t batch, loss = 3.250\n",
      "\t batch, loss = 2.919\n",
      "\t batch, loss = 3.060\n",
      "\t batch, loss = 3.158\n",
      "\t batch, loss = 2.878\n",
      "\t batch, loss = 3.246\n",
      "\t batch, loss = 3.031\n",
      "\t batch, loss = 2.981\n",
      "\t batch, loss = 3.271\n",
      "\t batch, loss = 3.209\n",
      "\t batch, loss = 2.943\n",
      "\t batch, loss = 2.991\n",
      "\t batch, loss = 3.140\n",
      "\t batch, loss = 2.901\n",
      "\t batch, loss = 3.153\n",
      "\t batch, loss = 3.039\n",
      "\t batch, loss = 3.098\n",
      "\t batch, loss = 2.739\n",
      "\t batch, loss = 2.942\n",
      "\t batch, loss = 2.954\n",
      "\t batch, loss = 2.906\n",
      "\t batch, loss = 2.724\n",
      "\t batch, loss = 2.948\n",
      "\t batch, loss = 2.927\n",
      "\t batch, loss = 2.600\n",
      "\t batch, loss = 3.303\n",
      "\t batch, loss = 2.862\n",
      "\t batch, loss = 2.774\n",
      "\t batch, loss = 3.076\n",
      "\t batch, loss = 3.221\n",
      "\t batch, loss = 2.812\n",
      "\t batch, loss = 2.853\n",
      "\t batch, loss = 3.011\n",
      "\t batch, loss = 2.848\n",
      "\t batch, loss = 3.094\n",
      "\t batch, loss = 3.063\n",
      "\t batch, loss = 3.084\n",
      "\t batch, loss = 2.842\n",
      "\t batch, loss = 2.727\n",
      "\t batch, loss = 2.974\n",
      "\t batch, loss = 3.212\n",
      "\t batch, loss = 3.245\n",
      "\t batch, loss = 3.086\n",
      "\t batch, loss = 2.778\n",
      "\t batch, loss = 2.982\n",
      "\t batch, loss = 2.884\n",
      "\t batch, loss = 3.338\n",
      "\t batch, loss = 2.993\n",
      "\t batch, loss = 3.018\n",
      "\t batch, loss = 3.025\n",
      "\t batch, loss = 2.988\n",
      "\t batch, loss = 2.892\n",
      "\t batch, loss = 3.082\n",
      "\t batch, loss = 2.946\n",
      "\t batch, loss = 2.759\n",
      "\t batch, loss = 3.119\n",
      "\t batch, loss = 3.029\n",
      "\t batch, loss = 3.358\n",
      "\t batch, loss = 2.824\n",
      "\t batch, loss = 3.084\n",
      "\t batch, loss = 2.870\n",
      "\t batch, loss = 2.735\n",
      "\t batch, loss = 3.187\n",
      "\t batch, loss = 3.374\n",
      "\t batch, loss = 3.382\n",
      "\t batch, loss = 2.789\n",
      "\t batch, loss = 2.795\n",
      "\t batch, loss = 3.052\n",
      "\t batch, loss = 2.699\n",
      "\t batch, loss = 2.994\n",
      "\t batch, loss = 3.286\n",
      "\t batch, loss = 2.528\n",
      "\t batch, loss = 3.104\n",
      "\t batch, loss = 3.211\n",
      "\t batch, loss = 3.069\n",
      "\t batch, loss = 3.070\n",
      "\t batch, loss = 3.308\n",
      "\t batch, loss = 2.809\n",
      "\t batch, loss = 2.751\n",
      "\t batch, loss = 3.069\n",
      "\t batch, loss = 3.292\n",
      "\t batch, loss = 2.892\n",
      "\t batch, loss = 2.940\n",
      "\t batch, loss = 2.781\n",
      "\t batch, loss = 3.078\n",
      "\t batch, loss = 3.015\n",
      "\t batch, loss = 2.993\n",
      "\t batch, loss = 3.143\n",
      "\t batch, loss = 2.786\n",
      "\t batch, loss = 3.072\n",
      "\t batch, loss = 2.832\n",
      "\t batch, loss = 3.005\n",
      "\t batch, loss = 2.537\n",
      "\t batch, loss = 3.034\n",
      "\t batch, loss = 3.037\n",
      "\t batch, loss = 3.177\n",
      "\t batch, loss = 2.906\n",
      "\t batch, loss = 2.744\n",
      "\t batch, loss = 3.174\n",
      "\t batch, loss = 2.905\n",
      "\t batch, loss = 3.066\n",
      "\t batch, loss = 2.944\n",
      "\t batch, loss = 3.259\n",
      "\t batch, loss = 2.767\n",
      "\t batch, loss = 2.904\n",
      "\t batch, loss = 2.771\n",
      "\t batch, loss = 3.136\n",
      "\t batch, loss = 3.240\n",
      "\t batch, loss = 2.593\n",
      "\t batch, loss = 2.936\n",
      "\t batch, loss = 3.215\n",
      "\t batch, loss = 3.171\n",
      "\t batch, loss = 2.968\n",
      "\t batch, loss = 3.085\n",
      "\t batch, loss = 2.608\n",
      "\t batch, loss = 2.915\n",
      "\t batch, loss = 2.839\n",
      "\t batch, loss = 3.359\n",
      "\t batch, loss = 2.613\n",
      "\t batch, loss = 2.980\n",
      "\t batch, loss = 2.915\n",
      "\t batch, loss = 2.612\n",
      "\t batch, loss = 3.084\n",
      "\t batch, loss = 2.893\n",
      "\t batch, loss = 2.920\n",
      "\t batch, loss = 3.090\n",
      "\t batch, loss = 2.561\n",
      "\t batch, loss = 3.133\n",
      "\t batch, loss = 2.856\n",
      "\t batch, loss = 2.915\n",
      "\t batch, loss = 3.391\n",
      "\t batch, loss = 2.928\n",
      "\t batch, loss = 3.057\n",
      "\t batch, loss = 2.738\n",
      "\t batch, loss = 2.843\n",
      "\t batch, loss = 2.828\n",
      "\t batch, loss = 2.846\n",
      "\t batch, loss = 3.068\n",
      "\t batch, loss = 3.121\n",
      "\t batch, loss = 2.961\n",
      "\t batch, loss = 3.247\n",
      "\t batch, loss = 3.100\n",
      "\t batch, loss = 2.821\n",
      "\t batch, loss = 2.961\n",
      "\t batch, loss = 2.706\n",
      "\t batch, loss = 2.602\n",
      "\t batch, loss = 2.677\n",
      "\t batch, loss = 3.247\n",
      "\t batch, loss = 2.884\n",
      "\t batch, loss = 2.669\n",
      "\t batch, loss = 3.021\n",
      "\t batch, loss = 2.853\n",
      "\t batch, loss = 2.862\n",
      "\t batch, loss = 2.721\n",
      "\t batch, loss = 2.863\n",
      "\t batch, loss = 2.790\n",
      "\t batch, loss = 2.613\n",
      "\t batch, loss = 2.897\n",
      "\t batch, loss = 2.466\n",
      "\t batch, loss = 2.769\n",
      "\t batch, loss = 3.245\n",
      "\t batch, loss = 3.071\n",
      "\t batch, loss = 2.619\n",
      "\t batch, loss = 2.526\n",
      "\t batch, loss = 3.211\n",
      "\t batch, loss = 2.462\n",
      "\t batch, loss = 2.454\n",
      "\t batch, loss = 3.258\n",
      "Epoch = 4\n",
      "\t batch, loss = 3.014\n",
      "\t batch, loss = 2.744\n",
      "\t batch, loss = 3.168\n",
      "\t batch, loss = 3.260\n",
      "\t batch, loss = 2.770\n",
      "\t batch, loss = 2.720\n",
      "\t batch, loss = 2.717\n",
      "\t batch, loss = 2.906\n",
      "\t batch, loss = 2.830\n",
      "\t batch, loss = 3.021\n",
      "\t batch, loss = 3.203\n",
      "\t batch, loss = 2.839\n",
      "\t batch, loss = 3.163\n",
      "\t batch, loss = 2.476\n",
      "\t batch, loss = 3.331\n",
      "\t batch, loss = 2.906\n",
      "\t batch, loss = 2.821\n",
      "\t batch, loss = 3.176\n",
      "\t batch, loss = 3.152\n",
      "\t batch, loss = 2.958\n",
      "\t batch, loss = 3.428\n",
      "\t batch, loss = 2.665\n",
      "\t batch, loss = 2.986\n",
      "\t batch, loss = 2.821\n",
      "\t batch, loss = 3.073\n",
      "\t batch, loss = 2.889\n",
      "\t batch, loss = 2.430\n",
      "\t batch, loss = 2.327\n",
      "\t batch, loss = 3.245\n",
      "\t batch, loss = 2.914\n",
      "\t batch, loss = 2.832\n",
      "\t batch, loss = 2.825\n",
      "\t batch, loss = 3.366\n",
      "\t batch, loss = 2.796\n",
      "\t batch, loss = 3.228\n",
      "\t batch, loss = 3.194\n",
      "\t batch, loss = 3.135\n",
      "\t batch, loss = 2.998\n",
      "\t batch, loss = 2.903\n",
      "\t batch, loss = 3.174\n",
      "\t batch, loss = 2.954\n",
      "\t batch, loss = 2.755\n",
      "\t batch, loss = 2.967\n",
      "\t batch, loss = 3.029\n",
      "\t batch, loss = 3.259\n",
      "\t batch, loss = 3.030\n",
      "\t batch, loss = 3.063\n",
      "\t batch, loss = 2.931\n",
      "\t batch, loss = 3.164\n",
      "\t batch, loss = 2.794\n",
      "\t batch, loss = 2.335\n",
      "\t batch, loss = 3.059\n",
      "\t batch, loss = 2.959\n",
      "\t batch, loss = 2.849\n",
      "\t batch, loss = 2.663\n",
      "\t batch, loss = 2.860\n",
      "\t batch, loss = 2.932\n",
      "\t batch, loss = 2.545\n",
      "\t batch, loss = 2.941\n",
      "\t batch, loss = 2.498\n",
      "\t batch, loss = 2.880\n",
      "\t batch, loss = 2.991\n",
      "\t batch, loss = 3.273\n",
      "\t batch, loss = 3.033\n",
      "\t batch, loss = 3.261\n",
      "\t batch, loss = 3.190\n",
      "\t batch, loss = 3.025\n",
      "\t batch, loss = 2.944\n",
      "\t batch, loss = 2.852\n",
      "\t batch, loss = 2.902\n",
      "\t batch, loss = 2.823\n",
      "\t batch, loss = 2.972\n",
      "\t batch, loss = 3.138\n",
      "\t batch, loss = 2.540\n",
      "\t batch, loss = 3.007\n",
      "\t batch, loss = 3.128\n",
      "\t batch, loss = 2.939\n",
      "\t batch, loss = 3.051\n",
      "\t batch, loss = 2.756\n",
      "\t batch, loss = 2.921\n",
      "\t batch, loss = 3.354\n",
      "\t batch, loss = 3.048\n",
      "\t batch, loss = 3.020\n",
      "\t batch, loss = 3.081\n",
      "\t batch, loss = 3.210\n",
      "\t batch, loss = 2.949\n",
      "\t batch, loss = 3.006\n",
      "\t batch, loss = 2.885\n",
      "\t batch, loss = 2.949\n",
      "\t batch, loss = 3.136\n",
      "\t batch, loss = 2.940\n",
      "\t batch, loss = 3.176\n",
      "\t batch, loss = 3.238\n",
      "\t batch, loss = 2.937\n",
      "\t batch, loss = 2.835\n",
      "\t batch, loss = 2.839\n",
      "\t batch, loss = 2.844\n",
      "\t batch, loss = 2.839\n",
      "\t batch, loss = 2.541\n",
      "\t batch, loss = 2.976\n",
      "\t batch, loss = 2.824\n",
      "\t batch, loss = 2.964\n",
      "\t batch, loss = 2.958\n",
      "\t batch, loss = 2.670\n",
      "\t batch, loss = 3.006\n",
      "\t batch, loss = 3.103\n",
      "\t batch, loss = 2.756\n",
      "\t batch, loss = 2.944\n",
      "\t batch, loss = 2.743\n",
      "\t batch, loss = 2.988\n",
      "\t batch, loss = 2.331\n",
      "\t batch, loss = 2.656\n",
      "\t batch, loss = 2.981\n",
      "\t batch, loss = 3.005\n",
      "\t batch, loss = 2.613\n",
      "\t batch, loss = 3.233\n",
      "\t batch, loss = 3.010\n",
      "\t batch, loss = 2.816\n",
      "\t batch, loss = 2.617\n",
      "\t batch, loss = 2.996\n",
      "\t batch, loss = 3.145\n",
      "\t batch, loss = 2.372\n",
      "\t batch, loss = 2.600\n",
      "\t batch, loss = 2.965\n",
      "\t batch, loss = 2.671\n",
      "\t batch, loss = 3.527\n",
      "\t batch, loss = 2.834\n",
      "\t batch, loss = 2.932\n",
      "\t batch, loss = 3.150\n",
      "\t batch, loss = 2.786\n",
      "\t batch, loss = 2.859\n",
      "\t batch, loss = 3.127\n",
      "\t batch, loss = 2.557\n",
      "\t batch, loss = 2.573\n",
      "\t batch, loss = 2.916\n",
      "\t batch, loss = 2.926\n",
      "\t batch, loss = 2.388\n",
      "\t batch, loss = 2.880\n",
      "\t batch, loss = 2.559\n",
      "\t batch, loss = 3.027\n",
      "\t batch, loss = 2.935\n",
      "\t batch, loss = 3.048\n",
      "\t batch, loss = 3.039\n",
      "\t batch, loss = 3.168\n",
      "\t batch, loss = 3.072\n",
      "\t batch, loss = 2.951\n",
      "\t batch, loss = 2.756\n",
      "\t batch, loss = 2.989\n",
      "\t batch, loss = 3.276\n",
      "\t batch, loss = 2.765\n",
      "\t batch, loss = 3.381\n",
      "\t batch, loss = 3.048\n",
      "\t batch, loss = 2.878\n",
      "\t batch, loss = 2.848\n",
      "\t batch, loss = 3.084\n",
      "\t batch, loss = 2.625\n",
      "\t batch, loss = 3.178\n",
      "\t batch, loss = 3.115\n",
      "\t batch, loss = 3.060\n",
      "\t batch, loss = 2.455\n",
      "\t batch, loss = 2.748\n",
      "\t batch, loss = 3.138\n",
      "\t batch, loss = 2.950\n",
      "\t batch, loss = 3.101\n",
      "\t batch, loss = 3.015\n",
      "\t batch, loss = 3.123\n",
      "\t batch, loss = 3.103\n",
      "\t batch, loss = 2.939\n",
      "\t batch, loss = 2.867\n",
      "\t batch, loss = 3.290\n",
      "\t batch, loss = 2.921\n",
      "\t batch, loss = 3.094\n",
      "\t batch, loss = 3.062\n",
      "\t batch, loss = 2.921\n",
      "\t batch, loss = 2.744\n",
      "\t batch, loss = 2.628\n",
      "\t batch, loss = 2.722\n",
      "\t batch, loss = 2.999\n",
      "\t batch, loss = 3.289\n",
      "\t batch, loss = 2.997\n",
      "\t batch, loss = 2.824\n",
      "\t batch, loss = 2.710\n",
      "\t batch, loss = 2.661\n",
      "\t batch, loss = 3.086\n",
      "\t batch, loss = 2.712\n",
      "\t batch, loss = 2.877\n",
      "\t batch, loss = 2.980\n",
      "\t batch, loss = 3.344\n",
      "\t batch, loss = 2.866\n",
      "\t batch, loss = 2.646\n",
      "\t batch, loss = 3.107\n",
      "\t batch, loss = 2.927\n",
      "\t batch, loss = 2.936\n",
      "\t batch, loss = 3.041\n",
      "\t batch, loss = 2.619\n",
      "\t batch, loss = 2.851\n",
      "\t batch, loss = 2.692\n",
      "\t batch, loss = 3.134\n",
      "\t batch, loss = 2.691\n",
      "\t batch, loss = 2.573\n",
      "\t batch, loss = 2.839\n",
      "\t batch, loss = 3.074\n",
      "\t batch, loss = 2.914\n",
      "\t batch, loss = 3.001\n",
      "\t batch, loss = 3.100\n",
      "\t batch, loss = 3.144\n",
      "\t batch, loss = 2.820\n",
      "\t batch, loss = 3.379\n",
      "\t batch, loss = 2.838\n",
      "\t batch, loss = 2.832\n",
      "\t batch, loss = 3.267\n",
      "\t batch, loss = 2.790\n",
      "\t batch, loss = 3.030\n",
      "\t batch, loss = 2.885\n",
      "\t batch, loss = 3.031\n",
      "\t batch, loss = 3.089\n",
      "\t batch, loss = 2.958\n",
      "\t batch, loss = 3.172\n",
      "\t batch, loss = 3.011\n",
      "\t batch, loss = 2.933\n",
      "\t batch, loss = 2.731\n",
      "\t batch, loss = 3.054\n",
      "\t batch, loss = 2.743\n",
      "\t batch, loss = 3.002\n",
      "\t batch, loss = 3.188\n",
      "\t batch, loss = 2.906\n",
      "\t batch, loss = 2.883\n",
      "\t batch, loss = 3.163\n",
      "\t batch, loss = 2.998\n",
      "\t batch, loss = 3.043\n",
      "\t batch, loss = 2.784\n",
      "\t batch, loss = 2.797\n",
      "\t batch, loss = 3.018\n",
      "\t batch, loss = 2.554\n",
      "\t batch, loss = 2.823\n",
      "\t batch, loss = 2.792\n",
      "\t batch, loss = 2.780\n",
      "\t batch, loss = 3.092\n",
      "\t batch, loss = 2.850\n",
      "\t batch, loss = 2.718\n",
      "\t batch, loss = 3.074\n",
      "\t batch, loss = 2.809\n",
      "\t batch, loss = 2.849\n",
      "\t batch, loss = 2.727\n",
      "\t batch, loss = 3.178\n",
      "\t batch, loss = 2.972\n",
      "\t batch, loss = 2.992\n",
      "\t batch, loss = 2.775\n",
      "\t batch, loss = 3.090\n",
      "\t batch, loss = 2.988\n",
      "\t batch, loss = 3.114\n",
      "\t batch, loss = 2.973\n",
      "\t batch, loss = 2.739\n",
      "\t batch, loss = 2.943\n",
      "\t batch, loss = 3.043\n",
      "\t batch, loss = 3.029\n",
      "\t batch, loss = 3.190\n",
      "\t batch, loss = 3.204\n",
      "\t batch, loss = 2.929\n",
      "\t batch, loss = 2.531\n",
      "\t batch, loss = 2.246\n",
      "\t batch, loss = 3.150\n",
      "\t batch, loss = 3.150\n",
      "\t batch, loss = 2.658\n",
      "\t batch, loss = 2.804\n",
      "\t batch, loss = 3.001\n",
      "\t batch, loss = 2.498\n",
      "\t batch, loss = 2.760\n",
      "\t batch, loss = 2.960\n",
      "\t batch, loss = 2.668\n",
      "\t batch, loss = 2.905\n",
      "\t batch, loss = 2.998\n",
      "\t batch, loss = 2.923\n",
      "\t batch, loss = 2.691\n",
      "\t batch, loss = 3.184\n",
      "\t batch, loss = 2.909\n",
      "\t batch, loss = 2.726\n",
      "\t batch, loss = 2.830\n",
      "\t batch, loss = 2.826\n",
      "\t batch, loss = 2.693\n",
      "\t batch, loss = 2.531\n",
      "\t batch, loss = 3.063\n",
      "Epoch = 5\n",
      "\t batch, loss = 2.703\n",
      "\t batch, loss = 2.770\n",
      "\t batch, loss = 3.210\n",
      "\t batch, loss = 2.706\n",
      "\t batch, loss = 3.052\n",
      "\t batch, loss = 2.610\n",
      "\t batch, loss = 2.790\n",
      "\t batch, loss = 3.003\n",
      "\t batch, loss = 2.933\n",
      "\t batch, loss = 3.185\n",
      "\t batch, loss = 3.060\n",
      "\t batch, loss = 2.767\n",
      "\t batch, loss = 2.413\n",
      "\t batch, loss = 3.169\n",
      "\t batch, loss = 2.624\n",
      "\t batch, loss = 2.995\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m masks \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 6\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m loss \u001b[38;5;241m=\u001b[39m crit(logits\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size), labels\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      8\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/md_sims/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/md_sims/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m, in \u001b[0;36mMaskedTransformer.forward\u001b[0;34m(self, input_ids, mask)\u001b[0m\n\u001b[1;32m     12\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed(input_ids)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/md_sims/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/md_sims/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n\u001b[0;32m---> 11\u001b[0m     attn_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m attn_out\n\u001b[1;32m     13\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/md_sims/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/md_sims/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/md_sims/lib/python3.9/site-packages/torch/nn/modules/activation.py:1373\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1347\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1348\u001b[0m         query,\n\u001b[1;32m   1349\u001b[0m         key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m   1371\u001b[0m     )\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1373\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1391\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1393\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/miniconda3/envs/md_sims/lib/python3.9/site-packages/torch/nn/functional.py:6389\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   6387\u001b[0m attn_output_weights \u001b[38;5;241m=\u001b[39m attn_output_weights\u001b[38;5;241m.\u001b[39mview(bsz, num_heads, tgt_len, src_len)\n\u001b[1;32m   6388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average_attn_weights:\n\u001b[0;32m-> 6389\u001b[0m     attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mattn_output_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n\u001b[1;32m   6392\u001b[0m     \u001b[38;5;66;03m# squeeze the output if input was unbatched\u001b[39;00m\n\u001b[1;32m   6393\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    for batch in train_loader:\n",
    "        ids, labels = mask_inputs(batch['input_ids'], tokenizer.token_to_id('[MASK]'))\n",
    "        masks = batch['attention_mask']\n",
    "        opt.zero_grad()\n",
    "        logits = model(ids, masks)\n",
    "        loss = crit(logits.reshape(-1, vocab_size), labels.reshape(-1))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        print(f\"\\t batch, loss = {loss:.3f}\")\n",
    "    print(f\"Epoch = {epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedTransformer(\n",
       "  (embed): Embedding(517, 64)\n",
       "  (layers): ModuleList(\n",
       "    (0-2): 3 x TransformerBlock(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=64, out_features=517, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'masked_transformer_weights.pth')\n",
    "\n",
    "model = MaskedTransformer(vocab_size)\n",
    "model.load_state_dict(torch.load('masked_transformer_weights.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  CC(C)c1cc(C(=O)Nc2ccc(C[NH+]3CCCC3)cc2)n[nH]1\n",
      "\n",
      "Masked  :  [UNK]ĠCC(C[MASK]c1[MASK](C(=[MASK])Nc2ccc[MASK]C[NH+]3CCCC3)cc2)n[[MASK][MASK]1[MASK][CLS]\n",
      "Predicted: ĠCCĊ)ĊĊn)()OO)sĠOn1(nCSCCOOOĊ)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# 1. Load the 10 000th SMILES\n",
    "df_full = pd.read_csv('250k_rndm_zinc_drugs_clean_3.csv')\n",
    "smiles_orig = df_full.iloc[10001]['smiles']  # zero-based index\n",
    "\n",
    "# 2. Encode and build batch\n",
    "encoding = tokenizer.encode(smiles_orig)\n",
    "input_ids = torch.tensor(encoding.ids).unsqueeze(0)           # shape [1, L]\n",
    "attention_mask = torch.tensor(encoding.attention_mask).unsqueeze(0)\n",
    "\n",
    "# 3. Apply masking\n",
    "mask_id = tokenizer.token_to_id('[MASK]')\n",
    "ids_masked, _ = mask_inputs(input_ids.clone(), mask_id, mask_prob=0.15)\n",
    "\n",
    "# 4. Forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(ids_masked, attention_mask)               # [1, L, V]\n",
    "    preds = logits.argmax(dim=-1).squeeze(0).tolist()         # [L]\n",
    "\n",
    "# 5. Decode back to tokens → SMILES\n",
    "tokens_pred = [tokenizer.id_to_token(i) for i in preds]\n",
    "# strip off [CLS]/[SEP] and join\n",
    "smiles_pred = ''.join(tok for tok in tokens_pred if tok not in ('[CLS]','[SEP]','[PAD]'))\n",
    "\n",
    "print(\"Original: \", smiles_orig)\n",
    "print(\"Masked  : \", ''.join(\n",
    "    tokenizer.id_to_token(i) if i!=mask_id else '[MASK]'\n",
    "    for i in ids_masked.squeeze(0).tolist()))\n",
    "print(\"Predicted:\", smiles_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it's mostly predicting garbage, it has understood some common features like starting the chain with 2 carbons etc. This poor performance is mostly to be expected for such a small dataset and ~5 epochs with shallow nets and small batches, but I can see the shape of the structure starting form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Lyra"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "md_sims",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
