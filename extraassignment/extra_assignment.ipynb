{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Assignment: Transformers and SSMs\n",
    "Connor Blake, 5/23/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers, processors\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optax\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this assignment too long? If you are using AI tools to the maximum extent, not at all. If the internet goes down? Yes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Transfomer\n",
    "Explain, in as much detail as possible, how a Transformer works from the paper [Attention is All You Need](https://arxiv.org/pdf/1706.03762). Be explicit in the use of function signatures, tensor dimensionalities, function definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 1:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1 Notation and Shapes  \n",
    "\n",
    "* B — batch size  \n",
    "* T — source-sequence length  \n",
    "* T′ — target-sequence length  \n",
    "* H — number of attention heads  \n",
    "* $d_{\\text{model}}$ — model width  \n",
    "* $d_k = d_{\\text{model}} ⁄ H$ — key/query sub-dimension  \n",
    "* $d_v = d_{\\text{model}} ⁄ H$ — value sub-dimension  \n",
    "* $d_{FF} ≈ 4 d_{\\text{model}}$ — hidden width of the feed-forward layer  \n",
    "\n",
    "Indices  \n",
    "\n",
    "* b ∈ {0,…,B−1} — sequence in batch  \n",
    "* t ∈ {0,…,T−1} — source token position  \n",
    "* t′ ∈ {0,…,T′−1} — target token position  \n",
    "* h ∈ {0,…,H−1} — head index  \n",
    "* m ∈ {0,…,d_{\\text{model}}−1} — model feature  \n",
    "* i ∈ {0,…,d_k−1} — key/query feature  \n",
    "* j ∈ {0,…,d_v−1} — value feature  \n",
    "* r ∈ {0,…,$d_{\\text{FF}}$}-1$ — feed-forward feature  \n",
    "\n",
    "A tensor element $X_{b t m}$ contains feature $m$ of token $t$ in sequence $b$.\n",
    "\n",
    "### 2 Primitive Operations  \n",
    "\n",
    "#### 2.1 Softmax  \n",
    "\n",
    "$$\n",
    "\\text{softmax}(Z)_{b n} = \\frac{e^{Z_{b n}}}{\\sum_{n′} e^{Z_{b n′}}}, \\quad\n",
    "Z \\in \\mathbb R^{B \\times N}.\n",
    "$$\n",
    "\n",
    "#### 2.2 Layer Normalization  \n",
    "\n",
    "Given $X_{b t m} ∈ ℝ^{B×T×d}  $\n",
    "\n",
    "$$\n",
    "\\mu_{b t} = \\frac{1}{d} \\sum_{m} X_{b t m}, \\qquad\n",
    "\\sigma_{b t} = \\sqrt{\\frac{1}{d} \\sum_{m} (X_{b t m} - \\mu_{b t})^{2} + \\varepsilon},\n",
    "$$\n",
    "$$\n",
    "\\text{LayerNorm}(X)_{b t m} =\n",
    "\\frac{X_{b t m} - \\mu_{b t}}{\\sigma_{b t}} \\, \\gamma_{m} + \\beta_{m},\n",
    "$$\n",
    "\n",
    "with learned gain $γ_{m}$ and bias $β_{m}$.\n",
    "\n",
    "### 3 Scaled Dot-Product Attention  \n",
    "\n",
    "#### 3.1 Linear projections  \n",
    "\n",
    "Learn weight matrices  \n",
    "\n",
    "* $W^{Q}_{m i} ∈ ℝ^{d_{\\text{model}}×d_k}$  \n",
    "* $W^{K}_{m i} ∈ ℝ^{d_{\\text{model}}×d_k}  $\n",
    "* $W^{V}_{m j} ∈ ℝ^{d_{\\text{model}}×d_v}$\n",
    "\n",
    "and form  \n",
    "\n",
    "$$\n",
    "Q_{b t i} = X_{b t m} \\, W^{Q}_{m i}, \\quad\n",
    "K_{b t i} = X_{b t m} \\, W^{K}_{m i}, \\quad\n",
    "V_{b t j} = X_{b t m} \\, W^{V}_{m j}.\n",
    "$$\n",
    "\n",
    "Split the last index into H heads:  \n",
    "$Q_{b t h i}, K_{b t h i} ∈ ℝ^{B×T×H×d_k}; V_{b t h j} ∈ ℝ^{B×T×H×d_v}$.\n",
    "\n",
    "#### 3.2 Attention kernel and weights  \n",
    "\n",
    "$$\n",
    "S_{b h t_q t_k} = \\frac{1}{\\sqrt{d_k}}\n",
    "\\, Q_{b t_q h i} \\, K_{b t_k h i}.\n",
    "$$\n",
    "\n",
    "For decoder self-attention, entries with $t_k > t_q$ are set to $-\\infty$.\n",
    "\n",
    "$$\n",
    "\\alpha_{b h t_q t_k} = \\text{softmax}_{t_k} \\bigl( S_{b h t_q t_k} \\bigr).\n",
    "$$\n",
    "\n",
    "#### 3.3 Head output and combination  \n",
    "\n",
    "$$\n",
    "Z_{b t_q h j} = \\alpha_{b h t_q t_k} \\, V_{b t_k h j}.\n",
    "$$\n",
    "\n",
    "Concatenate heads as $Z_{b t n}$ with $n = h d_v + j$ and project\n",
    "\n",
    "$$\n",
    "\\tilde Z_{b t m} = Z_{b t n} \\, W^{O}_{n m},\n",
    "\\qquad W^{O} \\in \\mathbb R^{H d_v \\times d_{\\text{model}}}.\n",
    "$$\n",
    "\n",
    "\n",
    "### 4 Position-wise Feed-Forward Network  \n",
    "\n",
    "$$\n",
    "\\tilde X_{b t r} = \\max \\bigl( 0, \\, X_{b t m} W^{(1)}_{m r} + b^{(1)}_{r} \\bigr),\n",
    "$$\n",
    "$$\n",
    "Y_{b t m} = \\tilde X_{b t r} \\, W^{(2)}_{r m} + b^{(2)}_{m},\n",
    "\\qquad\n",
    "W^{(1)} \\in \\mathbb R^{d_{\\text{model}} \\times d_{ff}},\n",
    "\\; W^{(2)} \\in \\mathbb R^{d_{ff} \\times d_{\\text{model}}}.\n",
    "$$\n",
    "\n",
    "\n",
    "### 5 Positional Encoding  \n",
    "\n",
    "For position $t$ and feature $m$,\n",
    "\n",
    "$$\n",
    "\\text{PE}_{t m} =\n",
    "\\begin{cases}\n",
    "\\sin \\bigl( t / 10000^{\\,2m / d_{\\text{model}}} \\bigr) & m \\text{ even}, \\\\[6pt]\n",
    "\\cos \\bigl( t / 10000^{\\,2m / d_{\\text{model}}} \\bigr) & m \\text{ odd}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Add $PE_{t m}$ to token embeddings $E_{t m}$.\n",
    "\n",
    "### 6 Encoder Layer  \n",
    "\n",
    "With input $X^{(l)}_{b t m}$\n",
    "\n",
    "1. Multi-head self-attention  \n",
    "   $$ \\hat Z_{b t m} = \\text{MHA} \\bigl( \\text{LayerNorm}(X^{(l)}) \\bigr)_{b t m}. $$\n",
    "2. Residual add $Z_{b t m} = X^{(l)}_{b t m} + \\hat Z_{b t m}$.  \n",
    "3. Feed-forward  \n",
    "   $$ \\hat Y_{b t m} = \\text{FFN} \\bigl( \\text{LayerNorm}(Z) \\bigr)_{b t m}. $$\n",
    "4. Residual add $X^{(l+1)}_{b t m} = Z_{b t m} + \\hat Y_{b t m}$.\n",
    "\n",
    "Repeat for $l = 0,…,L_E−1$; final output is memory $M_{b t m}$.\n",
    "\n",
    "\n",
    "### 7 Decoder Layer  \n",
    "\n",
    "With decoder input $Y^{(l)}_{b t′ m}$ and memory $M_{b t m}$\n",
    "\n",
    "1. Masked self-attention  \n",
    "   $$ \\hat U = \\text{MHA}_{\\text{masked}} \\bigl( \\text{LayerNorm}(Y^{(l)}) \\bigr). $$\n",
    "2. $U = Y^{(l)} + \\hat U$.  \n",
    "3. Cross-attention  \n",
    "   $$ \\hat V = \\text{MHA} \\bigl( Q = \\text{LayerNorm}(U),\\, K = M,\\, V = M \\bigr). $$\n",
    "4. $V = U + \\hat V$.  \n",
    "5. Feed-forward  \n",
    "   $$ \\hat Y = \\text{FFN} \\bigl( \\text{LayerNorm}(V) \\bigr). $$\n",
    "6. $Y^{(l+1)} = V + \\hat Y$.\n",
    "\n",
    "Repeat for $l = 0,\\ldots,L_D−1$.\n",
    "\n",
    "### 8 End-to-End Forward Pass  \n",
    "\n",
    "1. Encode the source to obtain memory $M_{b t m}$.  \n",
    "2. Decode shifted-right targets with causal masking while attending to $M_{b t m}$.  \n",
    "3. Project decoder states with $W^{\\text{vocab}}_{m v}$ and apply softmax to obtain  \n",
    "   $$ p(y_{t′} \\mid y_{<t′}, x). $$\n",
    "\n",
    "\n",
    "### 9 Training Objective  \n",
    "\n",
    "$$\n",
    "\\mathcal L = -\\sum_{b,\\,t′} \\log p \\bigl( y_{t′}^{(b)} \\mid y_{<t′}^{(b)},\\, x^{(b)} \\bigr),\n",
    "$$\n",
    "\n",
    "minimised with Adam using learning-rate warm-up and inverse-square-root decay.\n",
    "\n",
    "### 10 Parameter and Complexity Summary  \n",
    "\n",
    "| sub-unit | parameters | time per layer | space per layer |\n",
    "|----------|------------|----------------|-----------------|\n",
    "| multi-head attention | $3 d_{\\text{model}} d_k H + d_{\\text{model}} d_v H$ | $O(B T^{2} d_k)$ | $O(B T d_{\\text{model}})$ |\n",
    "| feed-forward | $2 d_{\\text{model}} d_{\\text{FF}}$ | $O(B T d_{\\text{FF}})$ | $O(B T d_{\\text{FF}})$ |\n",
    "| layer norm | $2 d_{\\text{model}}$ | negligible | negligible |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: ChemBERTa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem is based on the [ChemBERTa](https://arxiv.org/pdf/2010.09885) paper.\n",
    "### a)\n",
    "Load the file `250k_rndm_zinc_drugs_clean_3.csv` and set up a word-level tokenizer with a byte-level pre tokenizer. Explain what these mean and why they are needed.\n",
    "### b)\n",
    "Using this tokenizer, pick a nontrivial SMILES string. Print out the string, the tokenization, and use rdkit to display it.\n",
    "### c)\n",
    "Set up a train/test loader using BERT-style masking. Explain why there must be collation.\n",
    "### d)\n",
    "Set up and define a Transformer class, a Masked Transformer class, masking function and optimizers. Explain why a Masked Transformer is needed. Choose a loss function and justify it.\n",
    "### e) \n",
    "Train the BERT-style model for a number of epochs until the loss is consistently less than ~3. Use the model to predict a previously-unseen SMILES string, and print out both the model, the masked, and the predicted. Qualitatively describe what's happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_csv('250k_rndm_zinc_drugs_clean_3.csv')\n",
    "df = df_full.iloc[:10000]\n",
    "smiles = df['smiles'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.WordLevel(unk_token='[UNK]'))\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "trainer = trainers.WordLevelTrainer(vocab_size=1000,\n",
    "    special_tokens=['[PAD]','[UNK]','[CLS]','[SEP]','[MASK]'])\n",
    "tokenizer.train_from_iterator(smiles, trainer=trainer)\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    '[CLS] $A [SEP]', special_tokens=[('[CLS]',1),('[SEP]',2)])\n",
    "tokenizer.enable_truncation(max_length=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Byte-level guarantees that every character is representable. Word-level allows the model to parse more complex strings like \"Cl\",\"=O\" as a single item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cc1occc1C(=O)NC1CCN(C(=O)C(=O)Nc2ccc(F)cc2F)CC1\n",
      "\n",
      "['[CLS]', 'ĠCc', '1', 'occc', '1', 'C', '(=', 'O', ')', 'NC', '1', 'CCN', '(', 'C', '(=', 'O', ')', 'C', '(=', 'O', ')', 'Nc', '2', 'ccc', '(', 'F', ')', 'cc', '2', 'F', ')', 'CC', '1', 'Ċ', '[SEP]']\n",
      "[1, 32, 7, 216, 7, 6, 12, 10, 5, 29, 7, 52, 8, 6, 12, 10, 5, 6, 12, 10, 5, 35, 9, 18, 8, 26, 5, 17, 9, 26, 5, 23, 7, 13, 2]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=300x300>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles0 = smiles[2100]\n",
    "print(smiles0)\n",
    "encoding0 = tokenizer.encode(smiles0)\n",
    "print(encoding0.tokens)\n",
    "print(encoding0.ids)\n",
    "print(encoding0.attention_mask)\n",
    "mol=Chem.MolFromSmiles(smiles0)\n",
    "Draw.MolToImage(mol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = {'input_ids':[], 'attention_mask':[]}\n",
    "for e in tokenizer.encode_batch(smiles): # tokenizes each\n",
    "    enc['input_ids'].append(e.ids)\n",
    "    enc['attention_mask'].append(e.attention_mask)\n",
    "ids = enc['input_ids']\n",
    "masks = enc['attention_mask']\n",
    "# train test split\n",
    "ids_train, ids_test, masks_train, masks_test = train_test_split(\n",
    "    ids, masks, test_size=0.1, random_state=42\n",
    ")\n",
    "# packs into dicts\n",
    "train_enc = {'input_ids': ids_train, 'attention_mask': masks_train}\n",
    "test_enc  = {'input_ids': ids_test,  'attention_mask': masks_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset custom class\n",
    "class SMILESDataset(Dataset):\n",
    "    def __init__(self, enc):\n",
    "        self.ids = enc['input_ids']\n",
    "        self.mask = enc['attention_mask']\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.ids[i], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.mask[i], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # this must exist to pad them all to the same length so they can be stacked and passed into transfomer\n",
    "    ids = [item['input_ids'] for item in batch]\n",
    "    masks = [item['attention_mask'] for item in batch]\n",
    "    pad_id = tokenizer.token_to_id('[PAD]')\n",
    "    ids_padded = pad_sequence(ids, batch_first=True, padding_value=pad_id)\n",
    "    masks_padded = pad_sequence(masks, batch_first=True, padding_value=0)\n",
    "    return {'input_ids': ids_padded, 'attention_mask': masks_padded}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all SMILES strings are the same length, so they must be padded to use standard uniform size tensor frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    SMILESDataset(train_enc),\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    SMILESDataset(test_enc),\n",
    "    batch_size=32,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, mlp_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(d_model, num_heads)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(nn.Linear(d_model, mlp_dim),\n",
    "                                 nn.GELU(),\n",
    "                                 nn.Linear(mlp_dim, d_model))\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "    def forward(self, x, mask):\n",
    "        attn_out, _ = self.attn(x, x, x, key_padding_mask=mask==0)\n",
    "        x = x + attn_out\n",
    "        y = self.ln1(x)\n",
    "        x = x + self.mlp(y)\n",
    "        return self.ln2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=64, num_heads=4,\n",
    "                 mlp_dim=128, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, mlp_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "    def forward(self, input_ids, mask):\n",
    "        x = self.embed(input_ids).transpose(0, 1)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.fc(x).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention mask is a way to tell the Transformer which positions in the input are “real” tokens and which are just padding or otherwise shouldn’t contribute to attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_inputs(ids, mask_token_id, mask_prob=0.15):\n",
    "    labels = ids.clone()\n",
    "    rand = torch.rand(ids.shape)\n",
    "    mask_positions = rand < mask_prob\n",
    "    labels[~mask_positions] = -100\n",
    "    ids[mask_positions] = mask_token_id\n",
    "    return ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.get_vocab_size()\n",
    "model = MaskedTransformer(vocab_size)\n",
    "opt = optim.Adam(model.parameters(), lr=1e-4)\n",
    "crit = nn.CrossEntropyLoss(ignore_index=-100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross entropy is a good choice for multiclass classification where there are probabilistic predictions. It is always positive and is uniquely minimized when fully confident correct answers are given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t batch, loss = 6.604\n",
      "\t batch, loss = 6.601\n",
      "\t batch, loss = 6.576\n",
      "\t batch, loss = 6.525\n",
      "\t batch, loss = 6.443\n",
      "\t batch, loss = 6.402\n",
      "\t batch, loss = 6.405\n",
      "\t batch, loss = 6.319\n",
      "\t batch, loss = 6.276\n",
      "\t batch, loss = 6.280\n",
      "\t batch, loss = 6.198\n",
      "\t batch, loss = 6.273\n",
      "\t batch, loss = 6.195\n",
      "\t batch, loss = 6.073\n",
      "\t batch, loss = 6.038\n",
      "\t batch, loss = 6.067\n",
      "\t batch, loss = 6.057\n",
      "\t batch, loss = 5.842\n",
      "\t batch, loss = 6.023\n",
      "\t batch, loss = 5.902\n",
      "\t batch, loss = 5.951\n",
      "\t batch, loss = 5.935\n",
      "\t batch, loss = 5.783\n",
      "\t batch, loss = 5.842\n",
      "\t batch, loss = 5.779\n",
      "\t batch, loss = 5.724\n",
      "\t batch, loss = 5.604\n",
      "\t batch, loss = 5.780\n",
      "\t batch, loss = 5.871\n",
      "\t batch, loss = 5.274\n",
      "\t batch, loss = 5.747\n",
      "\t batch, loss = 5.668\n",
      "\t batch, loss = 5.705\n",
      "\t batch, loss = 5.406\n",
      "\t batch, loss = 5.473\n",
      "\t batch, loss = 5.162\n",
      "\t batch, loss = 5.528\n",
      "\t batch, loss = 5.342\n",
      "\t batch, loss = 5.199\n",
      "\t batch, loss = 5.176\n",
      "\t batch, loss = 5.117\n",
      "\t batch, loss = 5.092\n",
      "\t batch, loss = 5.262\n",
      "\t batch, loss = 5.274\n",
      "\t batch, loss = 4.990\n",
      "\t batch, loss = 5.146\n",
      "\t batch, loss = 5.214\n",
      "\t batch, loss = 5.312\n",
      "\t batch, loss = 5.056\n",
      "\t batch, loss = 5.127\n",
      "\t batch, loss = 5.001\n",
      "\t batch, loss = 4.616\n",
      "\t batch, loss = 5.041\n",
      "\t batch, loss = 5.214\n",
      "\t batch, loss = 4.577\n",
      "\t batch, loss = 5.048\n",
      "\t batch, loss = 4.845\n",
      "\t batch, loss = 4.953\n",
      "\t batch, loss = 4.799\n",
      "\t batch, loss = 4.860\n",
      "\t batch, loss = 4.992\n",
      "\t batch, loss = 5.318\n",
      "\t batch, loss = 4.656\n",
      "\t batch, loss = 4.630\n",
      "\t batch, loss = 4.762\n",
      "\t batch, loss = 5.016\n",
      "\t batch, loss = 4.624\n",
      "\t batch, loss = 4.916\n",
      "\t batch, loss = 4.805\n",
      "\t batch, loss = 4.709\n",
      "\t batch, loss = 5.011\n",
      "\t batch, loss = 4.909\n",
      "\t batch, loss = 4.870\n",
      "\t batch, loss = 4.330\n",
      "\t batch, loss = 4.752\n",
      "\t batch, loss = 4.655\n",
      "\t batch, loss = 4.902\n",
      "\t batch, loss = 4.485\n",
      "\t batch, loss = 4.549\n",
      "\t batch, loss = 4.691\n",
      "\t batch, loss = 4.504\n",
      "\t batch, loss = 4.165\n",
      "\t batch, loss = 4.658\n",
      "\t batch, loss = 4.715\n",
      "\t batch, loss = 4.733\n",
      "\t batch, loss = 4.026\n",
      "\t batch, loss = 4.382\n",
      "\t batch, loss = 4.648\n",
      "\t batch, loss = 4.486\n",
      "\t batch, loss = 4.755\n",
      "\t batch, loss = 4.630\n",
      "\t batch, loss = 4.444\n",
      "\t batch, loss = 4.359\n",
      "\t batch, loss = 4.754\n",
      "\t batch, loss = 4.488\n",
      "\t batch, loss = 4.317\n",
      "\t batch, loss = 4.205\n",
      "\t batch, loss = 3.910\n",
      "\t batch, loss = 4.287\n",
      "\t batch, loss = 4.274\n",
      "\t batch, loss = 3.834\n",
      "\t batch, loss = 4.549\n",
      "\t batch, loss = 4.787\n",
      "\t batch, loss = 3.986\n",
      "\t batch, loss = 4.475\n",
      "\t batch, loss = 4.234\n",
      "\t batch, loss = 4.360\n",
      "\t batch, loss = 4.321\n",
      "\t batch, loss = 4.073\n",
      "\t batch, loss = 4.631\n",
      "\t batch, loss = 4.048\n",
      "\t batch, loss = 3.811\n",
      "\t batch, loss = 4.227\n",
      "\t batch, loss = 3.845\n",
      "\t batch, loss = 4.297\n",
      "\t batch, loss = 3.883\n",
      "\t batch, loss = 4.829\n",
      "\t batch, loss = 4.476\n",
      "\t batch, loss = 4.457\n",
      "\t batch, loss = 4.051\n",
      "\t batch, loss = 4.268\n",
      "\t batch, loss = 4.430\n",
      "\t batch, loss = 4.822\n",
      "\t batch, loss = 4.004\n",
      "\t batch, loss = 4.542\n",
      "\t batch, loss = 4.298\n",
      "\t batch, loss = 4.391\n",
      "\t batch, loss = 4.252\n",
      "\t batch, loss = 3.960\n",
      "\t batch, loss = 4.033\n",
      "\t batch, loss = 4.474\n",
      "\t batch, loss = 4.094\n",
      "\t batch, loss = 3.729\n",
      "\t batch, loss = 4.068\n",
      "\t batch, loss = 4.039\n",
      "\t batch, loss = 4.323\n",
      "\t batch, loss = 4.139\n",
      "\t batch, loss = 4.279\n",
      "\t batch, loss = 4.067\n",
      "\t batch, loss = 3.983\n",
      "\t batch, loss = 4.479\n",
      "\t batch, loss = 4.141\n",
      "\t batch, loss = 4.357\n",
      "\t batch, loss = 4.076\n",
      "\t batch, loss = 4.072\n",
      "\t batch, loss = 4.224\n",
      "\t batch, loss = 4.062\n",
      "\t batch, loss = 4.330\n",
      "\t batch, loss = 4.110\n",
      "\t batch, loss = 3.748\n",
      "\t batch, loss = 4.145\n",
      "\t batch, loss = 3.495\n",
      "\t batch, loss = 3.766\n",
      "\t batch, loss = 4.099\n",
      "\t batch, loss = 4.057\n",
      "\t batch, loss = 4.167\n",
      "\t batch, loss = 3.787\n",
      "\t batch, loss = 3.920\n",
      "\t batch, loss = 4.073\n",
      "\t batch, loss = 3.835\n",
      "\t batch, loss = 4.484\n",
      "\t batch, loss = 3.972\n",
      "\t batch, loss = 4.266\n",
      "\t batch, loss = 4.050\n",
      "\t batch, loss = 4.026\n",
      "\t batch, loss = 3.670\n",
      "\t batch, loss = 3.877\n",
      "\t batch, loss = 3.925\n",
      "\t batch, loss = 3.723\n",
      "\t batch, loss = 4.126\n",
      "\t batch, loss = 3.871\n",
      "\t batch, loss = 4.215\n",
      "\t batch, loss = 4.043\n",
      "\t batch, loss = 4.133\n",
      "\t batch, loss = 4.179\n",
      "\t batch, loss = 3.612\n",
      "\t batch, loss = 4.017\n",
      "\t batch, loss = 3.404\n",
      "\t batch, loss = 3.341\n",
      "\t batch, loss = 3.928\n",
      "\t batch, loss = 4.322\n",
      "\t batch, loss = 3.908\n",
      "\t batch, loss = 4.110\n",
      "\t batch, loss = 3.832\n",
      "\t batch, loss = 3.972\n",
      "\t batch, loss = 3.743\n",
      "\t batch, loss = 3.628\n",
      "\t batch, loss = 3.586\n",
      "\t batch, loss = 3.949\n",
      "\t batch, loss = 3.602\n",
      "\t batch, loss = 4.006\n",
      "\t batch, loss = 3.628\n",
      "\t batch, loss = 3.890\n",
      "\t batch, loss = 3.819\n",
      "\t batch, loss = 3.579\n",
      "\t batch, loss = 3.760\n",
      "\t batch, loss = 3.589\n",
      "\t batch, loss = 3.876\n",
      "\t batch, loss = 3.792\n",
      "\t batch, loss = 3.886\n",
      "\t batch, loss = 3.994\n",
      "\t batch, loss = 3.961\n",
      "\t batch, loss = 3.995\n",
      "\t batch, loss = 4.270\n",
      "\t batch, loss = 3.681\n",
      "\t batch, loss = 3.888\n",
      "\t batch, loss = 3.856\n",
      "\t batch, loss = 3.703\n",
      "\t batch, loss = 3.927\n",
      "\t batch, loss = 3.841\n",
      "\t batch, loss = 4.036\n",
      "\t batch, loss = 4.043\n",
      "\t batch, loss = 3.548\n",
      "\t batch, loss = 3.761\n",
      "\t batch, loss = 3.602\n",
      "\t batch, loss = 3.636\n",
      "\t batch, loss = 3.862\n",
      "\t batch, loss = 3.872\n",
      "\t batch, loss = 3.695\n",
      "\t batch, loss = 3.977\n",
      "\t batch, loss = 3.293\n",
      "\t batch, loss = 4.106\n",
      "\t batch, loss = 3.497\n",
      "\t batch, loss = 3.548\n",
      "\t batch, loss = 3.366\n",
      "\t batch, loss = 3.917\n",
      "\t batch, loss = 3.441\n",
      "\t batch, loss = 3.926\n",
      "\t batch, loss = 3.326\n",
      "\t batch, loss = 3.822\n",
      "\t batch, loss = 3.283\n",
      "\t batch, loss = 3.372\n",
      "\t batch, loss = 3.665\n",
      "\t batch, loss = 3.792\n",
      "\t batch, loss = 3.882\n",
      "\t batch, loss = 4.001\n",
      "\t batch, loss = 3.464\n",
      "\t batch, loss = 3.853\n",
      "\t batch, loss = 3.784\n",
      "\t batch, loss = 3.477\n",
      "\t batch, loss = 3.523\n",
      "\t batch, loss = 3.717\n",
      "\t batch, loss = 3.809\n",
      "\t batch, loss = 3.571\n",
      "\t batch, loss = 3.695\n",
      "\t batch, loss = 3.796\n",
      "\t batch, loss = 3.401\n",
      "\t batch, loss = 3.475\n",
      "\t batch, loss = 3.550\n",
      "\t batch, loss = 3.387\n",
      "\t batch, loss = 3.714\n",
      "\t batch, loss = 3.492\n",
      "\t batch, loss = 3.598\n",
      "\t batch, loss = 3.607\n",
      "\t batch, loss = 3.607\n",
      "\t batch, loss = 3.683\n",
      "\t batch, loss = 3.439\n",
      "\t batch, loss = 3.043\n",
      "\t batch, loss = 3.303\n",
      "\t batch, loss = 3.687\n",
      "\t batch, loss = 3.390\n",
      "\t batch, loss = 3.669\n",
      "\t batch, loss = 3.077\n",
      "\t batch, loss = 3.608\n",
      "\t batch, loss = 3.750\n",
      "\t batch, loss = 3.466\n",
      "\t batch, loss = 3.395\n",
      "\t batch, loss = 3.795\n",
      "\t batch, loss = 3.293\n",
      "\t batch, loss = 3.587\n",
      "\t batch, loss = 3.707\n",
      "\t batch, loss = 3.763\n",
      "\t batch, loss = 3.615\n",
      "\t batch, loss = 3.904\n",
      "\t batch, loss = 3.546\n",
      "\t batch, loss = 3.570\n",
      "\t batch, loss = 3.216\n",
      "\t batch, loss = 3.668\n",
      "\t batch, loss = 3.435\n",
      "\t batch, loss = 3.613\n",
      "\t batch, loss = 3.310\n",
      "\t batch, loss = 3.603\n",
      "Epoch = 0\n",
      "\t batch, loss = 3.479\n",
      "\t batch, loss = 3.473\n",
      "\t batch, loss = 3.568\n",
      "\t batch, loss = 3.318\n",
      "\t batch, loss = 3.149\n",
      "\t batch, loss = 4.068\n",
      "\t batch, loss = 3.534\n",
      "\t batch, loss = 3.147\n",
      "\t batch, loss = 3.336\n",
      "\t batch, loss = 3.504\n",
      "\t batch, loss = 3.542\n",
      "\t batch, loss = 3.478\n",
      "\t batch, loss = 3.229\n",
      "\t batch, loss = 3.586\n",
      "\t batch, loss = 3.525\n",
      "\t batch, loss = 3.216\n",
      "\t batch, loss = 3.169\n",
      "\t batch, loss = 3.692\n",
      "\t batch, loss = 3.695\n",
      "\t batch, loss = 3.428\n",
      "\t batch, loss = 3.666\n",
      "\t batch, loss = 3.810\n",
      "\t batch, loss = 3.483\n",
      "\t batch, loss = 3.522\n",
      "\t batch, loss = 3.570\n",
      "\t batch, loss = 3.019\n",
      "\t batch, loss = 3.423\n",
      "\t batch, loss = 3.929\n",
      "\t batch, loss = 3.256\n",
      "\t batch, loss = 3.519\n",
      "\t batch, loss = 3.359\n",
      "\t batch, loss = 3.035\n",
      "\t batch, loss = 3.099\n",
      "\t batch, loss = 2.976\n",
      "\t batch, loss = 3.626\n",
      "\t batch, loss = 3.793\n",
      "\t batch, loss = 3.133\n",
      "\t batch, loss = 3.149\n",
      "\t batch, loss = 3.425\n",
      "\t batch, loss = 3.351\n",
      "\t batch, loss = 3.558\n",
      "\t batch, loss = 3.383\n",
      "\t batch, loss = 3.259\n",
      "\t batch, loss = 3.653\n",
      "\t batch, loss = 3.095\n",
      "\t batch, loss = 3.385\n",
      "\t batch, loss = 3.555\n",
      "\t batch, loss = 3.708\n",
      "\t batch, loss = 3.212\n",
      "\t batch, loss = 3.601\n",
      "\t batch, loss = 3.183\n",
      "\t batch, loss = 3.476\n",
      "\t batch, loss = 2.922\n",
      "\t batch, loss = 3.586\n",
      "\t batch, loss = 3.174\n",
      "\t batch, loss = 3.808\n",
      "\t batch, loss = 3.363\n",
      "\t batch, loss = 3.280\n",
      "\t batch, loss = 3.013\n",
      "\t batch, loss = 3.373\n",
      "\t batch, loss = 3.818\n",
      "\t batch, loss = 3.159\n",
      "\t batch, loss = 3.426\n",
      "\t batch, loss = 3.368\n",
      "\t batch, loss = 3.224\n",
      "\t batch, loss = 3.125\n",
      "\t batch, loss = 3.174\n",
      "\t batch, loss = 3.287\n",
      "\t batch, loss = 3.326\n",
      "\t batch, loss = 3.356\n",
      "\t batch, loss = 3.380\n",
      "\t batch, loss = 3.329\n",
      "\t batch, loss = 3.604\n",
      "\t batch, loss = 3.611\n",
      "\t batch, loss = 3.178\n",
      "\t batch, loss = 3.093\n",
      "\t batch, loss = 3.513\n",
      "\t batch, loss = 3.618\n",
      "\t batch, loss = 3.231\n",
      "\t batch, loss = 3.586\n",
      "\t batch, loss = 3.696\n",
      "\t batch, loss = 3.120\n",
      "\t batch, loss = 3.244\n",
      "\t batch, loss = 3.036\n",
      "\t batch, loss = 3.729\n",
      "\t batch, loss = 3.310\n",
      "\t batch, loss = 3.425\n",
      "\t batch, loss = 2.865\n",
      "\t batch, loss = 3.442\n",
      "\t batch, loss = 3.318\n",
      "\t batch, loss = 3.607\n",
      "\t batch, loss = 2.966\n",
      "\t batch, loss = 3.282\n",
      "\t batch, loss = 3.470\n",
      "\t batch, loss = 2.996\n",
      "\t batch, loss = 3.443\n",
      "\t batch, loss = 3.350\n",
      "\t batch, loss = 3.278\n",
      "\t batch, loss = 3.263\n",
      "\t batch, loss = 3.376\n",
      "\t batch, loss = 3.293\n",
      "\t batch, loss = 3.190\n",
      "\t batch, loss = 3.153\n",
      "\t batch, loss = 2.976\n",
      "\t batch, loss = 3.393\n",
      "\t batch, loss = 3.676\n",
      "\t batch, loss = 3.581\n",
      "\t batch, loss = 3.396\n",
      "\t batch, loss = 3.561\n",
      "\t batch, loss = 3.384\n",
      "\t batch, loss = 2.930\n",
      "\t batch, loss = 3.666\n",
      "\t batch, loss = 3.268\n",
      "\t batch, loss = 3.034\n",
      "\t batch, loss = 3.280\n",
      "\t batch, loss = 2.953\n",
      "\t batch, loss = 3.346\n",
      "\t batch, loss = 3.380\n",
      "\t batch, loss = 3.531\n",
      "\t batch, loss = 3.229\n",
      "\t batch, loss = 3.540\n",
      "\t batch, loss = 3.332\n",
      "\t batch, loss = 3.381\n",
      "\t batch, loss = 3.624\n",
      "\t batch, loss = 3.085\n",
      "\t batch, loss = 3.281\n",
      "\t batch, loss = 2.966\n",
      "\t batch, loss = 3.432\n",
      "\t batch, loss = 3.081\n",
      "\t batch, loss = 3.412\n",
      "\t batch, loss = 3.267\n",
      "\t batch, loss = 3.560\n",
      "\t batch, loss = 3.343\n",
      "\t batch, loss = 3.458\n",
      "\t batch, loss = 3.124\n",
      "\t batch, loss = 3.492\n",
      "\t batch, loss = 3.385\n",
      "\t batch, loss = 3.089\n",
      "\t batch, loss = 2.959\n",
      "\t batch, loss = 3.124\n",
      "\t batch, loss = 3.410\n",
      "\t batch, loss = 3.366\n",
      "\t batch, loss = 2.755\n",
      "\t batch, loss = 3.460\n",
      "\t batch, loss = 3.194\n",
      "\t batch, loss = 2.884\n",
      "\t batch, loss = 3.665\n",
      "\t batch, loss = 3.048\n",
      "\t batch, loss = 3.501\n",
      "\t batch, loss = 3.225\n",
      "\t batch, loss = 3.319\n",
      "\t batch, loss = 3.215\n",
      "\t batch, loss = 3.429\n",
      "\t batch, loss = 3.044\n",
      "\t batch, loss = 3.172\n",
      "\t batch, loss = 3.122\n",
      "\t batch, loss = 3.353\n",
      "\t batch, loss = 2.841\n",
      "\t batch, loss = 3.033\n",
      "\t batch, loss = 3.281\n",
      "\t batch, loss = 2.958\n",
      "\t batch, loss = 3.283\n",
      "\t batch, loss = 2.985\n",
      "\t batch, loss = 3.098\n",
      "\t batch, loss = 3.144\n",
      "\t batch, loss = 3.043\n",
      "\t batch, loss = 3.234\n",
      "\t batch, loss = 3.053\n",
      "\t batch, loss = 3.073\n",
      "\t batch, loss = 3.144\n",
      "\t batch, loss = 3.045\n",
      "\t batch, loss = 2.651\n",
      "\t batch, loss = 2.995\n",
      "\t batch, loss = 3.142\n",
      "\t batch, loss = 3.303\n",
      "\t batch, loss = 3.259\n",
      "\t batch, loss = 2.884\n",
      "\t batch, loss = 3.251\n",
      "\t batch, loss = 3.101\n",
      "\t batch, loss = 3.029\n",
      "\t batch, loss = 3.168\n",
      "\t batch, loss = 2.884\n",
      "\t batch, loss = 3.168\n",
      "\t batch, loss = 3.630\n",
      "\t batch, loss = 3.306\n",
      "\t batch, loss = 3.206\n",
      "\t batch, loss = 3.287\n",
      "\t batch, loss = 3.057\n",
      "\t batch, loss = 2.959\n",
      "\t batch, loss = 3.200\n",
      "\t batch, loss = 3.530\n",
      "\t batch, loss = 2.764\n",
      "\t batch, loss = 2.873\n",
      "\t batch, loss = 3.286\n",
      "\t batch, loss = 3.044\n",
      "\t batch, loss = 2.982\n",
      "\t batch, loss = 3.671\n",
      "\t batch, loss = 3.314\n",
      "\t batch, loss = 3.193\n",
      "\t batch, loss = 3.301\n",
      "\t batch, loss = 3.474\n",
      "\t batch, loss = 3.235\n",
      "\t batch, loss = 3.248\n",
      "\t batch, loss = 3.340\n",
      "\t batch, loss = 3.310\n",
      "\t batch, loss = 3.034\n",
      "\t batch, loss = 3.573\n",
      "\t batch, loss = 2.992\n",
      "\t batch, loss = 3.159\n",
      "\t batch, loss = 3.217\n",
      "\t batch, loss = 3.159\n",
      "\t batch, loss = 2.862\n",
      "\t batch, loss = 2.902\n",
      "\t batch, loss = 3.267\n",
      "\t batch, loss = 3.140\n",
      "\t batch, loss = 2.888\n",
      "\t batch, loss = 3.238\n",
      "\t batch, loss = 3.610\n",
      "\t batch, loss = 3.255\n",
      "\t batch, loss = 2.865\n",
      "\t batch, loss = 3.065\n",
      "\t batch, loss = 3.077\n",
      "\t batch, loss = 3.297\n",
      "\t batch, loss = 3.606\n",
      "\t batch, loss = 3.285\n",
      "\t batch, loss = 3.543\n",
      "\t batch, loss = 2.960\n",
      "\t batch, loss = 2.580\n",
      "\t batch, loss = 3.304\n",
      "\t batch, loss = 2.831\n",
      "\t batch, loss = 3.257\n",
      "\t batch, loss = 3.421\n",
      "\t batch, loss = 3.083\n",
      "\t batch, loss = 3.336\n",
      "\t batch, loss = 3.540\n",
      "\t batch, loss = 3.250\n",
      "\t batch, loss = 3.090\n",
      "\t batch, loss = 3.552\n",
      "\t batch, loss = 2.994\n",
      "\t batch, loss = 3.153\n",
      "\t batch, loss = 3.169\n",
      "\t batch, loss = 3.229\n",
      "\t batch, loss = 3.326\n",
      "\t batch, loss = 3.071\n",
      "\t batch, loss = 3.335\n",
      "\t batch, loss = 3.322\n",
      "\t batch, loss = 3.478\n",
      "\t batch, loss = 2.777\n",
      "\t batch, loss = 3.011\n",
      "\t batch, loss = 3.342\n",
      "\t batch, loss = 3.286\n",
      "\t batch, loss = 2.902\n",
      "\t batch, loss = 2.631\n",
      "\t batch, loss = 2.978\n",
      "\t batch, loss = 3.251\n",
      "\t batch, loss = 3.203\n",
      "\t batch, loss = 3.095\n",
      "\t batch, loss = 3.278\n",
      "\t batch, loss = 3.397\n",
      "\t batch, loss = 3.270\n",
      "\t batch, loss = 3.157\n",
      "\t batch, loss = 3.081\n",
      "\t batch, loss = 2.999\n",
      "\t batch, loss = 3.040\n",
      "\t batch, loss = 3.281\n",
      "\t batch, loss = 3.145\n",
      "\t batch, loss = 2.996\n",
      "\t batch, loss = 2.913\n",
      "\t batch, loss = 3.039\n",
      "\t batch, loss = 3.122\n",
      "\t batch, loss = 3.147\n",
      "\t batch, loss = 3.508\n",
      "\t batch, loss = 3.118\n",
      "\t batch, loss = 2.950\n",
      "\t batch, loss = 3.246\n",
      "\t batch, loss = 3.009\n",
      "\t batch, loss = 2.995\n",
      "\t batch, loss = 3.146\n",
      "\t batch, loss = 3.132\n",
      "\t batch, loss = 3.190\n",
      "\t batch, loss = 3.149\n",
      "\t batch, loss = 3.427\n",
      "Epoch = 1\n",
      "\t batch, loss = 2.704\n",
      "\t batch, loss = 3.318\n",
      "\t batch, loss = 3.049\n",
      "\t batch, loss = 3.244\n",
      "\t batch, loss = 3.248\n",
      "\t batch, loss = 3.237\n",
      "\t batch, loss = 3.273\n",
      "\t batch, loss = 3.112\n",
      "\t batch, loss = 3.444\n",
      "\t batch, loss = 3.237\n",
      "\t batch, loss = 3.421\n",
      "\t batch, loss = 3.219\n",
      "\t batch, loss = 3.099\n",
      "\t batch, loss = 3.245\n",
      "\t batch, loss = 3.210\n",
      "\t batch, loss = 2.949\n",
      "\t batch, loss = 3.115\n",
      "\t batch, loss = 3.107\n",
      "\t batch, loss = 3.343\n",
      "\t batch, loss = 3.307\n",
      "\t batch, loss = 3.217\n",
      "\t batch, loss = 3.198\n",
      "\t batch, loss = 2.991\n",
      "\t batch, loss = 3.046\n",
      "\t batch, loss = 3.254\n",
      "\t batch, loss = 2.822\n",
      "\t batch, loss = 3.448\n",
      "\t batch, loss = 3.273\n",
      "\t batch, loss = 2.709\n",
      "\t batch, loss = 3.155\n",
      "\t batch, loss = 3.039\n",
      "\t batch, loss = 3.213\n",
      "\t batch, loss = 3.145\n",
      "\t batch, loss = 3.057\n",
      "\t batch, loss = 3.027\n",
      "\t batch, loss = 3.119\n",
      "\t batch, loss = 3.318\n",
      "\t batch, loss = 3.285\n",
      "\t batch, loss = 3.011\n",
      "\t batch, loss = 3.290\n",
      "\t batch, loss = 3.190\n",
      "\t batch, loss = 3.219\n",
      "\t batch, loss = 3.039\n",
      "\t batch, loss = 3.207\n",
      "\t batch, loss = 3.093\n",
      "\t batch, loss = 3.068\n",
      "\t batch, loss = 2.918\n",
      "\t batch, loss = 2.961\n",
      "\t batch, loss = 3.078\n",
      "\t batch, loss = 3.036\n",
      "\t batch, loss = 3.182\n",
      "\t batch, loss = 3.142\n",
      "\t batch, loss = 2.811\n",
      "\t batch, loss = 2.908\n",
      "\t batch, loss = 3.446\n",
      "\t batch, loss = 2.947\n",
      "\t batch, loss = 2.860\n",
      "\t batch, loss = 3.107\n",
      "\t batch, loss = 3.336\n",
      "\t batch, loss = 3.500\n",
      "\t batch, loss = 3.400\n",
      "\t batch, loss = 3.199\n",
      "\t batch, loss = 3.187\n",
      "\t batch, loss = 3.197\n",
      "\t batch, loss = 2.686\n",
      "\t batch, loss = 3.282\n",
      "\t batch, loss = 3.626\n",
      "\t batch, loss = 3.505\n",
      "\t batch, loss = 2.572\n",
      "\t batch, loss = 2.753\n",
      "\t batch, loss = 3.051\n",
      "\t batch, loss = 3.489\n",
      "\t batch, loss = 3.198\n",
      "\t batch, loss = 2.813\n",
      "\t batch, loss = 2.942\n",
      "\t batch, loss = 3.312\n",
      "\t batch, loss = 2.824\n",
      "\t batch, loss = 3.254\n",
      "\t batch, loss = 3.242\n",
      "\t batch, loss = 3.427\n",
      "\t batch, loss = 3.232\n",
      "\t batch, loss = 2.891\n",
      "\t batch, loss = 3.168\n",
      "\t batch, loss = 2.919\n",
      "\t batch, loss = 3.286\n",
      "\t batch, loss = 2.989\n",
      "\t batch, loss = 2.976\n",
      "\t batch, loss = 3.140\n",
      "\t batch, loss = 3.059\n",
      "\t batch, loss = 2.903\n",
      "\t batch, loss = 2.906\n",
      "\t batch, loss = 3.282\n",
      "\t batch, loss = 3.521\n",
      "\t batch, loss = 3.413\n",
      "\t batch, loss = 3.142\n",
      "\t batch, loss = 3.424\n",
      "\t batch, loss = 3.149\n",
      "\t batch, loss = 3.020\n",
      "\t batch, loss = 2.939\n",
      "\t batch, loss = 3.253\n",
      "\t batch, loss = 3.001\n",
      "\t batch, loss = 2.921\n",
      "\t batch, loss = 2.993\n",
      "\t batch, loss = 3.379\n",
      "\t batch, loss = 3.037\n",
      "\t batch, loss = 3.042\n",
      "\t batch, loss = 2.902\n",
      "\t batch, loss = 3.108\n",
      "\t batch, loss = 3.105\n",
      "\t batch, loss = 3.294\n",
      "\t batch, loss = 2.932\n",
      "\t batch, loss = 3.204\n",
      "\t batch, loss = 2.986\n",
      "\t batch, loss = 3.100\n",
      "\t batch, loss = 3.200\n",
      "\t batch, loss = 2.815\n",
      "\t batch, loss = 2.855\n",
      "\t batch, loss = 2.826\n",
      "\t batch, loss = 3.179\n",
      "\t batch, loss = 3.010\n",
      "\t batch, loss = 2.770\n",
      "\t batch, loss = 2.862\n",
      "\t batch, loss = 3.075\n",
      "\t batch, loss = 3.051\n",
      "\t batch, loss = 3.142\n",
      "\t batch, loss = 3.015\n",
      "\t batch, loss = 2.975\n",
      "\t batch, loss = 3.133\n",
      "\t batch, loss = 3.040\n",
      "\t batch, loss = 3.359\n",
      "\t batch, loss = 3.294\n",
      "\t batch, loss = 2.989\n",
      "\t batch, loss = 3.036\n",
      "\t batch, loss = 2.690\n",
      "\t batch, loss = 3.505\n",
      "\t batch, loss = 3.445\n",
      "\t batch, loss = 3.431\n",
      "\t batch, loss = 2.708\n",
      "\t batch, loss = 3.195\n",
      "\t batch, loss = 3.171\n",
      "\t batch, loss = 2.987\n",
      "\t batch, loss = 3.058\n",
      "\t batch, loss = 3.221\n",
      "\t batch, loss = 3.115\n",
      "\t batch, loss = 3.270\n",
      "\t batch, loss = 3.343\n",
      "\t batch, loss = 3.082\n",
      "\t batch, loss = 3.240\n",
      "\t batch, loss = 3.117\n",
      "\t batch, loss = 2.963\n",
      "\t batch, loss = 2.765\n",
      "\t batch, loss = 2.822\n",
      "\t batch, loss = 2.980\n",
      "\t batch, loss = 3.300\n",
      "\t batch, loss = 3.182\n",
      "\t batch, loss = 3.243\n",
      "\t batch, loss = 3.301\n",
      "\t batch, loss = 3.199\n",
      "\t batch, loss = 2.894\n",
      "\t batch, loss = 3.154\n",
      "\t batch, loss = 2.784\n",
      "\t batch, loss = 2.782\n",
      "\t batch, loss = 3.268\n",
      "\t batch, loss = 3.205\n",
      "\t batch, loss = 3.213\n",
      "\t batch, loss = 2.644\n",
      "\t batch, loss = 3.073\n",
      "\t batch, loss = 3.384\n",
      "\t batch, loss = 3.351\n",
      "\t batch, loss = 3.480\n",
      "\t batch, loss = 2.900\n",
      "\t batch, loss = 3.217\n",
      "\t batch, loss = 2.915\n",
      "\t batch, loss = 3.092\n",
      "\t batch, loss = 3.433\n",
      "\t batch, loss = 2.724\n",
      "\t batch, loss = 2.982\n",
      "\t batch, loss = 2.800\n",
      "\t batch, loss = 3.157\n",
      "\t batch, loss = 3.345\n",
      "\t batch, loss = 3.066\n",
      "\t batch, loss = 2.943\n",
      "\t batch, loss = 3.380\n",
      "\t batch, loss = 3.046\n",
      "\t batch, loss = 2.731\n",
      "\t batch, loss = 2.950\n",
      "\t batch, loss = 2.818\n",
      "\t batch, loss = 3.242\n",
      "\t batch, loss = 2.815\n",
      "\t batch, loss = 3.145\n",
      "\t batch, loss = 2.839\n",
      "\t batch, loss = 3.025\n",
      "\t batch, loss = 3.056\n",
      "\t batch, loss = 3.107\n",
      "\t batch, loss = 2.685\n",
      "\t batch, loss = 3.102\n",
      "\t batch, loss = 3.043\n",
      "\t batch, loss = 3.255\n",
      "\t batch, loss = 3.146\n",
      "\t batch, loss = 3.079\n",
      "\t batch, loss = 3.193\n",
      "\t batch, loss = 3.095\n",
      "\t batch, loss = 3.150\n",
      "\t batch, loss = 2.610\n",
      "\t batch, loss = 2.919\n",
      "\t batch, loss = 2.940\n",
      "\t batch, loss = 2.793\n",
      "\t batch, loss = 3.032\n",
      "\t batch, loss = 3.167\n",
      "\t batch, loss = 3.293\n",
      "\t batch, loss = 3.144\n",
      "\t batch, loss = 3.041\n",
      "\t batch, loss = 3.089\n",
      "\t batch, loss = 3.455\n",
      "\t batch, loss = 3.086\n",
      "\t batch, loss = 3.040\n",
      "\t batch, loss = 2.788\n",
      "\t batch, loss = 3.277\n",
      "\t batch, loss = 3.117\n",
      "\t batch, loss = 3.114\n",
      "\t batch, loss = 2.983\n",
      "\t batch, loss = 2.912\n",
      "\t batch, loss = 2.955\n",
      "\t batch, loss = 2.927\n",
      "\t batch, loss = 3.321\n",
      "\t batch, loss = 2.776\n",
      "\t batch, loss = 3.381\n",
      "\t batch, loss = 2.983\n",
      "\t batch, loss = 3.130\n",
      "\t batch, loss = 2.726\n",
      "\t batch, loss = 2.759\n",
      "\t batch, loss = 3.203\n",
      "\t batch, loss = 2.813\n",
      "\t batch, loss = 3.264\n",
      "\t batch, loss = 3.168\n",
      "\t batch, loss = 3.496\n",
      "\t batch, loss = 3.134\n",
      "\t batch, loss = 2.888\n",
      "\t batch, loss = 3.148\n",
      "\t batch, loss = 2.620\n",
      "\t batch, loss = 2.612\n",
      "\t batch, loss = 3.390\n",
      "\t batch, loss = 3.194\n",
      "\t batch, loss = 3.039\n",
      "\t batch, loss = 2.544\n",
      "\t batch, loss = 3.037\n",
      "\t batch, loss = 2.963\n",
      "\t batch, loss = 3.309\n",
      "\t batch, loss = 3.305\n",
      "\t batch, loss = 2.834\n",
      "\t batch, loss = 3.014\n",
      "\t batch, loss = 3.366\n",
      "\t batch, loss = 3.062\n",
      "\t batch, loss = 3.179\n",
      "\t batch, loss = 3.155\n",
      "\t batch, loss = 2.745\n",
      "\t batch, loss = 3.372\n",
      "\t batch, loss = 3.012\n",
      "\t batch, loss = 3.248\n",
      "\t batch, loss = 3.201\n",
      "\t batch, loss = 3.156\n",
      "\t batch, loss = 3.162\n",
      "\t batch, loss = 3.090\n",
      "\t batch, loss = 3.044\n",
      "\t batch, loss = 2.887\n",
      "\t batch, loss = 2.741\n",
      "\t batch, loss = 3.198\n",
      "\t batch, loss = 2.943\n",
      "\t batch, loss = 3.095\n",
      "\t batch, loss = 2.916\n",
      "\t batch, loss = 2.994\n",
      "\t batch, loss = 3.177\n",
      "\t batch, loss = 2.789\n",
      "\t batch, loss = 3.003\n",
      "\t batch, loss = 2.670\n",
      "\t batch, loss = 2.539\n",
      "\t batch, loss = 3.193\n",
      "\t batch, loss = 2.768\n",
      "\t batch, loss = 2.990\n",
      "\t batch, loss = 3.231\n",
      "\t batch, loss = 3.015\n",
      "\t batch, loss = 3.688\n",
      "Epoch = 2\n",
      "\t batch, loss = 2.808\n",
      "\t batch, loss = 3.073\n",
      "\t batch, loss = 3.242\n",
      "\t batch, loss = 3.356\n",
      "\t batch, loss = 3.123\n",
      "\t batch, loss = 2.918\n",
      "\t batch, loss = 3.324\n",
      "\t batch, loss = 3.511\n",
      "\t batch, loss = 2.959\n",
      "\t batch, loss = 3.238\n",
      "\t batch, loss = 2.882\n",
      "\t batch, loss = 3.136\n",
      "\t batch, loss = 3.089\n",
      "\t batch, loss = 2.931\n",
      "\t batch, loss = 3.122\n",
      "\t batch, loss = 3.198\n",
      "\t batch, loss = 2.903\n",
      "\t batch, loss = 3.412\n",
      "\t batch, loss = 2.877\n",
      "\t batch, loss = 2.719\n",
      "\t batch, loss = 3.175\n",
      "\t batch, loss = 2.872\n",
      "\t batch, loss = 3.038\n",
      "\t batch, loss = 2.579\n",
      "\t batch, loss = 2.924\n",
      "\t batch, loss = 3.129\n",
      "\t batch, loss = 2.974\n",
      "\t batch, loss = 3.367\n",
      "\t batch, loss = 2.952\n",
      "\t batch, loss = 3.180\n",
      "\t batch, loss = 2.976\n",
      "\t batch, loss = 2.970\n",
      "\t batch, loss = 2.833\n",
      "\t batch, loss = 2.562\n",
      "\t batch, loss = 2.891\n",
      "\t batch, loss = 2.900\n",
      "\t batch, loss = 2.922\n",
      "\t batch, loss = 3.194\n",
      "\t batch, loss = 2.966\n",
      "\t batch, loss = 3.117\n",
      "\t batch, loss = 2.928\n",
      "\t batch, loss = 3.025\n",
      "\t batch, loss = 2.993\n",
      "\t batch, loss = 3.244\n",
      "\t batch, loss = 3.191\n",
      "\t batch, loss = 2.921\n",
      "\t batch, loss = 3.352\n",
      "\t batch, loss = 2.982\n",
      "\t batch, loss = 3.449\n",
      "\t batch, loss = 3.199\n",
      "\t batch, loss = 2.829\n",
      "\t batch, loss = 2.770\n",
      "\t batch, loss = 3.140\n",
      "\t batch, loss = 3.212\n",
      "\t batch, loss = 2.937\n",
      "\t batch, loss = 3.121\n",
      "\t batch, loss = 2.967\n",
      "\t batch, loss = 3.088\n",
      "\t batch, loss = 3.186\n",
      "\t batch, loss = 3.243\n",
      "\t batch, loss = 3.137\n",
      "\t batch, loss = 3.026\n",
      "\t batch, loss = 2.905\n",
      "\t batch, loss = 2.624\n",
      "\t batch, loss = 3.203\n",
      "\t batch, loss = 2.768\n",
      "\t batch, loss = 3.298\n",
      "\t batch, loss = 2.998\n",
      "\t batch, loss = 2.854\n",
      "\t batch, loss = 2.805\n",
      "\t batch, loss = 2.899\n",
      "\t batch, loss = 2.961\n",
      "\t batch, loss = 3.051\n",
      "\t batch, loss = 3.513\n",
      "\t batch, loss = 3.018\n",
      "\t batch, loss = 3.075\n",
      "\t batch, loss = 3.102\n",
      "\t batch, loss = 2.845\n",
      "\t batch, loss = 3.181\n",
      "\t batch, loss = 2.840\n",
      "\t batch, loss = 2.848\n",
      "\t batch, loss = 3.215\n",
      "\t batch, loss = 3.119\n",
      "\t batch, loss = 2.547\n",
      "\t batch, loss = 2.964\n",
      "\t batch, loss = 3.020\n",
      "\t batch, loss = 2.594\n",
      "\t batch, loss = 2.715\n",
      "\t batch, loss = 2.856\n",
      "\t batch, loss = 3.544\n",
      "\t batch, loss = 3.049\n",
      "\t batch, loss = 2.803\n",
      "\t batch, loss = 3.171\n",
      "\t batch, loss = 3.075\n",
      "\t batch, loss = 2.937\n",
      "\t batch, loss = 3.067\n",
      "\t batch, loss = 2.838\n",
      "\t batch, loss = 3.160\n",
      "\t batch, loss = 2.879\n",
      "\t batch, loss = 3.099\n",
      "\t batch, loss = 2.963\n",
      "\t batch, loss = 2.933\n",
      "\t batch, loss = 2.968\n",
      "\t batch, loss = 2.953\n",
      "\t batch, loss = 3.153\n",
      "\t batch, loss = 3.089\n",
      "\t batch, loss = 3.182\n",
      "\t batch, loss = 3.031\n",
      "\t batch, loss = 3.015\n",
      "\t batch, loss = 3.356\n",
      "\t batch, loss = 2.633\n",
      "\t batch, loss = 3.419\n",
      "\t batch, loss = 2.954\n",
      "\t batch, loss = 2.795\n",
      "\t batch, loss = 3.287\n",
      "\t batch, loss = 3.135\n",
      "\t batch, loss = 2.569\n",
      "\t batch, loss = 3.315\n",
      "\t batch, loss = 2.864\n",
      "\t batch, loss = 3.146\n",
      "\t batch, loss = 2.649\n",
      "\t batch, loss = 2.949\n",
      "\t batch, loss = 2.965\n",
      "\t batch, loss = 3.160\n",
      "\t batch, loss = 2.900\n",
      "\t batch, loss = 3.158\n",
      "\t batch, loss = 3.106\n",
      "\t batch, loss = 3.186\n",
      "\t batch, loss = 2.951\n",
      "\t batch, loss = 3.271\n",
      "\t batch, loss = 3.441\n",
      "\t batch, loss = 2.615\n",
      "\t batch, loss = 2.819\n",
      "\t batch, loss = 3.345\n",
      "\t batch, loss = 2.793\n",
      "\t batch, loss = 2.814\n",
      "\t batch, loss = 3.046\n",
      "\t batch, loss = 3.098\n",
      "\t batch, loss = 3.227\n",
      "\t batch, loss = 3.221\n",
      "\t batch, loss = 3.072\n",
      "\t batch, loss = 3.146\n",
      "\t batch, loss = 3.349\n",
      "\t batch, loss = 2.807\n",
      "\t batch, loss = 3.166\n",
      "\t batch, loss = 3.134\n",
      "\t batch, loss = 2.672\n",
      "\t batch, loss = 2.935\n",
      "\t batch, loss = 3.088\n",
      "\t batch, loss = 2.953\n",
      "\t batch, loss = 2.790\n",
      "\t batch, loss = 2.962\n",
      "\t batch, loss = 2.821\n",
      "\t batch, loss = 3.347\n",
      "\t batch, loss = 2.689\n",
      "\t batch, loss = 2.792\n",
      "\t batch, loss = 3.340\n",
      "\t batch, loss = 3.263\n",
      "\t batch, loss = 2.772\n",
      "\t batch, loss = 3.133\n",
      "\t batch, loss = 3.171\n",
      "\t batch, loss = 3.120\n",
      "\t batch, loss = 2.892\n",
      "\t batch, loss = 3.258\n",
      "\t batch, loss = 2.867\n",
      "\t batch, loss = 3.101\n",
      "\t batch, loss = 3.294\n",
      "\t batch, loss = 2.715\n",
      "\t batch, loss = 2.856\n",
      "\t batch, loss = 2.869\n",
      "\t batch, loss = 3.372\n",
      "\t batch, loss = 2.855\n",
      "\t batch, loss = 2.852\n",
      "\t batch, loss = 2.746\n",
      "\t batch, loss = 2.560\n",
      "\t batch, loss = 3.062\n",
      "\t batch, loss = 3.262\n",
      "\t batch, loss = 3.352\n",
      "\t batch, loss = 3.110\n",
      "\t batch, loss = 3.180\n",
      "\t batch, loss = 2.502\n",
      "\t batch, loss = 2.928\n",
      "\t batch, loss = 3.177\n",
      "\t batch, loss = 2.980\n",
      "\t batch, loss = 2.837\n",
      "\t batch, loss = 3.271\n",
      "\t batch, loss = 2.818\n",
      "\t batch, loss = 3.287\n",
      "\t batch, loss = 3.041\n",
      "\t batch, loss = 2.884\n",
      "\t batch, loss = 2.581\n",
      "\t batch, loss = 3.046\n",
      "\t batch, loss = 2.702\n",
      "\t batch, loss = 3.051\n",
      "\t batch, loss = 3.114\n",
      "\t batch, loss = 2.973\n",
      "\t batch, loss = 3.080\n",
      "\t batch, loss = 3.070\n",
      "\t batch, loss = 3.232\n",
      "\t batch, loss = 3.321\n",
      "\t batch, loss = 3.227\n",
      "\t batch, loss = 2.944\n",
      "\t batch, loss = 3.118\n",
      "\t batch, loss = 3.335\n",
      "\t batch, loss = 2.900\n",
      "\t batch, loss = 3.192\n",
      "\t batch, loss = 2.551\n",
      "\t batch, loss = 2.870\n",
      "\t batch, loss = 2.805\n",
      "\t batch, loss = 2.945\n",
      "\t batch, loss = 3.237\n",
      "\t batch, loss = 3.607\n",
      "\t batch, loss = 3.063\n",
      "\t batch, loss = 3.003\n",
      "\t batch, loss = 2.721\n",
      "\t batch, loss = 3.167\n",
      "\t batch, loss = 2.907\n",
      "\t batch, loss = 3.008\n",
      "\t batch, loss = 3.138\n",
      "\t batch, loss = 3.493\n",
      "\t batch, loss = 2.884\n",
      "\t batch, loss = 2.954\n",
      "\t batch, loss = 3.182\n",
      "\t batch, loss = 3.036\n",
      "\t batch, loss = 2.791\n",
      "\t batch, loss = 3.215\n",
      "\t batch, loss = 3.067\n",
      "\t batch, loss = 3.059\n",
      "\t batch, loss = 2.668\n",
      "\t batch, loss = 2.855\n",
      "\t batch, loss = 2.906\n",
      "\t batch, loss = 3.019\n",
      "\t batch, loss = 2.826\n",
      "\t batch, loss = 3.097\n",
      "\t batch, loss = 2.779\n",
      "\t batch, loss = 3.188\n",
      "\t batch, loss = 3.092\n",
      "\t batch, loss = 2.930\n",
      "\t batch, loss = 2.811\n",
      "\t batch, loss = 3.532\n",
      "\t batch, loss = 3.064\n",
      "\t batch, loss = 2.945\n",
      "\t batch, loss = 3.037\n",
      "\t batch, loss = 3.085\n",
      "\t batch, loss = 3.078\n",
      "\t batch, loss = 3.030\n",
      "\t batch, loss = 3.061\n",
      "\t batch, loss = 3.046\n",
      "\t batch, loss = 2.911\n",
      "\t batch, loss = 3.028\n",
      "\t batch, loss = 2.334\n",
      "\t batch, loss = 3.136\n",
      "\t batch, loss = 2.752\n",
      "\t batch, loss = 3.211\n",
      "\t batch, loss = 2.508\n",
      "\t batch, loss = 2.816\n",
      "\t batch, loss = 2.884\n",
      "\t batch, loss = 3.068\n",
      "\t batch, loss = 2.845\n",
      "\t batch, loss = 3.157\n",
      "\t batch, loss = 3.015\n",
      "\t batch, loss = 3.035\n",
      "\t batch, loss = 3.407\n",
      "\t batch, loss = 2.852\n",
      "\t batch, loss = 3.227\n",
      "\t batch, loss = 3.156\n",
      "\t batch, loss = 3.030\n",
      "\t batch, loss = 3.147\n",
      "\t batch, loss = 2.701\n",
      "\t batch, loss = 3.177\n",
      "\t batch, loss = 2.944\n",
      "\t batch, loss = 3.123\n",
      "\t batch, loss = 2.867\n",
      "\t batch, loss = 2.689\n",
      "\t batch, loss = 3.292\n",
      "\t batch, loss = 2.853\n",
      "\t batch, loss = 3.075\n",
      "\t batch, loss = 2.779\n",
      "\t batch, loss = 2.674\n",
      "\t batch, loss = 2.697\n",
      "\t batch, loss = 3.254\n",
      "\t batch, loss = 3.291\n",
      "Epoch = 3\n",
      "\t batch, loss = 3.365\n",
      "\t batch, loss = 3.194\n",
      "\t batch, loss = 3.138\n",
      "\t batch, loss = 2.829\n",
      "\t batch, loss = 3.036\n",
      "\t batch, loss = 2.909\n",
      "\t batch, loss = 3.296\n",
      "\t batch, loss = 2.974\n",
      "\t batch, loss = 3.172\n",
      "\t batch, loss = 3.122\n",
      "\t batch, loss = 2.952\n",
      "\t batch, loss = 3.002\n",
      "\t batch, loss = 3.134\n",
      "\t batch, loss = 3.322\n",
      "\t batch, loss = 2.893\n",
      "\t batch, loss = 2.946\n",
      "\t batch, loss = 2.920\n",
      "\t batch, loss = 3.089\n",
      "\t batch, loss = 3.098\n",
      "\t batch, loss = 2.891\n",
      "\t batch, loss = 3.108\n",
      "\t batch, loss = 3.032\n",
      "\t batch, loss = 3.303\n",
      "\t batch, loss = 3.177\n",
      "\t batch, loss = 2.901\n",
      "\t batch, loss = 2.785\n",
      "\t batch, loss = 2.773\n",
      "\t batch, loss = 3.163\n",
      "\t batch, loss = 3.168\n",
      "\t batch, loss = 2.816\n",
      "\t batch, loss = 2.995\n",
      "\t batch, loss = 2.874\n",
      "\t batch, loss = 3.014\n",
      "\t batch, loss = 3.009\n",
      "\t batch, loss = 3.176\n",
      "\t batch, loss = 3.073\n",
      "\t batch, loss = 3.116\n",
      "\t batch, loss = 2.647\n",
      "\t batch, loss = 2.791\n",
      "\t batch, loss = 3.060\n",
      "\t batch, loss = 2.981\n",
      "\t batch, loss = 2.872\n",
      "\t batch, loss = 2.966\n",
      "\t batch, loss = 3.263\n",
      "\t batch, loss = 2.859\n",
      "\t batch, loss = 3.399\n",
      "\t batch, loss = 2.928\n",
      "\t batch, loss = 3.032\n",
      "\t batch, loss = 2.916\n",
      "\t batch, loss = 2.733\n",
      "\t batch, loss = 3.213\n",
      "\t batch, loss = 2.639\n",
      "\t batch, loss = 3.193\n",
      "\t batch, loss = 2.731\n",
      "\t batch, loss = 2.898\n",
      "\t batch, loss = 2.976\n",
      "\t batch, loss = 3.061\n",
      "\t batch, loss = 3.167\n",
      "\t batch, loss = 2.987\n",
      "\t batch, loss = 2.984\n",
      "\t batch, loss = 2.754\n",
      "\t batch, loss = 3.319\n",
      "\t batch, loss = 3.050\n",
      "\t batch, loss = 3.078\n",
      "\t batch, loss = 2.875\n",
      "\t batch, loss = 3.066\n",
      "\t batch, loss = 3.152\n",
      "\t batch, loss = 2.894\n",
      "\t batch, loss = 2.875\n",
      "\t batch, loss = 3.219\n",
      "\t batch, loss = 2.607\n",
      "\t batch, loss = 2.847\n",
      "\t batch, loss = 2.999\n",
      "\t batch, loss = 2.964\n",
      "\t batch, loss = 3.042\n",
      "\t batch, loss = 3.300\n",
      "\t batch, loss = 3.143\n",
      "\t batch, loss = 2.813\n",
      "\t batch, loss = 2.790\n",
      "\t batch, loss = 2.931\n",
      "\t batch, loss = 3.123\n",
      "\t batch, loss = 2.840\n",
      "\t batch, loss = 3.232\n",
      "\t batch, loss = 3.026\n",
      "\t batch, loss = 2.996\n",
      "\t batch, loss = 3.189\n",
      "\t batch, loss = 2.751\n",
      "\t batch, loss = 3.187\n",
      "\t batch, loss = 2.685\n",
      "\t batch, loss = 3.020\n",
      "\t batch, loss = 2.809\n",
      "\t batch, loss = 2.980\n",
      "\t batch, loss = 2.847\n",
      "\t batch, loss = 3.172\n",
      "\t batch, loss = 3.270\n",
      "\t batch, loss = 2.904\n",
      "\t batch, loss = 3.011\n",
      "\t batch, loss = 2.926\n",
      "\t batch, loss = 2.808\n",
      "\t batch, loss = 3.262\n",
      "\t batch, loss = 2.744\n",
      "\t batch, loss = 3.276\n",
      "\t batch, loss = 2.682\n",
      "\t batch, loss = 2.878\n",
      "\t batch, loss = 2.917\n",
      "\t batch, loss = 2.927\n",
      "\t batch, loss = 3.178\n",
      "\t batch, loss = 2.989\n",
      "\t batch, loss = 3.066\n",
      "\t batch, loss = 2.971\n",
      "\t batch, loss = 3.078\n",
      "\t batch, loss = 3.055\n",
      "\t batch, loss = 3.129\n",
      "\t batch, loss = 3.279\n",
      "\t batch, loss = 3.250\n",
      "\t batch, loss = 2.919\n",
      "\t batch, loss = 3.060\n",
      "\t batch, loss = 3.158\n",
      "\t batch, loss = 2.878\n",
      "\t batch, loss = 3.246\n",
      "\t batch, loss = 3.031\n",
      "\t batch, loss = 2.981\n",
      "\t batch, loss = 3.271\n",
      "\t batch, loss = 3.209\n",
      "\t batch, loss = 2.943\n",
      "\t batch, loss = 2.991\n",
      "\t batch, loss = 3.140\n",
      "\t batch, loss = 2.901\n",
      "\t batch, loss = 3.153\n",
      "\t batch, loss = 3.039\n",
      "\t batch, loss = 3.098\n",
      "\t batch, loss = 2.739\n",
      "\t batch, loss = 2.942\n",
      "\t batch, loss = 2.954\n",
      "\t batch, loss = 2.906\n",
      "\t batch, loss = 2.724\n",
      "\t batch, loss = 2.948\n",
      "\t batch, loss = 2.927\n",
      "\t batch, loss = 2.600\n",
      "\t batch, loss = 3.303\n",
      "\t batch, loss = 2.862\n",
      "\t batch, loss = 2.774\n",
      "\t batch, loss = 3.076\n",
      "\t batch, loss = 3.221\n",
      "\t batch, loss = 2.812\n",
      "\t batch, loss = 2.853\n",
      "\t batch, loss = 3.011\n",
      "\t batch, loss = 2.848\n",
      "\t batch, loss = 3.094\n",
      "\t batch, loss = 3.063\n",
      "\t batch, loss = 3.084\n",
      "\t batch, loss = 2.842\n",
      "\t batch, loss = 2.727\n",
      "\t batch, loss = 2.974\n",
      "\t batch, loss = 3.212\n",
      "\t batch, loss = 3.245\n",
      "\t batch, loss = 3.086\n",
      "\t batch, loss = 2.778\n",
      "\t batch, loss = 2.982\n",
      "\t batch, loss = 2.884\n",
      "\t batch, loss = 3.338\n",
      "\t batch, loss = 2.993\n",
      "\t batch, loss = 3.018\n",
      "\t batch, loss = 3.025\n",
      "\t batch, loss = 2.988\n",
      "\t batch, loss = 2.892\n",
      "\t batch, loss = 3.082\n",
      "\t batch, loss = 2.946\n",
      "\t batch, loss = 2.759\n",
      "\t batch, loss = 3.119\n",
      "\t batch, loss = 3.029\n",
      "\t batch, loss = 3.358\n",
      "\t batch, loss = 2.824\n",
      "\t batch, loss = 3.084\n",
      "\t batch, loss = 2.870\n",
      "\t batch, loss = 2.735\n",
      "\t batch, loss = 3.187\n",
      "\t batch, loss = 3.374\n",
      "\t batch, loss = 3.382\n",
      "\t batch, loss = 2.789\n",
      "\t batch, loss = 2.795\n",
      "\t batch, loss = 3.052\n",
      "\t batch, loss = 2.699\n",
      "\t batch, loss = 2.994\n",
      "\t batch, loss = 3.286\n",
      "\t batch, loss = 2.528\n",
      "\t batch, loss = 3.104\n",
      "\t batch, loss = 3.211\n",
      "\t batch, loss = 3.069\n",
      "\t batch, loss = 3.070\n",
      "\t batch, loss = 3.308\n",
      "\t batch, loss = 2.809\n",
      "\t batch, loss = 2.751\n",
      "\t batch, loss = 3.069\n",
      "\t batch, loss = 3.292\n",
      "\t batch, loss = 2.892\n",
      "\t batch, loss = 2.940\n",
      "\t batch, loss = 2.781\n",
      "\t batch, loss = 3.078\n",
      "\t batch, loss = 3.015\n",
      "\t batch, loss = 2.993\n",
      "\t batch, loss = 3.143\n",
      "\t batch, loss = 2.786\n",
      "\t batch, loss = 3.072\n",
      "\t batch, loss = 2.832\n",
      "\t batch, loss = 3.005\n",
      "\t batch, loss = 2.537\n",
      "\t batch, loss = 3.034\n",
      "\t batch, loss = 3.037\n",
      "\t batch, loss = 3.177\n",
      "\t batch, loss = 2.906\n",
      "\t batch, loss = 2.744\n",
      "\t batch, loss = 3.174\n",
      "\t batch, loss = 2.905\n",
      "\t batch, loss = 3.066\n",
      "\t batch, loss = 2.944\n",
      "\t batch, loss = 3.259\n",
      "\t batch, loss = 2.767\n",
      "\t batch, loss = 2.904\n",
      "\t batch, loss = 2.771\n",
      "\t batch, loss = 3.136\n",
      "\t batch, loss = 3.240\n",
      "\t batch, loss = 2.593\n",
      "\t batch, loss = 2.936\n",
      "\t batch, loss = 3.215\n",
      "\t batch, loss = 3.171\n",
      "\t batch, loss = 2.968\n",
      "\t batch, loss = 3.085\n",
      "\t batch, loss = 2.608\n",
      "\t batch, loss = 2.915\n",
      "\t batch, loss = 2.839\n",
      "\t batch, loss = 3.359\n",
      "\t batch, loss = 2.613\n",
      "\t batch, loss = 2.980\n",
      "\t batch, loss = 2.915\n",
      "\t batch, loss = 2.612\n",
      "\t batch, loss = 3.084\n",
      "\t batch, loss = 2.893\n",
      "\t batch, loss = 2.920\n",
      "\t batch, loss = 3.090\n",
      "\t batch, loss = 2.561\n",
      "\t batch, loss = 3.133\n",
      "\t batch, loss = 2.856\n",
      "\t batch, loss = 2.915\n",
      "\t batch, loss = 3.391\n",
      "\t batch, loss = 2.928\n",
      "\t batch, loss = 3.057\n",
      "\t batch, loss = 2.738\n",
      "\t batch, loss = 2.843\n",
      "\t batch, loss = 2.828\n",
      "\t batch, loss = 2.846\n",
      "\t batch, loss = 3.068\n",
      "\t batch, loss = 3.121\n",
      "\t batch, loss = 2.961\n",
      "\t batch, loss = 3.247\n",
      "\t batch, loss = 3.100\n",
      "\t batch, loss = 2.821\n",
      "\t batch, loss = 2.961\n",
      "\t batch, loss = 2.706\n",
      "\t batch, loss = 2.602\n",
      "\t batch, loss = 2.677\n",
      "\t batch, loss = 3.247\n",
      "\t batch, loss = 2.884\n",
      "\t batch, loss = 2.669\n",
      "\t batch, loss = 3.021\n",
      "\t batch, loss = 2.853\n",
      "\t batch, loss = 2.862\n",
      "\t batch, loss = 2.721\n",
      "\t batch, loss = 2.863\n",
      "\t batch, loss = 2.790\n",
      "\t batch, loss = 2.613\n",
      "\t batch, loss = 2.897\n",
      "\t batch, loss = 2.466\n",
      "\t batch, loss = 2.769\n",
      "\t batch, loss = 3.245\n",
      "\t batch, loss = 3.071\n",
      "\t batch, loss = 2.619\n",
      "\t batch, loss = 2.526\n",
      "\t batch, loss = 3.211\n",
      "\t batch, loss = 2.462\n",
      "\t batch, loss = 2.454\n",
      "\t batch, loss = 3.258\n",
      "Epoch = 4\n",
      "\t batch, loss = 3.014\n",
      "\t batch, loss = 2.744\n",
      "\t batch, loss = 3.168\n",
      "\t batch, loss = 3.260\n",
      "\t batch, loss = 2.770\n",
      "\t batch, loss = 2.720\n",
      "\t batch, loss = 2.717\n",
      "\t batch, loss = 2.906\n",
      "\t batch, loss = 2.830\n",
      "\t batch, loss = 3.021\n",
      "\t batch, loss = 3.203\n",
      "\t batch, loss = 2.839\n",
      "\t batch, loss = 3.163\n",
      "\t batch, loss = 2.476\n",
      "\t batch, loss = 3.331\n",
      "\t batch, loss = 2.906\n",
      "\t batch, loss = 2.821\n",
      "\t batch, loss = 3.176\n",
      "\t batch, loss = 3.152\n",
      "\t batch, loss = 2.958\n",
      "\t batch, loss = 3.428\n",
      "\t batch, loss = 2.665\n",
      "\t batch, loss = 2.986\n",
      "\t batch, loss = 2.821\n",
      "\t batch, loss = 3.073\n",
      "\t batch, loss = 2.889\n",
      "\t batch, loss = 2.430\n",
      "\t batch, loss = 2.327\n",
      "\t batch, loss = 3.245\n",
      "\t batch, loss = 2.914\n",
      "\t batch, loss = 2.832\n",
      "\t batch, loss = 2.825\n",
      "\t batch, loss = 3.366\n",
      "\t batch, loss = 2.796\n",
      "\t batch, loss = 3.228\n",
      "\t batch, loss = 3.194\n",
      "\t batch, loss = 3.135\n",
      "\t batch, loss = 2.998\n",
      "\t batch, loss = 2.903\n",
      "\t batch, loss = 3.174\n",
      "\t batch, loss = 2.954\n",
      "\t batch, loss = 2.755\n",
      "\t batch, loss = 2.967\n",
      "\t batch, loss = 3.029\n",
      "\t batch, loss = 3.259\n",
      "\t batch, loss = 3.030\n",
      "\t batch, loss = 3.063\n",
      "\t batch, loss = 2.931\n",
      "\t batch, loss = 3.164\n",
      "\t batch, loss = 2.794\n",
      "\t batch, loss = 2.335\n",
      "\t batch, loss = 3.059\n",
      "\t batch, loss = 2.959\n",
      "\t batch, loss = 2.849\n",
      "\t batch, loss = 2.663\n",
      "\t batch, loss = 2.860\n",
      "\t batch, loss = 2.932\n",
      "\t batch, loss = 2.545\n",
      "\t batch, loss = 2.941\n",
      "\t batch, loss = 2.498\n",
      "\t batch, loss = 2.880\n",
      "\t batch, loss = 2.991\n",
      "\t batch, loss = 3.273\n",
      "\t batch, loss = 3.033\n",
      "\t batch, loss = 3.261\n",
      "\t batch, loss = 3.190\n",
      "\t batch, loss = 3.025\n",
      "\t batch, loss = 2.944\n",
      "\t batch, loss = 2.852\n",
      "\t batch, loss = 2.902\n",
      "\t batch, loss = 2.823\n",
      "\t batch, loss = 2.972\n",
      "\t batch, loss = 3.138\n",
      "\t batch, loss = 2.540\n",
      "\t batch, loss = 3.007\n",
      "\t batch, loss = 3.128\n",
      "\t batch, loss = 2.939\n",
      "\t batch, loss = 3.051\n",
      "\t batch, loss = 2.756\n",
      "\t batch, loss = 2.921\n",
      "\t batch, loss = 3.354\n",
      "\t batch, loss = 3.048\n",
      "\t batch, loss = 3.020\n",
      "\t batch, loss = 3.081\n",
      "\t batch, loss = 3.210\n",
      "\t batch, loss = 2.949\n",
      "\t batch, loss = 3.006\n",
      "\t batch, loss = 2.885\n",
      "\t batch, loss = 2.949\n",
      "\t batch, loss = 3.136\n",
      "\t batch, loss = 2.940\n",
      "\t batch, loss = 3.176\n",
      "\t batch, loss = 3.238\n",
      "\t batch, loss = 2.937\n",
      "\t batch, loss = 2.835\n",
      "\t batch, loss = 2.839\n",
      "\t batch, loss = 2.844\n",
      "\t batch, loss = 2.839\n",
      "\t batch, loss = 2.541\n",
      "\t batch, loss = 2.976\n",
      "\t batch, loss = 2.824\n",
      "\t batch, loss = 2.964\n",
      "\t batch, loss = 2.958\n",
      "\t batch, loss = 2.670\n",
      "\t batch, loss = 3.006\n",
      "\t batch, loss = 3.103\n",
      "\t batch, loss = 2.756\n",
      "\t batch, loss = 2.944\n",
      "\t batch, loss = 2.743\n",
      "\t batch, loss = 2.988\n",
      "\t batch, loss = 2.331\n",
      "\t batch, loss = 2.656\n",
      "\t batch, loss = 2.981\n",
      "\t batch, loss = 3.005\n",
      "\t batch, loss = 2.613\n",
      "\t batch, loss = 3.233\n",
      "\t batch, loss = 3.010\n",
      "\t batch, loss = 2.816\n",
      "\t batch, loss = 2.617\n",
      "\t batch, loss = 2.996\n",
      "\t batch, loss = 3.145\n",
      "\t batch, loss = 2.372\n",
      "\t batch, loss = 2.600\n",
      "\t batch, loss = 2.965\n",
      "\t batch, loss = 2.671\n",
      "\t batch, loss = 3.527\n",
      "\t batch, loss = 2.834\n",
      "\t batch, loss = 2.932\n",
      "\t batch, loss = 3.150\n",
      "\t batch, loss = 2.786\n",
      "\t batch, loss = 2.859\n",
      "\t batch, loss = 3.127\n",
      "\t batch, loss = 2.557\n",
      "\t batch, loss = 2.573\n",
      "\t batch, loss = 2.916\n",
      "\t batch, loss = 2.926\n",
      "\t batch, loss = 2.388\n",
      "\t batch, loss = 2.880\n",
      "\t batch, loss = 2.559\n",
      "\t batch, loss = 3.027\n",
      "\t batch, loss = 2.935\n",
      "\t batch, loss = 3.048\n",
      "\t batch, loss = 3.039\n",
      "\t batch, loss = 3.168\n",
      "\t batch, loss = 3.072\n",
      "\t batch, loss = 2.951\n",
      "\t batch, loss = 2.756\n",
      "\t batch, loss = 2.989\n",
      "\t batch, loss = 3.276\n",
      "\t batch, loss = 2.765\n",
      "\t batch, loss = 3.381\n",
      "\t batch, loss = 3.048\n",
      "\t batch, loss = 2.878\n",
      "\t batch, loss = 2.848\n",
      "\t batch, loss = 3.084\n",
      "\t batch, loss = 2.625\n",
      "\t batch, loss = 3.178\n",
      "\t batch, loss = 3.115\n",
      "\t batch, loss = 3.060\n",
      "\t batch, loss = 2.455\n",
      "\t batch, loss = 2.748\n",
      "\t batch, loss = 3.138\n",
      "\t batch, loss = 2.950\n",
      "\t batch, loss = 3.101\n",
      "\t batch, loss = 3.015\n",
      "\t batch, loss = 3.123\n",
      "\t batch, loss = 3.103\n",
      "\t batch, loss = 2.939\n",
      "\t batch, loss = 2.867\n",
      "\t batch, loss = 3.290\n",
      "\t batch, loss = 2.921\n",
      "\t batch, loss = 3.094\n",
      "\t batch, loss = 3.062\n",
      "\t batch, loss = 2.921\n",
      "\t batch, loss = 2.744\n",
      "\t batch, loss = 2.628\n",
      "\t batch, loss = 2.722\n",
      "\t batch, loss = 2.999\n",
      "\t batch, loss = 3.289\n",
      "\t batch, loss = 2.997\n",
      "\t batch, loss = 2.824\n",
      "\t batch, loss = 2.710\n",
      "\t batch, loss = 2.661\n",
      "\t batch, loss = 3.086\n",
      "\t batch, loss = 2.712\n",
      "\t batch, loss = 2.877\n",
      "\t batch, loss = 2.980\n",
      "\t batch, loss = 3.344\n",
      "\t batch, loss = 2.866\n",
      "\t batch, loss = 2.646\n",
      "\t batch, loss = 3.107\n",
      "\t batch, loss = 2.927\n",
      "\t batch, loss = 2.936\n",
      "\t batch, loss = 3.041\n",
      "\t batch, loss = 2.619\n",
      "\t batch, loss = 2.851\n",
      "\t batch, loss = 2.692\n",
      "\t batch, loss = 3.134\n",
      "\t batch, loss = 2.691\n",
      "\t batch, loss = 2.573\n",
      "\t batch, loss = 2.839\n",
      "\t batch, loss = 3.074\n",
      "\t batch, loss = 2.914\n",
      "\t batch, loss = 3.001\n",
      "\t batch, loss = 3.100\n",
      "\t batch, loss = 3.144\n",
      "\t batch, loss = 2.820\n",
      "\t batch, loss = 3.379\n",
      "\t batch, loss = 2.838\n",
      "\t batch, loss = 2.832\n",
      "\t batch, loss = 3.267\n",
      "\t batch, loss = 2.790\n",
      "\t batch, loss = 3.030\n",
      "\t batch, loss = 2.885\n",
      "\t batch, loss = 3.031\n",
      "\t batch, loss = 3.089\n",
      "\t batch, loss = 2.958\n",
      "\t batch, loss = 3.172\n",
      "\t batch, loss = 3.011\n",
      "\t batch, loss = 2.933\n",
      "\t batch, loss = 2.731\n",
      "\t batch, loss = 3.054\n",
      "\t batch, loss = 2.743\n",
      "\t batch, loss = 3.002\n",
      "\t batch, loss = 3.188\n",
      "\t batch, loss = 2.906\n",
      "\t batch, loss = 2.883\n",
      "\t batch, loss = 3.163\n",
      "\t batch, loss = 2.998\n",
      "\t batch, loss = 3.043\n",
      "\t batch, loss = 2.784\n",
      "\t batch, loss = 2.797\n",
      "\t batch, loss = 3.018\n",
      "\t batch, loss = 2.554\n",
      "\t batch, loss = 2.823\n",
      "\t batch, loss = 2.792\n",
      "\t batch, loss = 2.780\n",
      "\t batch, loss = 3.092\n",
      "\t batch, loss = 2.850\n",
      "\t batch, loss = 2.718\n",
      "\t batch, loss = 3.074\n",
      "\t batch, loss = 2.809\n",
      "\t batch, loss = 2.849\n",
      "\t batch, loss = 2.727\n",
      "\t batch, loss = 3.178\n",
      "\t batch, loss = 2.972\n",
      "\t batch, loss = 2.992\n",
      "\t batch, loss = 2.775\n",
      "\t batch, loss = 3.090\n",
      "\t batch, loss = 2.988\n",
      "\t batch, loss = 3.114\n",
      "\t batch, loss = 2.973\n",
      "\t batch, loss = 2.739\n",
      "\t batch, loss = 2.943\n",
      "\t batch, loss = 3.043\n",
      "\t batch, loss = 3.029\n",
      "\t batch, loss = 3.190\n",
      "\t batch, loss = 3.204\n",
      "\t batch, loss = 2.929\n",
      "\t batch, loss = 2.531\n",
      "\t batch, loss = 2.246\n",
      "\t batch, loss = 3.150\n",
      "\t batch, loss = 3.150\n",
      "\t batch, loss = 2.658\n",
      "\t batch, loss = 2.804\n",
      "\t batch, loss = 3.001\n",
      "\t batch, loss = 2.498\n",
      "\t batch, loss = 2.760\n",
      "\t batch, loss = 2.960\n",
      "\t batch, loss = 2.668\n",
      "\t batch, loss = 2.905\n",
      "\t batch, loss = 2.998\n",
      "\t batch, loss = 2.923\n",
      "\t batch, loss = 2.691\n",
      "\t batch, loss = 3.184\n",
      "\t batch, loss = 2.909\n",
      "\t batch, loss = 2.726\n",
      "\t batch, loss = 2.830\n",
      "\t batch, loss = 2.826\n",
      "\t batch, loss = 2.693\n",
      "\t batch, loss = 2.531\n",
      "\t batch, loss = 3.063\n",
      "Epoch = 5\n",
      "\t batch, loss = 2.703\n",
      "\t batch, loss = 2.770\n",
      "\t batch, loss = 3.210\n",
      "\t batch, loss = 2.706\n",
      "\t batch, loss = 3.052\n",
      "\t batch, loss = 2.610\n",
      "\t batch, loss = 2.790\n",
      "\t batch, loss = 3.003\n",
      "\t batch, loss = 2.933\n",
      "\t batch, loss = 3.185\n",
      "\t batch, loss = 3.060\n",
      "\t batch, loss = 2.767\n",
      "\t batch, loss = 2.413\n",
      "\t batch, loss = 3.169\n",
      "\t batch, loss = 2.624\n",
      "\t batch, loss = 2.995\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m masks \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 6\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m loss \u001b[38;5;241m=\u001b[39m crit(logits\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, vocab_size), labels\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      8\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/md_sims/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/md_sims/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m, in \u001b[0;36mMaskedTransformer.forward\u001b[0;34m(self, input_ids, mask)\u001b[0m\n\u001b[1;32m     12\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed(input_ids)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/md_sims/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/md_sims/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n\u001b[0;32m---> 11\u001b[0m     attn_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m attn_out\n\u001b[1;32m     13\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/md_sims/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/md_sims/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/md_sims/lib/python3.9/site-packages/torch/nn/modules/activation.py:1373\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1347\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1348\u001b[0m         query,\n\u001b[1;32m   1349\u001b[0m         key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m   1371\u001b[0m     )\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1373\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1391\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1393\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/miniconda3/envs/md_sims/lib/python3.9/site-packages/torch/nn/functional.py:6389\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   6387\u001b[0m attn_output_weights \u001b[38;5;241m=\u001b[39m attn_output_weights\u001b[38;5;241m.\u001b[39mview(bsz, num_heads, tgt_len, src_len)\n\u001b[1;32m   6388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average_attn_weights:\n\u001b[0;32m-> 6389\u001b[0m     attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mattn_output_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n\u001b[1;32m   6392\u001b[0m     \u001b[38;5;66;03m# squeeze the output if input was unbatched\u001b[39;00m\n\u001b[1;32m   6393\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    for batch in train_loader:\n",
    "        ids, labels = mask_inputs(batch['input_ids'], tokenizer.token_to_id('[MASK]'))\n",
    "        masks = batch['attention_mask']\n",
    "        opt.zero_grad()\n",
    "        logits = model(ids, masks)\n",
    "        loss = crit(logits.reshape(-1, vocab_size), labels.reshape(-1))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        print(f\"\\t batch, loss = {loss:.3f}\")\n",
    "    print(f\"Epoch = {epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedTransformer(\n",
       "  (embed): Embedding(517, 64)\n",
       "  (layers): ModuleList(\n",
       "    (0-2): 3 x TransformerBlock(\n",
       "      (attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=64, out_features=517, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), 'masked_transformer_weights.pth')\n",
    "\n",
    "model = MaskedTransformer(vocab_size)\n",
    "model.load_state_dict(torch.load('masked_transformer_weights.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  CC(C)c1cc(C(=O)Nc2ccc(C[NH+]3CCCC3)cc2)n[nH]1\n",
      "\n",
      "Masked  :  [UNK]ĠCC(C[MASK]c1[MASK](C(=[MASK])Nc2ccc[MASK]C[NH+]3CCCC3)cc2)n[[MASK][MASK]1[MASK][CLS]\n",
      "Predicted: ĠCCĊ)ĊĊn)()OO)sĠOn1(nCSCCOOOĊ)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# 1. Load the 10 000th SMILES\n",
    "df_full = pd.read_csv('250k_rndm_zinc_drugs_clean_3.csv')\n",
    "smiles_orig = df_full.iloc[10001]['smiles']  # zero-based index\n",
    "\n",
    "# 2. Encode and build batch\n",
    "encoding = tokenizer.encode(smiles_orig)\n",
    "input_ids = torch.tensor(encoding.ids).unsqueeze(0)           # shape [1, L]\n",
    "attention_mask = torch.tensor(encoding.attention_mask).unsqueeze(0)\n",
    "\n",
    "# 3. Apply masking\n",
    "mask_id = tokenizer.token_to_id('[MASK]')\n",
    "ids_masked, _ = mask_inputs(input_ids.clone(), mask_id, mask_prob=0.15)\n",
    "\n",
    "# 4. Forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(ids_masked, attention_mask)               # [1, L, V]\n",
    "    preds = logits.argmax(dim=-1).squeeze(0).tolist()         # [L]\n",
    "\n",
    "# 5. Decode back to tokens → SMILES\n",
    "tokens_pred = [tokenizer.id_to_token(i) for i in preds]\n",
    "# strip off [CLS]/[SEP] and join\n",
    "smiles_pred = ''.join(tok for tok in tokens_pred if tok not in ('[CLS]','[SEP]','[PAD]'))\n",
    "\n",
    "print(\"Original: \", smiles_orig)\n",
    "print(\"Masked  : \", ''.join(\n",
    "    tokenizer.id_to_token(i) if i!=mask_id else '[MASK]'\n",
    "    for i in ids_masked.squeeze(0).tolist()))\n",
    "print(\"Predicted:\", smiles_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it's mostly predicting garbage, it has understood some common features like starting the chain with 2 carbons etc. This poor performance is mostly to be expected for such a small dataset and ~5 epochs with shallow nets and small batches, but I can see the shape of the structure starting form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Lyra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed briefly in class and in presentations, the new SOTA [Lyra](https://arxiv.org/abs/2503.16351) architecture is a subquadratic sequence modeling architecture built on two units, projected gated convolution (PGC), and S4D (a state space model). See page 23 for details. It has shown SOTA performance on various protein modeling tasks.\n",
    "### a)\n",
    "Implement the PGC, and explain the intuition for what is happening. Describe the complexity of one application of the unit.\n",
    "### b)\n",
    "Lyra uses [S4D](https://arxiv.org/pdf/2206.11893). Find a copy of the model, understand it ([Annotated S4](https://srush.github.io/annotated-s4/)), and give an overview of how the model works. Then make a short example of using S4D to predict sequences, e.g. a simple wave.\n",
    "### c)\n",
    "Write a torch module that combines the above to generate the Lyra model using the model spec from page 23.\n",
    "### d)\n",
    "Train the LYRA model to predict 1 dimensional data for the function $f(x) = \\cos(x) + .5\\sin(2x)$. Hint: It may make more sense to define a \"LyraMini\" so it trains faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        rms = x.pow(2).mean(-1, keepdim=True).add(self.eps).sqrt()\n",
    "        return self.scale * (x / rms)\n",
    "\n",
    "class ProjectedGatedConv(nn.Module):\n",
    "    def __init__(self, features, hidden_dim, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.project_in = nn.Linear(features, hidden_dim)\n",
    "        self.norm1 = RMSNorm(hidden_dim)\n",
    "        self.conv = nn.Conv1d(hidden_dim, hidden_dim, kernel_size,\n",
    "                              padding=kernel_size // 2, groups=hidden_dim)\n",
    "        self.gate_lin = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.project_out = nn.Linear(hidden_dim, features)\n",
    "        self.norm2 = RMSNorm(features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.project_in(x)\n",
    "\n",
    "        h = self.norm1(h)\n",
    "\n",
    "        h_t = h.transpose(1, 2)\n",
    "        conv_out = self.conv(h_t).transpose(1, 2)\n",
    "\n",
    "        gate = self.gate_lin(h)\n",
    "\n",
    "        gated = conv_out * gate\n",
    "\n",
    "        out = self.project_out(gated)\n",
    "\n",
    "        out = self.norm2(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuition\n",
    "1. Projection + RMSNorm  \n",
    "   We map the original features into a lower (or higher) dimensional space `hidden_dim` and apply RMS normalization to stabilize activations and gradients.  \n",
    "\n",
    "2. Parallel paths\n",
    "   - Local path: a depthwise 1D convolution of kernel size $K$ learns position‐local motifs of length $K$.  \n",
    "   - Global path: a dense layer computes a gating signal at each position based on the entire receptive field.  \n",
    "\n",
    "3. Gating  \n",
    "   Element‐wise multiplication of the convolution output and the gate lets global context modulate local feature extraction, amplifying or suppressing motifs depending on broader sequence context.  \n",
    "\n",
    "4. Projection back + RMSNorm \n",
    "   We project the gated features back to the original feature dimension and apply RMS normalization again, ensuring a consistent scale for downstream layers.  \n",
    "\n",
    "Computational complexity (per sequence of length $L$, input features $F$, hidden size $H$)  \n",
    "\n",
    "- Two projections: each $O(LFH)$  \n",
    "- Depthwise convolution: $O(LHK)$  \n",
    "- Final projection: $O(LHF)$  \n",
    "- Two RMSNorms: $O(LH) + O(LF)$  \n",
    "\n",
    "Overall cost:  \n",
    "$$\n",
    "O\\bigl(L\\,(2FH + HK + FH)\\bigr) \\;=\\; O\\bigl(L\\,H\\,(3F + K)\\bigr).\n",
    "$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Minimal version of S4D with extra options and features stripped out, for pedagogical purposes.\"\"\"\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "# https://github.com/state-spaces/s4/blob/main/src/models/nn/dropout.py\n",
    "class DropoutNd(nn.Module):\n",
    "    def __init__(self, p: float = 0.5, tie=True, transposed=True):\n",
    "        \"\"\"\n",
    "        tie: tie dropout mask across sequence lengths (Dropout1d/2d/3d)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if p < 0 or p >= 1:\n",
    "            raise ValueError(\"dropout probability has to be in [0, 1), \" \"but got {}\".format(p))\n",
    "        self.p = p\n",
    "        self.tie = tie\n",
    "        self.transposed = transposed\n",
    "        self.binomial = torch.distributions.binomial.Binomial(probs=1-self.p)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"X: (batch, dim, lengths...).\"\"\"\n",
    "        if self.training:\n",
    "            if not self.transposed: X = rearrange(X, 'b ... d -> b d ...')\n",
    "            # binomial = torch.distributions.binomial.Binomial(probs=1-self.p) # This is incredibly slow because of CPU -> GPU copying\n",
    "            mask_shape = X.shape[:2] + (1,)*(X.ndim-2) if self.tie else X.shape\n",
    "            # mask = self.binomial.sample(mask_shape)\n",
    "            mask = torch.rand(*mask_shape, device=X.device) < 1.-self.p\n",
    "            X = X * mask * (1.0/(1-self.p))\n",
    "            if not self.transposed: X = rearrange(X, 'b d ... -> b ... d')\n",
    "            return X\n",
    "        return X\n",
    "    \n",
    "# https://github.com/state-spaces/s4/blob/main/models/s4/s4d.py\n",
    "class S4DKernel(nn.Module):\n",
    "    \"\"\"Generate convolution kernel from diagonal SSM parameters.\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, N=64, dt_min=0.001, dt_max=0.1, lr=None):\n",
    "        super().__init__()\n",
    "        # Generate dt\n",
    "        H = d_model\n",
    "        log_dt = torch.rand(H) * (\n",
    "            math.log(dt_max) - math.log(dt_min)\n",
    "        ) + math.log(dt_min)\n",
    "\n",
    "        C = torch.randn(H, N // 2, dtype=torch.cfloat)\n",
    "        self.C = nn.Parameter(torch.view_as_real(C))\n",
    "        self.register(\"log_dt\", log_dt, lr)\n",
    "\n",
    "        log_A_real = torch.log(0.5 * torch.ones(H, N//2))\n",
    "        A_imag = math.pi * repeat(torch.arange(N//2), 'n -> h n', h=H)\n",
    "        self.register(\"log_A_real\", log_A_real, lr)\n",
    "        self.register(\"A_imag\", A_imag, lr)\n",
    "\n",
    "    def forward(self, L):\n",
    "        \"\"\"\n",
    "        returns: (..., c, L) where c is number of channels (default 1)\n",
    "        \"\"\"\n",
    "\n",
    "        # Materialize parameters\n",
    "        dt = torch.exp(self.log_dt) # (H)\n",
    "        C = torch.view_as_complex(self.C) # (H N)\n",
    "        A = -torch.exp(self.log_A_real) + 1j * self.A_imag # (H N)\n",
    "\n",
    "        # Vandermonde multiplication\n",
    "        dtA = A * dt.unsqueeze(-1)  # (H N)\n",
    "        K = dtA.unsqueeze(-1) * torch.arange(L, device=A.device) # (H N L)\n",
    "        C = C * (torch.exp(dtA)-1.) / A\n",
    "        K = 2 * torch.einsum('hn, hnl -> hl', C, torch.exp(K)).real\n",
    "\n",
    "        return K\n",
    "\n",
    "    def register(self, name, tensor, lr=None):\n",
    "        \"\"\"Register a tensor with a configurable learning rate and 0 weight decay\"\"\"\n",
    "\n",
    "        if lr == 0.0:\n",
    "            self.register_buffer(name, tensor)\n",
    "        else:\n",
    "            self.register_parameter(name, nn.Parameter(tensor))\n",
    "\n",
    "            optim = {\"weight_decay\": 0.0}\n",
    "            if lr is not None: optim[\"lr\"] = lr\n",
    "            setattr(getattr(self, name), \"_optim\", optim)\n",
    "\n",
    "\n",
    "class S4D(nn.Module):\n",
    "    def __init__(self, d_model, d_state=64, dropout=0.0, transposed=True, **kernel_args):\n",
    "        super().__init__()\n",
    "\n",
    "        self.h = d_model\n",
    "        self.n = d_state\n",
    "        self.d_output = self.h\n",
    "        self.transposed = transposed\n",
    "\n",
    "        self.D = nn.Parameter(torch.randn(self.h))\n",
    "\n",
    "        # SSM Kernel\n",
    "        self.kernel = S4DKernel(self.h, N=self.n, **kernel_args)\n",
    "\n",
    "        # Pointwise\n",
    "        self.activation = nn.GELU()\n",
    "        # dropout_fn = nn.Dropout2d # NOTE: bugged in PyTorch 1.11\n",
    "        dropout_fn = DropoutNd\n",
    "        self.dropout = dropout_fn(dropout) if dropout > 0.0 else nn.Identity()\n",
    "\n",
    "        # position-wise output transform to mix features\n",
    "        self.output_linear = nn.Sequential(\n",
    "            nn.Conv1d(self.h, 2*self.h, kernel_size=1),\n",
    "            nn.GLU(dim=-2),\n",
    "        )\n",
    "\n",
    "    def forward(self, u, **kwargs): # absorbs return_output and transformer src mask\n",
    "        \"\"\" Input and output shape (B, H, L) \"\"\"\n",
    "        if not self.transposed: u = u.transpose(-1, -2)\n",
    "        L = u.size(-1)\n",
    "\n",
    "        # Compute SSM Kernel\n",
    "        k = self.kernel(L=L) # (H L)\n",
    "\n",
    "        # Convolution\n",
    "        k_f = torch.fft.rfft(k, n=2*L) # (H L)\n",
    "        u_f = torch.fft.rfft(u, n=2*L) # (B H L)\n",
    "        y = torch.fft.irfft(u_f*k_f, n=2*L)[..., :L] # (B H L)\n",
    "\n",
    "        # Compute D term in state space equation - essentially a skip connection\n",
    "        y = y + u * self.D.unsqueeze(-1)\n",
    "\n",
    "        y = self.dropout(self.activation(y))\n",
    "        y = self.output_linear(y)\n",
    "        if not self.transposed: y = y.transpose(-1, -2)\n",
    "        return y, None # Return a dummy state to satisfy this repo's interface, but this can be modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5461, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5472, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5457, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5452, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5414, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5412, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5443, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5395, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5414, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5386, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5371, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5346, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5395, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5359, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5359, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5341, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5314, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5309, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5307, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5346, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5278, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5289, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5223, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5286, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5239, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5233, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5255, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5193, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5209, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5231, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5218, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5233, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5184, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5218, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5164, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5159, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5160, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5157, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5117, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5147, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5116, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5147, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5122, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5145, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5100, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5098, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5109, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5071, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5079, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5079, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5054, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5040, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5017, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5051, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5044, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5012, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5016, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5023, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5021, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5010, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5018, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4950, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4983, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4961, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4971, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4945, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4970, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4982, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4920, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4949, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4888, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4892, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4893, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4901, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4895, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4884, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4869, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4841, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4842, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4826, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4820, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4856, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4818, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4819, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4791, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4801, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4764, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4789, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4802, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4790, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4703, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4712, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4720, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4736, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4739, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4706, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4705, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4720, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4665, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4681, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4689, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4696, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4629, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4620, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4618, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4610, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4635, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4592, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4571, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4610, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4560, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4573, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4525, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4595, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4527, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4517, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4542, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4526, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4510, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4471, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4494, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4486, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4423, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4491, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4439, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4431, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4415, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4342, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4384, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4399, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4389, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4462, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4378, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4407, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4306, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4299, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4420, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4284, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4262, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4229, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4290, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4286, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4287, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4285, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4167, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4170, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4288, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4229, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4029, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4192, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4186, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4140, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4180, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4083, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4258, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4068, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4013, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4142, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4010, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3945, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4125, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3989, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3949, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3997, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3939, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3972, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4031, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4018, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3884, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3885, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3876, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3910, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3876, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3943, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3784, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3736, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3878, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3817, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3732, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3897, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3583, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3690, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3829, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3726, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3672, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3849, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3713, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3792, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3817, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3764, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3600, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3785, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3562, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3567, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3469, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3666, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3722, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3670, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3594, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3464, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3374, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3367, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3451, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3358, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3528, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3485, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3471, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3452, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3321, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3480, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3245, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3286, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3474, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3279, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3395, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3146, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3447, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3367, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3231, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3203, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3304, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3121, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3296, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3300, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3324, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3107, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3045, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2933, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3146, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2945, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3166, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3059, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2916, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3026, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3076, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2993, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3005, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3103, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3146, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3058, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3073, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2885, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2991, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2774, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2967, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2887, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2983, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2984, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3112, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2983, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3049, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2818, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2954, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2887, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2911, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2884, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2886, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2807, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2804, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2834, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2766, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2688, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2869, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2749, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2564, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2671, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2649, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2649, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2616, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2601, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2491, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2671, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2756, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2532, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2829, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2780, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2911, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2780, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2511, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2609, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2481, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2594, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2380, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2558, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2692, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2572, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2612, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2609, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2638, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2385, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2556, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2482, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2426, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2399, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2354, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2362, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2396, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2447, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2487, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2464, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2520, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2613, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2494, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2481, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2421, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2463, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2351, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2441, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2510, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2426, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2340, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2361, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2422, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2138, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2325, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2305, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2192, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2153, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2178, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2158, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2147, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1942, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2079, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2220, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2121, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2212, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2141, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2216, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2185, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2119, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2357, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2152, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2008, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2070, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2071, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2048, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1974, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1959, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2164, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2257, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2027, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1943, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2186, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2006, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2010, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2131, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1964, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1962, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2151, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2071, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1992, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2187, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2057, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2167, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2028, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2061, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1810, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2149, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2196, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1816, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1996, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1959, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1839, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1895, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2042, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1782, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2011, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2045, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1979, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1858, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1814, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1885, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1989, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1891, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1910, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1884, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1826, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1818, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1803, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1887, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1855, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1703, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2029, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1659, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1922, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1678, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1854, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1736, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1795, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1925, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1873, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1832, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1853, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1882, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1844, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1841, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1727, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1792, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1727, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1672, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1617, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1829, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1725, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1761, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1688, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1670, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1692, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1809, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1648, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1853, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1676, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1779, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1766, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1772, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1848, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1624, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1714, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1710, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1616, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1728, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1709, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1634, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1612, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1553, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1493, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1534, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1634, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1502, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1568, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1621, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1612, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1659, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1698, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1683, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1741, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1534, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1489, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1641, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1543, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1726, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1514, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1699, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1564, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1579, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1620, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1561, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1500, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1562, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1496, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1499, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1491, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1499, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1474, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1504, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1585, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1552, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1527, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1395, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1398, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1488, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1505, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1443, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1498, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1441, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1380, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1555, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1510, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1522, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1546, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1478, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1456, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1442, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1383, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1487, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1526, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1376, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1483, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1372, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1498, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1427, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1462, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1400, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1389, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1511, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1407, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1498, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1282, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1362, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1380, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1487, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1408, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1429, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1373, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1329, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1396, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1366, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1409, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1341, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1317, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1372, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1366, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1329, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1228, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1486, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1340, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1325, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1308, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1375, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1343, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1384, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1310, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1372, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1344, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1324, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1286, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1315, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1269, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1315, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1308, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1310, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1244, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1276, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1272, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1308, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1288, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1394, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1244, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1205, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1312, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1331, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1237, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1292, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1213, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1234, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1299, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1268, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1328, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1227, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1307, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1283, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1279, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1293, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1240, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1305, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1363, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1252, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1177, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1286, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1206, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1233, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1159, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1244, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1225, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1308, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1263, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1261, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1255, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1335, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1270, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1226, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1298, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1236, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1218, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1227, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1220, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1080, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1183, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1112, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1153, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1220, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1292, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1209, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1195, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1175, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1218, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1171, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1139, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1194, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1182, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1159, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1209, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1187, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1182, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1142, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1252, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1256, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1206, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1180, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1083, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1226, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1165, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1156, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1184, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1265, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1079, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1193, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1184, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1144, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1153, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1086, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1147, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1103, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1190, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1254, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1130, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1123, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1082, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1202, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1092, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1207, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1177, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1083, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1162, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1091, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1138, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1122, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1148, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1185, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1066, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1095, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1080, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1171, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1188, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1140, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1163, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1168, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1139, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1197, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1200, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1203, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1180, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1118, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1176, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1161, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1065, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1117, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1230, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1106, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1065, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1118, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1199, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1167, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1081, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1071, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1120, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1125, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1133, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1121, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1220, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1157, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1164, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1119, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1037, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1083, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1151, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1093, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1139, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1117, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1103, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1097, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1186, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1186, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1148, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1112, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1030, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1142, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1071, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1086, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1169, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1138, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1128, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1073, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1005, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1112, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1142, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1107, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1093, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1126, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1127, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1085, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1119, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1085, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1094, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1091, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1098, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1050, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1067, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1098, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1097, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1039, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1094, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1095, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1096, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1095, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1147, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1055, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1096, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1079, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1026, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1038, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1054, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1123, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1122, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1072, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1030, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1010, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1045, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1023, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1043, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1038, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1046, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1083, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1080, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1036, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1017, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1094, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1128, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1065, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1047, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1087, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1055, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1139, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1046, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1049, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0991, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1073, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1121, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1004, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1032, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1096, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1067, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1020, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1105, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1050, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1112, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1115, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1001, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1048, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1013, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1042, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0991, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1063, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1063, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0986, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1065, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1064, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0987, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0977, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0951, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1003, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1042, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1083, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1052, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0972, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1062, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1056, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1017, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1040, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1013, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1081, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0970, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1045, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1012, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1053, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1063, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1089, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1024, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1075, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1079, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1024, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0965, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0953, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1024, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0992, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1050, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0991, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1030, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0969, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1024, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1034, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1049, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1037, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1031, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1039, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1040, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1011, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0995, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0985, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1074, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1062, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0991, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1045, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1018, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1054, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1034, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1040, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0995, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1003, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1074, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1010, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0961, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0971, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0910, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1003, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0978, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1011, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1028, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1025, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0989, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1000, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1036, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0991, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1026, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1017, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1003, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1059, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0979, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0976, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1027, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1036, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0996, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1052, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1054, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0996, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1023, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0973, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0973, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1030, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0981, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0965, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0979, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0926, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0988, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1000, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0930, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1004, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1059, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1040, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0986, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0979, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1016, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1037, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0985, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0930, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1060, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0984, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1007, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0991, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0985, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1021, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1024, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0984, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0914, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1034, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0981, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0970, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0991, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0993, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0959, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0973, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0973, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1006, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0979, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0988, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0962, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0970, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0990, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1010, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0997, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0984, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1018, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1010, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0983, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1026, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0971, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0979, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0975, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1026, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1015, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1001, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1008, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0934, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0986, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0911, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1002, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0953, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0972, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0991, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0971, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0975, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0962, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0911, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0971, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0959, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0974, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0969, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0940, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0950, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0944, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0974, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1000, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1025, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0991, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0968, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0989, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0966, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0929, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0936, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0961, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0964, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0984, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0978, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0932, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0991, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0946, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0997, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0937, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0991, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0990, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0966, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0966, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0914, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0990, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0985, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0983, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0967, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0982, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1012, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0956, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1014, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0901, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0948, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0973, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0956, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0993, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0936, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1005, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0923, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0954, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0938, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0953, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0937, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0941, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0931, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0947, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0982, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0920, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0959, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0948, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0958, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0997, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0959, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0949, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0957, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0954, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0978, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0969, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0970, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0982, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0939, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0971, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0953, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0959, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0961, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0920, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0903, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0921, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0968, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0933, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0982, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0888, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0953, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0928, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0942, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0963, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0951, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0961, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0946, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0968, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0922, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0908, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1005, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0942, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0937, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0925, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0926, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0895, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0925, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0972, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0937, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0972, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0920, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0919, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0948, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0974, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0965, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0915, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0931, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0934, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0919, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0922, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0908, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0915, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0940, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0919, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0958, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0930, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0893, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0980, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0931, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0923, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0906, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0918, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0952, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0958, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0933, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0945, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0920, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0957, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0941, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0916, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0956, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0938, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0939, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0908, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0920, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0913, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0920, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0892, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0903, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0957, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0932, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0853, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0933, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0884, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0943, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0923, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0943, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0946, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0919, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0910, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0933, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0914, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0942, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0962, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0918, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0922, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0912, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0892, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0954, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0933, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0918, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0937, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0911, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0910, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0898, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0925, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0945, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0931, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0865, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0911, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0929, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0922, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0966, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0893, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0872, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0920, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0876, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0919, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0902, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0916, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0903, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0935, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0919, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0873, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0905, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0890, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0936, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0879, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0907, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0875, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0892, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0930, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0894, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0896, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0893, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0929, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0878, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0903, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0950, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0919, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0892, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0904, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0913, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0907, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0890, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0934, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0920, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0868, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0952, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0888, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0910, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0897, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0881, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0881, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0918, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0930, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0892, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0903, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0936, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0883, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0908, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0894, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0916, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0913, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0903, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0898, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0910, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0876, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0896, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0902, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0881, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0872, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0896, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0893, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0901, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0899, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0904, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0873, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0894, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0887, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0909, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0918, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0903, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0895, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0912, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0869, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0901, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0875, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0866, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0887, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0856, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0879, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0918, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0881, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0868, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0895, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0897, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0884, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0842, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0886, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0883, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0885, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0878, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0908, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0915, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0892, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0894, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0895, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0889, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0862, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0892, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0893, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0893, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0910, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0869, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0895, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0854, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0881, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0864, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0872, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0889, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0878, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0892, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0879, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0853, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0853, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0847, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0909, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0883, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0884, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0878, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0870, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0877, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0869, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0859, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0865, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0889, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0853, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0884, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0874, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0851, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0901, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0862, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0878, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0870, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0863, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0876, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0891, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0879, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0853, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0864, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0845, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0846, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0877, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0861, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0886, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0866, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0869, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0875, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0843, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0877, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0869, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0890, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0877, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0878, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0826, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0850, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0837, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0867, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0862, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0884, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0894, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0877, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0873, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0868, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0871, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0860, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0857, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0891, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0860, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0849, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0869, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0843, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0870, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0863, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0860, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0835, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0866, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0877, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0861, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0862, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0841, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0869, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0836, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0866, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0884, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0852, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0854, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0835, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0838, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0837, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0858, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0853, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0868, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0841, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0872, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0860, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0881, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0845, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0861, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0846, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0848, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0835, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0849, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0858, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0861, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0864, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0857, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0867, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0856, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0813, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0873, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0830, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0849, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0830, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0859, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0859, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0837, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0822, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0863, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0859, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0833, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0833, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0846, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0813, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0856, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0840, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0850, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0830, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0845, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0835, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0840, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0838, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0855, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0861, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0844, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0836, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0867, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0836, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0854, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0852, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0819, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0839, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0837, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0828, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0840, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0854, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0844, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0821, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0858, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0835, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0834, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0858, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0831, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0839, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0844, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0823, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0851, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0837, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0825, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0848, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0835, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0848, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0831, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0856, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0850, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0848, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0849, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0853, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0845, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0832, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0830, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0820, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0835, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0827, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0856, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0831, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0836, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0838, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0846, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0830, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0829, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0824, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0833, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0837, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0840, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0855, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0836, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0845, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0839, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0826, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0838, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0830, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0813, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0828, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0824, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0818, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0840, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0836, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0838, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0806, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0824, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0829, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0803, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0810, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0836, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0826, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0830, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0833, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0840, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0826, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0846, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0828, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0812, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0807, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0835, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0819, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0824, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0819, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0829, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0821, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0833, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0818, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0831, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0836, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0844, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0815, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0803, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0830, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0803, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0830, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0826, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0823, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0807, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0815, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0829, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0807, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0827, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0823, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0821, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0806, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0831, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0832, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0824, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0794, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0806, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0825, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0818, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0815, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0797, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0824, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0825, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0804, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0815, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0815, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0815, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0810, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0812, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0824, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0823, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0811, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0794, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0824, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0815, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0826, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0816, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0809, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0814, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0811, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0817, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0815, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0812, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0792, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0819, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0801, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0811, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0796, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0797, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0829, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0810, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0811, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0813, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0824, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0803, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0808, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0786, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0819, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0792, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0804, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0815, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0805, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0805, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0797, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0796, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0785, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0806, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0804, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0803, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0818, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0806, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0792, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0813, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0803, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0801, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0798, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0793, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0811, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0782, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0805, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0813, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0810, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0805, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0796, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0798, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0794, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0787, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0794, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0805, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0801, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0795, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0805, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0796, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0811, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0775, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0804, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0794, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0792, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0796, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0783, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0799, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0783, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0792, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0789, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0799, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0789, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0795, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0797, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0791, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0811, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0793, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0792, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0808, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0791, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0781, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0788, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0796, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0787, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0785, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0798, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0774, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0804, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0786, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0792, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0786, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0778, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0781, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0796, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0786, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0792, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0788, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0793, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0777, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0791, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0790, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0783, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0801, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0779, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0789, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0781, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0805, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0790, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0783, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0789, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0777, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0764, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0786, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0775, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0776, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0785, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0798, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0787, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0786, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0787, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0792, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0786, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0800, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0792, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0781, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0780, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0783, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0788, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0785, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0774, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0770, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0782, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0782, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0774, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0778, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0789, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0780, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0785, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0780, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0773, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0775, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0781, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0780, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0781, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0780, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0786, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0768, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0775, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0771, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0773, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0782, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0786, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0772, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0780, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0778, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0774, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0784, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0773, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0778, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0777, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0767, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0760, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0776, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0776, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0777, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0782, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0775, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0777, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0777, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0772, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0776, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0764, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0775, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0767, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0769, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0778, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0775, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0768, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0766, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0777, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0765, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0765, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0769, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0768, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0764, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0765, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0770, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0764, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0768, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0768, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0773, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0773, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0770, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0769, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0757, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0765, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0768, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0762, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0777, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0757, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0765, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0773, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0758, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0766, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0764, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0750, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0765, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0767, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0750, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0770, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0771, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0755, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0758, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0759, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0765, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0774, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0762, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0776, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0771, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0755, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0766, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0748, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0756, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0765, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0765, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0757, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0758, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0761, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0768, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0758, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0754, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0754, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0757, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0754, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0747, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0758, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0760, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0756, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0752, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0758, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0759, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0756, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0751, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0734, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0758, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0759, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0750, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0755, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0759, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0753, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0756, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0759, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0756, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0747, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0747, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0757, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0754, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0748, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0759, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0760, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0750, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0749, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0754, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0756, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0758, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0752, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0745, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0743, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0754, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0751, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0754, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0742, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0753, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0749, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0756, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0749, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0746, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0745, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0752, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0745, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0744, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0749, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0754, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0744, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0748, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0751, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0736, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0742, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0750, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0750, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0750, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0739, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0746, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0744, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0750, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0753, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0743, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0741, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0755, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0737, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0750, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0742, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0742, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0746, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0741, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0741, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0733, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0754, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0740, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0742, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0742, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0742, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0733, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0745, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0740, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0739, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0745, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0742, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0737, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0743, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0743, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0741, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0732, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0734, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0735, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0739, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0736, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0735, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0734, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0742, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0737, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0734, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0731, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0741, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0734, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0735, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0733, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0737, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0733, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0737, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0734, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0736, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0739, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0730, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0723, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0729, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0730, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0737, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0728, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0731, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0723, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0724, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0732, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0730, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0732, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0735, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0726, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0726, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0732, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0730, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0731, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0731, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0730, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0729, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0726, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0726, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0726, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0728, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0729, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0731, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0725, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0730, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0724, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0729, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0726, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0726, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0730, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0725, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0723, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0721, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0719, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0724, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0726, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0721, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0725, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0721, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0719, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0725, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0718, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0722, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0717, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0721, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0721, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0717, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0722, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0720, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0721, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0724, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0717, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0719, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0723, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0716, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0715, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0718, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0716, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0724, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0715, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0715, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0718, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0720, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0712, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0717, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0717, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0718, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0714, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0714, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0709, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0718, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0714, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0712, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0710, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0708, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0717, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0715, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0709, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0708, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0713, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0713, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0711, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0707, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0708, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0711, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0711, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0708, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0712, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0711, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0711, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0707, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0707, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0711, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0712, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0706, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0705, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0708, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0706, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0705, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0701, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0704, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0709, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0705, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0708, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0699, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0703, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0700, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0707, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0709, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0703, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0702, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0700, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0702, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0698, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0707, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0708, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0703, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0701, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0701, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0700, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0700, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0698, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0697, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0702, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0698, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0706, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0702, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0704, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0707, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0699, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0705, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0700, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0700, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0698, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0698, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0693, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0699, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0701, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0695, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0695, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0699, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0696, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0696, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0696, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0696, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0696, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0691, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0694, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0697, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0694, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0691, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0695, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0692, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0692, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0695, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0696, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0689, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0692, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0690, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0694, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0692, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0692, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0685, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0687, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0692, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0689, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0685, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0689, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0688, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0690, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0690, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0684, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0688, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0688, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0685, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0685, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0687, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0685, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0684, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0686, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0686, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0683, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0689, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0684, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0686, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0685, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0688, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0683, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0681, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0684, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0684, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0684, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0679, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0682, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0683, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0687, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0687, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0682, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0680, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0678, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0681, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0679, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0678, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0683, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0678, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAAGdCAYAAAAfTAk2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB8UElEQVR4nO3deViU5frA8e/MwLAziKwqmzuIK25oLrmgppUtaptLmeU5dcw8/ipbzqlOJ091KjvtluaSqZVaWu7lmrig4oqIC4IKIir7zry/P15AyQWQGWYY7s91zTUv7zzzvPcgztzzrBpFURSEEEIIIWyI1tIBCCGEEEKYmiQ4QgghhLA5kuAIIYQQwuZIgiOEEEIImyMJjhBCCCFsjiQ4QgghhLA5kuAIIYQQwuZIgiOEEEIIm2Nn6QAswWg0cv78edzc3NBoNJYORwghhBDVoCgK2dnZNGnSBK321m00DTLBOX/+PAEBAZYOQwghhBC3ITk5mWbNmt2yTINMcNzc3AD1F+Tu7m7haIQQQghRHVlZWQQEBFR8jt9Kg0xwyrul3N3dJcERQggh6pnqDC+RQcZCCCGEsDmS4AghhBDC5kiCI4QQQgibIwmOEEIIIWyOJDhCCCGEsDmS4AghhBDC5kiCI4QQQgibIwmOEEIIIWyOJDhCCCGEsDlmTXC2bt3K3XffTZMmTdBoNPz0009VPmfLli1ERETg6OhI8+bN+eKLL64rs2zZMsLCwnBwcCAsLIwVK1aYIXohhBBC1FdmTXByc3Pp2LEjn3zySbXKnz59mrvuuos+ffqwf/9+Xn75ZaZMmcKyZcsqykRHRzNmzBjGjh3LgQMHGDt2LKNHj2bXrl3mehlCCCGEqGc0iqIodXIhjYYVK1YwcuTIm5Z58cUXWblyJXFxcRXnJk+ezIEDB4iOjgZgzJgxZGVlsWbNmooyQ4cOpVGjRixevLhasWRlZWEwGMjMzJS9qIQQQoh6oiaf31a12WZ0dDRRUVGVzg0ZMoQ5c+ZQXFyMvb090dHRPP/889eVmTVr1k3rLSwspLCwsOLnrKwsk8ZtzS7nFnH8QjYZeUVcySvmSl4RmXnF6LQaGjnr8XC2x9NFT2NXB9r6ueFor7N0yEIIIUStWVWCk5qaiq+vb6Vzvr6+lJSUkJ6ejr+//03LpKam3rTemTNn8sYbb5glZmuiKArHL+SwO/Ey+89cYV/SFRIv5VX7+XZaDe2auNM5sBFdghrRs7knPm6OZoxYCCGEMA+rSnDg+i3Qy3vQrj1/ozK32jp9xowZTJs2reLnrKwsAgICTBGuVUjLKuCn2HMs23uO+AvZ1z0e1NgZL1cHGjnbY3BSW21KjQpXylp1MvKKOJ9RQHpOIQfOZnLgbCbzdiSi1UCfVt48ENGMqDBfad0RQghRb1hVguPn53ddS0xaWhp2dnY0btz4lmX+3KpzLQcHBxwcHEwfsAUpisJvcWks3HmGbQkXMZaNpNLrtHQP8aRLoAedgxrROcADD2d9teo7eyWffUlX2J+UQcyZyxw+l8WW4xfZcvwibg52DO/gzxN3hNDa183Mr04IIYSoHatKcCIjI1m1alWlc+vXr6dr167Y29tXlNmwYUOlcTjr16+nV69edRqrpSiKwraEdN5fH8+Bs5kV5yOCGnF/l6aMaN8Eg7N9jevVaDQEeDoT4OnMvZ2aApCYnsvyfWdZtu8c5zLyWbInmaUxydzbsQlTB7Um2MvFZK9LCCGEMCWzJjg5OTmcOHGi4ufTp08TGxuLp6cngYGBzJgxg3PnzrFgwQJAnTH1ySefMG3aNCZNmkR0dDRz5sypNDvqueeeo2/fvrzzzjvce++9/Pzzz2zcuJHt27eb86VYhV2nLvH++uPsTrwMgJO9jnGRQTzUPZAQMyQbwV4uTItqw9RBrdl1+jLzdySy9kgqP8WeZ9XBFEZFNONvA1vR1MPJ5NcWQgghasOs08Q3b97MnXfeed358ePHM2/ePCZMmEBiYiKbN2+ueGzLli08//zzHDlyhCZNmvDiiy8yefLkSs//8ccfefXVVzl16hQtWrTg3//+N/fff3+146pv08Qz8op4Y9VRVuw/B4DeTsvYnkFM7tcCb7e67Xo7dDaTDzbEsyn+IgAOdlqmR7XhiTtC0GlvPg5KCCGEqK2afH7X2To41qQ+JThrD6fw6k9HSM8pRKuBh7sH8rcBrfAzWHZ2U0ziZd5dF8/u02prUudAD957sCMtfVwtGpcQQgjbJQlOFepDgnMpp5B/rDzCrwdTAGjl48q7D3agc2AjC0d2laIoLNmTzL9/jSOnsAS9nZbnB7VmUp8Q7HSyzZkQQgjTkgSnCtae4Ow9c5m/fLuPtOxCdFoNk/s1Z8rAVjjYWec07fMZ+cxYfogtx9Vuq+4hnnz2aBe8XG1r5poQQgjLkgSnCtac4Hy3K4l/rjxMcalCKx9XPhjdifbNDJYOq0qKorBs3zleX3mEnMISmhgc+XJs13oRuxBCiPqhJp/f0o9gJQpLSpmx/CAvrzhEcanCXe39+OmZ3vUmQdBoNDwY0YyfnulNc28XzmcW8MAXO1i296ylQxNCCNEASYJjBdKyC3h49k4W705Go4EXh7bl00e64OJgVcsUVUtLH1d+eqY3A9v6UFRi5O8/HOCNVUcoNTa4hkIhhBAWJAmOhSVfzmPUF9HsS8rA3dGOeY935y/9W9xy6wlr5+5oz1fjujJlYCsAvvkjkb8t3kdRidHCkQkhhGgoJMGxoBNp2Yz6Ipozl/II8HRi5bN30K+1t6XDMgmtVsO0wa357NEu6HVaVh9KZdKCGPKLSi0dmhBCiAZAEhwLOXwuk9Ff7iQ1q4BWPq78OLmXTW59cFd7f+ZM6IqTvY4txy8ybu4usgqKLR2WEEIIGycJjgXsPn2Zh2fv5HJuER2aGfj+6Uh83S27cJ859WnlzbdPdsfN0Y49iVd4ePZOLuUUWjosIYQQNkwSnDq298xlxs/dTXZhCT1CPFn0ZA8auVS923d9FxHkyZKnetLYRc+R81k88tUuMvKKLB2WEEIIGyUJTh06ej6LCd/sIb+4lL6tvZn/RHfcHGu+83d91a6Jge8nR+Lj5kD8hWwmfLOH3MISS4clhBDCBkmCU0dOp+cybu4usgtK6BrUiC8fi8DR3jpXJjanFt6ufPtkDzyc7YlNzuCphTEUlsjAYyGEEKYlCU4dSMnM57Gvd5GeU0SYvztzJnTDSd/wkptyrX3d+GZCN5z1Ov44cYnnFsdSUipTyIUQQpiOJDhmdjm3iMe+3sW5jHxCvFyY/0R3DE4Np1vqZjoHNuKrcV3R67SsPZLKjOWHMMpigEIIIUxEEhwzKiwp5akFMZy8mIu/wZGFE7vj7SYbUJbr3dKL/z3cGa0Gfth7llm/JVg6JCGEEDZCEhwzURSFl5cfJubMFdwc7Vg4sTvNGjlbOiyrMzTcj5n3twfgf78lsOrAeQtHJIQQwhZIgmMms7eeYtm+s+i0Gj59pAstfdwsHZLVGtMtkEl9QgCY/sMBDp7NsGxAQggh6j1JcMxg49EL/GftMQBeGx5KXxvZfsGcXhoWyp1tvCksMTJpQQypmQWWDkkIIUQ9JgmOiR1LzeK5JftRFHikRyDjewVbOqR6QafV8L+HO9PKx5ULWYU8tTCGgmKZPi6EEPXR2St5KIplJ45IgmNCl3IKeXJ+DLlFpUQ2b8wb97Sr17uC1zU3R3vmjO9GI2d7Dp7N5IUfD1r8P4gQQoiaOZeRz4iPt/Ps4v0WXcxVEhwTik/N5lJOEUGNnfns0S7Y6+TXW1OBjZ35/LEI7LQaVh44z7e7kiwdkhBCiGoqLjXyt+/2kZFXTPLlPOx0lvuSL5/AJtSrpRc/TI5kzviuDWJ/KXPp2bwxLw1rC8C/Vh3l8LlMC0ckhBCiOt5de4x9SRm4Odrx6SNdcLCz3KK2kuCYWHhTg8yYMoGJd4QwKNSXolIjf120j6yCYkuHJIQQ4hY2HL3AV9tOA/Degx0J8LTs0iiS4AirpNFoeH9UR5p6OJF0OY8XZTyOEEJYreTLefz9+1gAnugdwtBwP8sGhCQ4wooZnO359NEu2Os0rDmcyvwdiZYOSQghxJ8UlRh5dvF+sgpK6BjgUTHEwNIkwRFWrVOABzOGhQLw79VxHDor43GEEMKavLP2GAeSMzA42fPpI53R21lHamEdUQhxC4/3DmZIO1+KSxWmLt0v6+MIIYSV2J6Qzpzt6rib/47qaFVbEkmCI6yeRqPhnQc64OPmwMmLufxnzTFLhySEEA1eZn4x//fjAQAe7RHI4DBfC0dUmSQ4ol7wcNbz7oMdAJi3I5HtCekWjkgIIRq2f/58mJTMAoIbO/PK8FBLh3MdSXBEvdG/jQ+P9ggE4P9+PEBmvkwdF0IIS/j1YAo/xZ5Hq4EPxnTCWW9n6ZCuIwmOqFdeGR5KcGNnUjILeH3lEUuHI4QQDU5aVgGv/HQIgGfubEmXwEYWjujG6iTB+eyzzwgJCcHR0ZGIiAi2bdt207ITJkxAo9Fcd2vXrl1FmXnz5t2wTEGB7EBt65z1dnwwphNaDazYf47Vh1IsHZIQQjQYiqLwwrKDZOQVE97UnSkDW1k6pJsye4KzdOlSpk6dyiuvvML+/fvp06cPw4YNIynpxnsMffTRR6SkpFTckpOT8fT0ZNSoUZXKubu7VyqXkpKCo6OjuV+OsAJdAhvx1/4tAXhlxSHScwotHJEQQjQM38ckszn+Ino7LR+O7mTVey6aPbIPPviAiRMn8uSTTxIaGsqsWbMICAjg888/v2F5g8GAn59fxS0mJoYrV67w+OOPVyqn0WgqlfPzs/yqiaLuTBnYilB/d67kFfOvX45aOhwhhLB5aVkF/PvXOAD+L6oNrXyte1sisyY4RUVF7N27l6ioqErno6Ki2LFjR7XqmDNnDoMGDSIoKKjS+ZycHIKCgmjWrBkjRoxg//79N62jsLCQrKysSjdRv+nttLzzQHu0Gvg59jybjqVZOiQhhLBpr686QlZBCR2aGXi8d7Clw6mSWROc9PR0SktL8fWtPDfe19eX1NTUKp+fkpLCmjVrePLJJyudb9u2LfPmzWPlypUsXrwYR0dHevfuTUJCwg3rmTlzJgaDoeIWEBBw+y9KWI0OzTyYeEcIoHZV5RSWWDgiIYSwTeuOpLL6UCo6rYb/3N8BOyvumipXJxFqNJpKPyuKct25G5k3bx4eHh6MHDmy0vmePXvy2GOP0bFjR/r06cP3339P69at+fjjj29Yz4wZM8jMzKy4JScn3/ZrEdbl+cGtCfB04nxmAf9dF2/pcIQQwuZkFRTzj58PA/B03+aENXG3cETVY9YEx8vLC51Od11rTVpa2nWtOn+mKApz585l7Nix6PX6W5bVarV069btpi04Dg4OuLu7V7oJ2+Cst+Pt+9oDMD86kX1JVywckRBC2JZ31hzjQlYhIV4uVj1r6s/MmuDo9XoiIiLYsGFDpfMbNmygV69et3zuli1bOHHiBBMnTqzyOoqiEBsbi7+/f63iFfVTn1bePNClGYoCLy07SFGJ0dIhCSGETdh16hKLdqmznmfe3x5He52FI6o+s3dRTZs2ja+//pq5c+cSFxfH888/T1JSEpMnTwbU7qNx48Zd97w5c+bQo0cPwsPDr3vsjTfeYN26dZw6dYrY2FgmTpxIbGxsRZ2i4Xl1eCiNXfQcv5DDl1tOWjocIYSo94pKjMxYoS7o93D3AHo2b2zhiGrG7GsrjxkzhkuXLvHmm2+SkpJCeHg4q1evrpgVlZKSct2aOJmZmSxbtoyPPvrohnVmZGTw1FNPkZqaisFgoHPnzmzdupXu3bub++UIK9XIRc8/7g7juSWxfLLpBCM7NyXA03p2tRVCiPpmzvbTnLqYi5erAy8Ns769pqqiURRFsXQQdS0rKwuDwUBmZqaMx7EhiqLwyFe7iD51iagwX2aP62rpkIQQol46n5HPwPe3kF9cygejO3J/l2aWDgmo2ee39c/zEqKaNBoNb9zbDjuthvVHL7ApXtbGEUKI2/HvX+PILy6lW3Aj7uvc1NLh3BZJcIRNae3rVrEA1Rsrj1BYUmrZgIQQop7ZnpDOr4dS0GrgjXvCq7WsizWSBEfYnOcGtcbHzYHES3l8tfWUpcMRQoh6o6jEyD9WqmvejIsMrjdr3tyIJDjC5rg62PHKcHVA3CebTnD2Sp6FIxJCiPrh6sBiPc8Pbm3pcGpFEhxhk+7p2IQeIZ4UFBtlM04hhKiGlMx8Pv5dXTB3xrBQDE72Fo6odiTBETZJo9Hw5r3h6LQa1h25wPaEdEuHJIQQVu0/a46RV1RK16BG3N+lfg4svpYkOMJmtfFzY2xPdb2lt349Sqmxwa2IIIQQ1bI/6Qo/x55Ho4HX72lXbwcWX0sSHGHTnhvYCoOTPcdSs/khRjZZFUKIP1MUpaIr/4EuzQhvarBwRKYhCY6waY1c9BWbw/13/XFyCkssHJEQQliXXw6msC8pAyd7Hf83pI2lwzEZSXCEzRvbM4gQLxfScwr5bNMJS4cjhBBWo6C4lP+sOQbA5H4t8HV3tHBEpiMJjrB5ejstM4a1BeDr7adl2rgQQpSZs/005zLy8Tc48lTf5pYOx6QkwRENwuAwXyKbN6aoxMg7a+MtHY4QQljcxeyrrdovDG2Dk15n4YhMSxIc0SBoNBpeHRGKRgOrDpxn75krlg5JCCEs6oMN8eQWldKhmYF7O9b/aeF/JgmOaDDaNTEwKkLdEfffvx5FUWTauBCiYUq4kM3SPerM0tdGhKHV1v9p4X8mCY5oUKZHtcHRXsu+pAw2HL1g6XCEEMIi3l0Xj1GBIe186RbsaelwzEISHNGg+Lg78kTvEADeWxcvi/8JIRqcmMTLbDh6Aa0G/m9IW0uHYzaS4IgG5+l+LfBwtichLYdl+85aOhwhhKgziqLwzlp1WvjorgG09HG1cETmY2fpAISoawYne57p35J/r47jww3HuadjExztdVBaAmlH4fw+SDkABVmglIKxBIyloNGCe1MwNFNvHoHg3QYc3Cz9koQQDUlJIVxJhMun4cppuHwKss6r71PGkrL3rVLQu4KrD7j6qvdu/mzNC2ZP4hUc7LRMHVS/dwuviiQ4pnYlETKSIaSPpSMRtzA2Mohv/jiNS9YJjixaQ0TpQUg5CCX5NatIawfNukHzO6HFndCkC+jkv5UQwsSMRkjcBrHfQdxKKL699bz6Aev0zcjxj8TvfAno+4CTh0lDtRYapQFOJcnKysJgMJCZmYm7u7vpKj65CRaOBEMgTD0INrBZmU1KT4DDy8nc+z2G7D+tbOzgDk06qzdXXzWB0erUW2kxZJ2DzLPq7coZyD5f+fmOHnDHVOj5V7BzqKtXJISwVVkpEDMXDiyGzGv209O7gWeIemsUorYq6/RX37M0OijKhpw0yLkAOWlknIvHIzuhcv06Bwi9G7qMg+A+oLXukSs1+fyWBMeUCU5xPrzbAopzYdImaNrFdHWL26coarfTsV8h7hdIv7rQXzF2bC7tQHGru7jrrpHg2aJm/8GvJKqJ7cnf4fQWKMhUz3u2gKH/gdZRJn0pQogGQlHg0A/w63QoLHtfcTBA+weg06PQNKJGX6ILiksZ8N/N5Gem8U5ENlHOx9X3rPTjVws1CobOY6HTI+DexLSvx0QkwamC2RIcgB8mwJEV0Ps5GPymaesW1VeQCae3qonH8XVqy0s5rZ3apdTuPjZpuvP4kuM42mvZ8n931m4fFmMpHPweNv5T/cYE0GoIDJ0JjVvU7vUIIRqOvMvwy1Q4+rP6c5POEPkstB0O9k63VeXX207x1q9x+Bsc2TS9vzruEOB8LOybD4d+hMIs9ZxGC837q4lULa5pDpLgVMGsCc6RFWqS0ygYpsRKN1VdSouDoyvh5G9wNkYdaFfO3gVaDYK2d0OrwRV9zoqi8OAX0ew9c4VxkUG8eW947eMoyIKt78HOz8FYrHZ7jftZWvSEEFU7vh5WPqt+SdLaQb+X4I7nazW2L7ewhD7vbuJybhHvPNCeMd0Cry9UlKsmVPsWQFL01fMO7hB6D7QZqiY9Fp5UIQlOFcya4BTmwHst1cGqT28F/46mrV9UdiURDi+DQ8sg7Ujlxxq3Ugf+thwEIf3A/satMztOpvPIV7vQ67Rs+r/+NPUw0beV9AT46a9wdjc4NYIJq8E3zDR1CyFsz45PYP0r6rFXG7j/S7X1ppY+3XSC99bFE9zYmY3T+mGnq6Ib/vIpOLBEHfeTkXT1vNYegiKhVRS0HaGO/6ljkuBUwawJDsDSxyBuFfT5Owz8h+nrb+iMRohfDdGfQtKOq+e19mrrTOuhamLjcYNvKTfxyFc72XHyEg93D2Dm/R1MF2thNiy4F87tVQctP75GuquEENfb+h78/pZ63G0SRP3LJF1DWQXF9HlnE5n5xcwa04mRnWuw55TRqL7Hxv0CCevh8snKjwf0gPajoN394NK41rFWhyQ4VTB7gnPoR1g2UR1o+re90k1lKsUF6jeK6E/gUtnsJ41WHfnf/kF1JoBTo9uqeu+ZyzzweTQ6rYbf/96PoMYupos77zLMvxsuHAZDgJrkeASYrn4hRP2lKLDp32qCA3Dnq9Dv/0xW/YcbjvPRbwm08nFl7dS+6Gqz59Slk5CwQf2CmbgNFKN6XmunfrEc9Dp4tTJJ3DcjCU4VzJ7gFGars6lKC2HyH+BngnEdDVnmWdg7H/Z+A7kX1XOOBug6Ebo/Be7+JrnM+Lm72XL8Ivd3acoHozuZpM4KOWnwzTA1MfNsoSY5br6mvYYQon5RFFj/qvqlDWDwv6D3FJNVfyW3iD7vbiKnsIRPH+nC8A6mea8E1Onrh5fBoe/VhVFBnXLe9//USTZ2etNd69rL1uDz27onvNdXDm7quA+4Ogpe1IzRCAkbYfHDMKs9bH1XTW4MATBkJjx/BAb902TJDcDfo9RVPX/af44TaTkmqxdQVxEd97O6RtLlk7B8kvoahRAN17qXryY3w94zaXIDMHvbKXIKSwj1d2dYuJ9J68bdH3o9q441/etOaDFQ/VK/6S34si8k7zbt9W6DJDjmEnavei8JTtVKitSBbHG/wKa34bsx8EFbWPSA2hSqGNVuqAe/gSn7IfKvZhnJ36GZB4PDfDEqMGvj8aqfUFOGZjB2Odg7q+tP7J5t+msIIeqHvfNh52eABu7+H/R4yqTVp+cUMu+PRACmDW6NtjZdU1XxCYXHlsH9X4OzF1yMgzlR6ho+Rbnmu24VZE15c2kzVB30mh4PacfAp4Y7thbmqANT869A/mX1vigPwu6pXzOzFEVdfTPtmPpHn3ZM/Z3kXVK78gqzobToxs91NEDHR6DrE+BdN3umTBvcmg1HL/DLwRSeHZBFWz8Td2F6tVLXR1o9XV0vp8Wd6n5WQoiG42yM+h4AMOBViBhv8kt8vvkk+cWldGxmYFCoj8nrv45GAx1GQcuBardb7CJI3K5+DlqIJDjm4miAFgMgYZ3ailOTBOfyKfhm+PXbAADs+B/c9R5ETDBZqGaRlQIHvoP936qvpypaO/VbgF9H8O+gJnH+Het8galQf3eGd/Dn14MpfLjhOF+O7Wr6i3R7EuLXqOv1rHgaJm4AneXeBIQQdSj7Aiwdq36xC71bnW1rYmlZBXy78wwAzw9ujaYuJ7o4e8LIz6DDGHBwNdtYnOqoky6qzz77jJCQEBwdHYmIiGDbtm03Lbt582Y0Gs11t2PHjlUqt2zZMsLCwnBwcCAsLIwVK1aY+2XU3O10U2Ukwfx71OTG2QsCekKbu6DTY+oiS6VFsOo5+PlZdVaRNSktVruZvhsDH7aD395UkxutPfiEqVMJ73wFRi9QP9T/Eg1TD8OLifDKBZi8HUZ+Cj2ehsCeFls98/lBrdBoYN2RC8SlZJn+AhoN3PuJum/V+f2w9b+mv4YQwvqUFMEP49X3d682MPJzs8yy/WLLKQpLjHQJ9KBfa2+T118tzfup20lYkNlbcJYuXcrUqVP57LPP6N27N19++SXDhg3j6NGjBAbefJ2S+Pj4SiOkvb2v/iNFR0czZswY/vWvf3HfffexYsUKRo8ezfbt2+nRo4dZX0+NtL0LVtmpC9ClJ1Q9fS4rRU1uMpOhcUt1YbhrZ9ooCmz/EH7/F+xfqE47Hr3w9qccG42QvFNdfTl5t7q6r5s/uPmp954toFnXqneaTTmo7nB76AfIS796PqAndBkLYSPVTL6eaOnjxvD2/vxyMIWPf0/gs0fN8J/UvQkMf19dTmDre+qeVRZ+MxBCmNn6V9RVgh3c4aFFZhlLmJZdwKJdauvNc4PquPXGyph9mniPHj3o0qULn3/+ecW50NBQRo4cycyZM68rv3nzZu68806uXLmCh4fHDescM2YMWVlZrFmzpuLc0KFDadSoEYsXL64yJrNPE7/Wtw/AiY3qUtej5t98I8ecizDvLnXjM48gdRqx4SYLMp34Tf1gzL8CTp5w96yrrUXVcTZG3TMpbiVkp1RRWAPebSGge9lWAxp1v5KCLPU+8Q+4cOhqcRcf6PiQumFbHY2bMYf41GyGzNoKwLqpfWnjZ6blyX94HI4sV1ddnrz9pqstCyHqufJtfAAeXgJthpnlMm+vjmP21lN0CvBgxV972VyCU5PPb7O24BQVFbF3715eeumlSuejoqLYsWPHTZ6l6ty5MwUFBYSFhfHqq69y5513VjwWHR3N888/X6n8kCFDmDVr1g3rKiwspLCwsOLnrCwzdDvcTJ/pcGqLmkxseA2G/Pv6MrnpsHCkmty4N4Xxq26e3IA6iOupLeqKyakH4ftxEP6gOjbH2fPmz0vcDpv/oy7QVM7BoG6m1mowlBSqCU/OBcg6r7YQXT6lDg6+GKduyHYjOn1ZN9qj6rijWuyZYi3a+LkxLNyPNYdT+WTTCT5+uPbLpd/Q8PfhzA64lAA7PjbpAl9CCCtRlAfrXlWP75hmtuQmPaeQhdFlrTcDW9lcclNTZv0kSk9Pp7S0FF/fygua+fr6kpqaesPn+Pv7M3v2bCIiIigsLGThwoUMHDiQzZs307dvXwBSU1NrVOfMmTN54403TPCKbkNQpNrPuvxJdb0D96bqNOdyJzfBismQk6ou5T9+FTQKqrreRkHw5EbY8o7abXX4RzVxufujq/95FEUdF5MUrZY784d6XmsP4fdD+APquB47h5tfJ+eiupdS8i5IPaQu5OTorjatOrircYTec+vEqp7624BWrDmcyi8Hz/PcwJa09DFDK46zp5r0LpsI296HjmNqtMWEEKIe2PExZJ1V1/Hq94LZLvP1ttPkF5fSoZmB/m0sNPbGitTJV+0/Z5GKotw0s2zTpg1t2lydNhsZGUlycjL//e9/KxKcmtY5Y8YMpk2bVvFzVlYWAQF1uFR+h1HqH/fG19WFndz9oc1wdUGkP/4HKOqAszHf1myfIjsHda+rNsPhp8lqC9Dih9TtCkoKoaTg6lLaoLa0dB6r7kxb3XE7rt5qC0/b4TV5xTYhrIk7UWG+rD96gU9+P8Gsh8zUihP+AMR8A2e2w7pXYMxC81xHCFH3Ms/BH7PU48FvmG3yxOXcIhZEJwIwZYC03oCZZ1F5eXmh0+mua1lJS0u7rgXmVnr27ElCQkLFz35+fjWq08HBAXd390q3Otd7qrqBGgosfwq+GgB/fKT+HDEBntp8+2NWmkXA09ug198AjTo2pzjvanJj56hee0osjPhA9kGqgSkD1YHhKw+c59RFE69uXE6jgbveBY1O7co8+bt5riOEqHu/vaG+Hwf0VGeSmsmc7afIKyqlXRN3BtbFujf1gFkTHL1eT0REBBs2bKh0fsOGDfTq1ava9ezfvx9//6tL8kdGRl5X5/r162tUZ53TaGDYO+oW86VF6sBcRw91FtTdH4HeuXb12ztC1Fvw93h4Zjc8d0A9fjERZpyF4f+99bgecUPhTQ0MbOuDUYFPN52s+gm3y7eduq8WwOoX1OmkQoj67WwMHFyqHg+dabaNlzPyipi/Qx17M0XG3lQwexfVtGnTGDt2LF27diUyMpLZs2eTlJTE5MmTAbX76Ny5cyxYsACAWbNmERwcTLt27SgqKuLbb79l2bJlLFu2rKLO5557jr59+/LOO+9w77338vPPP7Nx40a2b99u7pdTO1odPPA1/PyM2oU07B11+X5TcvOVTRxNbMrAVvx2LI2fYs8xZWBL0+40fq3+L6ljqS4lwK7P1Q3rhBD1k6LA2rIJNp0eLZuFah5z/0gkp7CEtn5uDA6V9/9yZk9wxowZw6VLl3jzzTdJSUkhPDyc1atXExSkDqRNSUkhKSmponxRURHTp0/n3LlzODk50a5dO3799VfuuuuuijK9evViyZIlvPrqq7z22mu0aNGCpUuXWtcaODdj7wQPzrV0FKIGOgZ40L+NN5vjL/LFlpPMvL+DeS7k5KFu4/DTX2DLu9B+lLpejhCi/jn0A5zdA/Yu6lhJM8kuKGbeH6cBdWKEWfecqmfMvg6ONarTdXCETYhJvMyDX0Rjr9Ow7YUB+BnMtF6N0Qhzh6gz1zo8BPd/aZ7rCCHMp7QYPuqkTi4Z8Br0nW62S32x5ST/WXOMFt4ubHi+n80nODX5/JbdxIWohq7BnnQP8aS4VOGrbdXYW+t2abVq1yWoffcX4813LSGEeRz7RU1uXH0h8lmzXaaguJSvt6mtN3/p39Lmk5uakgRHiGp65s6WAHy3K4nLuWYcBNy0izoYHQU2X7/atxDCyu2Zo953GW/W1cl/iEkmPaeQph5O3NtJurP/TBIcIaqpbysvwpu6k19cWtHnbTb9Z6j3R1aoCywKIeqHi/HqoqsanboEiJkUlxr5Yovamvx0v+bY6+Tj/M/kNyJENWk0Gp7pr7bizNuRSHZBsfku5hd+dc2MTdKKI0S9Ud5602aYWZfmWBl7nnMZ+Xi56hndVdY2uxFJcISogSHt/Gjh7UJWQQmLdiVV/YTa6D8DNFqI/xXO7TPvtYQQtVeYAwfKNnzuNtFslzEaFT7bfAKAiXc0x9FeZ7Zr1WeS4AhRA1qthr+UteJ8ve00BcWl5ruYd2toP1o93nSDTVqFENbl0A9QmAWeLSCkv9kus/5oKicv5uLmaMdjPWXvupuRBEeIGrq3UxOaejiRnlPIDzHJ5r1Y/xfVvvwTGyFpp3mvJYS4fYpytXuq20R1RqRZLqNUrKo+oVcwbo72ZrmOLZAER4gastdpebpfcwC+2HKK4lJjFc+oBc/m0PlR9fj3t8x3HSFE7Zzdo27BY+cIHR8222W2JaRz6FwmTvY6Hu8dYrbr2AJJcIS4DaO7BuDlqudcRj6rD6WY92J9/w+09urMDGnFEcI67flavQ9/EJw9zXaZL7eqrTdjugXg6aI323VsgSQ4QtwGR3sdE3oFA2orjlkXBPcIhI4Pqce7Z5vvOkKI25Obri7pAGYdXHzobCZ/nLiETqvhyT7SelMVSXCEuE2P9QzCWa8jLiWLrQnp5r1Y90nq/dGfITvVvNcSQtRM7CIoLYImnc26qeYXZa0393RsQrNGzma7jq2QBEeI2+ThrOfh7uoMhi82nzTvxfw7QkBPMJbA3nnmvZYQomYO/qDedxlvtkucuZTLmrLu8Kf6NjfbdWyJJDhC1MLEO0Kw02qIPnWJA8kZ5r1YeStOzFwoMeNWEUKI6rsYrw4u1tpB2L1mu8xX205hVKB/G29C/WWT6OqQBEeIWmji4cQ9ZXvAlA/+M5vQe9TN+3IuwLFV5r2WEKJ6Di9T71sMNNvgYnVJirMAPN23hVmuYYskwRGilsrfcNYcTuV0eq75LmSnh4jH1ePdX5nvOkKI6lGUqwlO+wfNdpn5OxIpLDHSMcCDns3NN0PL1kiCI0QttfFzY0BbHxRFbUY2q4gJalN4UjSkHDTvtYQQt5Z6EC6dUNe+aTPMLJfILSxhQfQZACb3bY5GozHLdWyRJDhCmMDTZYP+ftx7lrTsAvNdyN1f7aoC2COtOEJYVHnrTesh4OBmlkss2ZNMZn4xIV4uRLXzM8s1bJUkOEKYQPcQTzoHelBUYmT+jkQzX+wp9f7gD5B/xbzXEkLcmNEIh5erx+Hm6Z4qLjUyd/tpACb1aY5OK603NSEJjhAmoNFoKlpxvt2ZRF5RifkuFtgTfNtDST7sX2S+6wghbu7sHshMBr0btBpslkusPpTCuYx8vFz13N+lqVmuYcskwRHCRAaH+RHU2JnM/OKKGQ9modFA9yfV473fqAMdhRB1q7x7qu1wsHcyefWKolSM6RsXGYyjvc7k17B1kuAIYSI6rYYn71CXT5+z/TSlRjMmHuEPgL2zOsDx3F7zXUcIcT1j6dWtGcw0eyr61CUOn8vC0V7LYz2DzHINWycJjhAm9GBEAB7O9iRdzmP9ETNuqeDgBqF3q8cHFpvvOkKI6yVug9w0cGoEzfub5RJfbVVbb0ZFyKaat0sSHCFMyEmvY2zZt63Z5p4y3mGMen94maxsLERdKu+eCrsXdPYmrz7hQjab4i+i0airpYvbIwmOECY2LjIYvU7L/qQM9p65bL4LNe8Prn7qTKqE9ea7jhDiqpIiOLpSPQ5/wCyX+HqbOnMqKsyXYC8Xs1yjIZAERwgT83Zz4L7O6oyH2VvN2Iqj1UGH0eqxdFMJUTdO/g4FGeq2KUG9TV59WnYBK/afA2RTzdqSBEcIM3iyj9qsvP7oBfNu39DxYfX++DrIM2NrkRBCdahs5/DwB9QvGSa2YMcZikqNdAn0ICJItmWoDUlwhDCDVr5Xt2+Ys92MrTi+YeDXHozFV8cFCCHMoygX4lerx2ZY3C+vqIRvd6nbMkjrTe1JgiOEmZS34vy49yxXcs04CLi8FefAEvNdQwgB8WugOA8ahUDTLiavftnes2TkFRPo6czgMNmWobYkwRHCTCKbN6ZdE3cKio18tzvJfBcKfxA0OjgXA+knzHcdIRq6Qz+q9+EPqAtumpDRqDD3j0RAnTkl2zLUniQ4QpiJRqOpmOI5f0ciRSVG81zIzRdaDFCPD0orjhBmkXcZTmxUj9uPMnn1vx9L43R6Lu6OdjwY0czk9TdEkuAIYUYjOjTBx82BtOxCfjl43nwX6viQen9gqboJoBDCtOJWqWPdfMPBp63Jq/+6bKzewz0CcXGwM3n9DVGdJDifffYZISEhODo6EhERwbZt225advny5QwePBhvb2/c3d2JjIxk3bp1lcrMmzcPjUZz3a2goMDcL0WIGtHbaRnfKxhQt29QzLVvVNvh4OAOmUmQtMM81xCiITt8TfeUqas+l8nOU5fRaTWMjww2ef0NldkTnKVLlzJ16lReeeUV9u/fT58+fRg2bBhJSTcek7B161YGDx7M6tWr2bt3L3feeSd33303+/fvr1TO3d2dlJSUSjdHR0dzvxwhauyR7oE42ms5cj6LnafMNJXb3gnC7lGPDy41zzWEaKiyUuB02RdzMyQ4c7erC/vd1d6fJh6m37izoTJ7gvPBBx8wceJEnnzySUJDQ5k1axYBAQF8/vnnNyw/a9YsXnjhBbp160arVq14++23adWqFatWrapUTqPR4OfnV+kmhDVq5KLngS5qn/qcsjcysyjfuuHIz1AsrZlCmMyRFYACAT2gkWk3vryQVcCqsu5r2ZbBtMya4BQVFbF3716ioqIqnY+KimLHjuo1oxuNRrKzs/H0rLzgUU5ODkFBQTRr1owRI0Zc18IjhDV5ouyN67djZlz4L+gOcG8KhZmydYMQplTRPWX6tW8WRCdSXKrQNagRnQI8TF5/Q2bWBCc9PZ3S0lJ8fX0rnff19SU1tXo7Lb///vvk5uYyevToinNt27Zl3rx5rFy5ksWLF+Po6Ejv3r1JSEi4YR2FhYVkZWVVuglRl1p4u1Ys/PfNH2ZqxdFqrzafSzeVEKZx6SSc2wsaLbQbadKq84tKWbRLHa5Rvm6WMJ06GWSs+dN6AYqiXHfuRhYvXszrr7/O0qVL8fHxqTjfs2dPHnvsMTp27EifPn34/vvvad26NR9//PEN65k5cyYGg6HiFhAQULsXJMRtKG9+/iHmLJl5xea5SHk3VcJ6dRNOIUTtHF6u3of0A1efW5etoWX71IX9AjydZGE/MzBrguPl5YVOp7uutSYtLe26Vp0/W7p0KRMnTuT7779n0KBBtyyr1Wrp1q3bTVtwZsyYQWZmZsUtOTm5Zi9ECBPo1aIxbf3cyC8uNd/Cf37h4NMOSovg6M/muYYQDYWiQOwi9djEa98YjUpFa+7jvWRhP3Mwa4Kj1+uJiIhgw4YNlc5v2LCBXr163fR5ixcvZsKECXz33XcMHz68yusoikJsbCz+/v43fNzBwQF3d/dKNyHq2rUL/6n97mZar6ZD2RvxwR/MU78QDUXidrhyGvRuEHavSavemnCRkxdzcXWwY1RXWdjPHMzeRTVt2jS+/vpr5s6dS1xcHM8//zxJSUlMnjwZUFtXxo0bV1F+8eLFjBs3jvfff5+ePXuSmppKamoqmZmZFWXeeOMN1q1bx6lTp4iNjWXixInExsZW1CmEtbq7YxO8XPWkZBaw7kj1xqHVWPk3zTPbIUNaK4W4bfsWqPftHwAHV5NWXb4tw5huAbg52pu0bqEye4IzZswYZs2axZtvvkmnTp3YunUrq1evJihInWqXkpJSaU2cL7/8kpKSEp555hn8/f0rbs8991xFmYyMDJ566ilCQ0OJiori3LlzbN26le7du5v75QhRK472Oh7pof7tzzXXlHFDMwjuox4fklYcIW5L/hWIW6kedx5367I1lHAhm63HL6LRIAv7mZFGMdvSqtYrKysLg8FAZmamdFeJOpeWXUDv//xOcanCT8/0Ns/U0L3zYdUU8A6Fv0abfGNAIWze7q9g9XR1TNtf/jDp/6GXVxziu11JRIX5MntcV5PV2xDU5PNb9qISoo75uDlyd8cmgBmnjIfdCzo9XIyDC4fNcw0hbJWiqF8SALqMM2lyk5FXxPJ9Z4Gr62MJ85AERwgLeKK3+sb268EUUjPNsOqwkwe0HqoeH/ze9PULYctSYuHCIdA5QIfRVRavicW7kykoNhLm706PEM+qnyBumyQ4QlhAeFMD3YM9KTEqfLvzjHkuUv7GfOgHMJaa5xpC2KJ9C9X70BHgbLokpLjUyILoRAAe7x1crfXgxO2TBEcIC3nijmAAFu06Q0GxGRKQVlHg6AHZKZC4zfT1C2GLivKuDs7vYtrBxeuOpJKSWYCXq76im1qYjyQ4QljI4DA/mno4cSWvmJ/2nzP9BewcoN196rF0UwlRPUd/hsIs8AiC4L4mrbp85uSjPYJwtNeZtG5xPUlwhLAQnVbDhF7BAMz94zRmmdBYvnXD0ZXqN1MhxK2Vr33TZay6v5uJxCZnsC8pA71Oy6M9A01Wr7g5SXCEsKDR3QJw1us4fiGHHScvmf4CgT3BIxCKsiF+tenrF8KWpCdA0g51Y81Oj5q06vIZkyM6+uPj5mjSusWNSYIjhAUZnOx5oIu6TPs3ZSubmpRGc7UVR7qphLi1fWVTw1sOBnfTjZG5kFXArwdTgKszKIX5SYIjhIWNL+um+u3YBc5cyjX9BcoTnBMbIeei6esXwhaUFELsd+px18dNWvWinWcoMSp0C25EeFODSesWNycJjhAW1tLHlb6tvVEUWBBthinjXq2gSRdQSuHIctPXL4QtOPYL5F0CtyZqC46JFJaUsmiXuh3RhF7SelOXJMERwgo83jsYgO/3JJNTWGL6C1R0Uy01fd1C2IK989T7LmNBZ2eyalcdSOFSbhH+BkeGtPM1Wb2iapLgCGEF+rXyprmXC9mFJRXLuJtU+AOg0cG5vepASiHEVZdOwumtgAY6P2ayahVFqRhcPDYyCDudfOTWJfltC2EFtFpNxViceTsSMRpNPGXc1RtaDlSPZbCxEJVVDC4epM46NJGYM1c4cj4LBzstD3eTqeF1TRIcIazEAxHNcHOw49TFXLYmmGEw8LXdVOZYc0eI+qikCPYvUo8jJpi06vLWm/s6N6WRi96kdYuqSYIjhJVwdbBjVNcAQG3FMbk2d4HeDTLOQNJO09cvRH0Uvxry0sHVD1oPMVm15zLyWXfkAgATysbYibolCY4QVmR8ryA0Gtgcf5GTF3NMW7neGULvVo9lNpUQqvLBxZ0fA529yapdGH2GUqNCZPPGtPVzN1m9ovokwRHCigQ1dmFgWx8A5pujFafdSPU+bhUYjaavX4j65PJpOLUJ0Kizp0wkv6iUJXvUqeGPS+uNxUiCI4SVKV8rY9nes2QXFJu28ub91W6q7BQ4F2PauoWob8r3nWoxABoFm6zan2PPkZFXTLNGTgwMlanhliIJjhBWpnfLxrT0cSW3qJQf95p4yridw9VxBkd/Nm3dQtQnxlI4sFg9jhhvsmoVRakYQzc+MhidVmOyukXNSIIjhJXRaK7uMj7fHFPGw+5R7+NWyWwq0XCd2aG2ZDoaoPVQk1W76/RljqVm42SvY3TZpAFhGZLgCGGF7u/SFDdHOxIv5bHluImnjLccBHZO6myq1IOmrVuI+uJQ2XpQYfeqLZsmMq9s09z7ujTF4Gy6Qcui5iTBEcIKOevtGFP27e8bUw821rtAq0Hq8dGVpq1biPqgpPBqF237USar9lxGPuuPpgJq95SwLElwhLBS4yKD0Whg63EzTBkPvVe9j5MERzRAJzZCQSa4+UNQb5NVuzD6DEYFerVoTBs/N5PVK26PJDhCWKnAxs4MbKvOwFhg6lac1kNAp4f045B2zLR1C2HtyrcrCX8AtDqTVFlQfHVqePkYOmFZkuAIYcXK3yh/NPWUcUd3aH6neiytOKIhKciC42vVYxN2T8nUcOsjCY4QVsysU8bLZ1PJOBzRkBz7FUoKoHEr8O9okirVXcMTARgXGSRTw62EJDhCWDGzThlvcxdodHDhEFw+Zbp6hbBm5bOn2o8CjWkSkfKp4Y72WpkabkUkwRHCypltyrizJwTfoR5LK45oCHLS4NRm9bj9gyartnxblfs6N8XDWXYNtxaS4Ahh5a6dMm7yXcYrFv2TBEc0AEdWgGKEphHQuIVJqlR3DS+bGi6Di62KJDhC1APlU8a3mHrKeNuy3cXP7VW/3Qphyw5e0z1lIt/uVKeGy67h1kcSHCHqAXXKuLrL+MLoM6ar2M0X/Nqrx6e3mq5eIazN5VPqBrMaLbS73yRVFhSXsni3OjVcWm+sT50kOJ999hkhISE4OjoSERHBtm3bbll+y5YtRERE4OjoSPPmzfniiy+uK7Ns2TLCwsJwcHAgLCyMFStWmCt8IazCeHNNGW/eX70/ucl0dQphbcpXLg7uoyb2JrAy9jwZecU09XBiUKiPSeoUpmP2BGfp0qVMnTqVV155hf3799OnTx+GDRtGUlLSDcufPn2au+66iz59+rB//35efvllpkyZwrJlyyrKREdHM2bMGMaOHcuBAwcYO3Yso0ePZteuXeZ+OUJYzB0tvWjh7UJOYQnLTDllvDzBObVZNt8UtuvYr+p9+bizWrp21/CxkUHY6aRDxNpoFMW872g9evSgS5cufP755xXnQkNDGTlyJDNnzryu/IsvvsjKlSuJi4urODd58mQOHDhAdHQ0AGPGjCErK4s1a9ZUlBk6dCiNGjVi8eLFVcaUlZWFwWAgMzMTd3fpMxX1x8LoRF77+QjNvVzYOK0fWlOst1GUB+8EQWkRPLsXvFrWvk4hrEl2KrzfRj2eFgfuTWpd5e7Tlxn9ZTQOdlp2zhhIIxeZPVUXavL5bdaUs6ioiL179xIVFVXpfFRUFDt27Ljhc6Kjo68rP2TIEGJiYiguLr5lmZvVKYStuL9LM9wc7DiVnsu2E+mmqVTvDAE91ONT0k0lbFD8avW+aYRJkhuoPDVckhvrZNYEJz09ndLSUnx9K/d3+vr6kpqaesPnpKam3rB8SUkJ6enptyxzszoLCwvJysqqdBOiPnJxsOPBrs0AmPfHadNVfG03lRC2prx7qu0Ik1SXkpnPWpkabvXqpNNQ86fVIhVFue5cVeX/fL4mdc6cORODwVBxCwiQlSZF/TW+bMr45uMXSUzPNU2l5ftSnd4GpSWmqVMIa1CQBae2qMcmSnC+3XmGUqNC9xBPQv1lmIO1MmuC4+XlhU6nu65lJS0t7boWmHJ+fn43LG9nZ0fjxo1vWeZmdc6YMYPMzMyKW3Jy8u2+JCEsLtjLhf6tvVEUmB+daJpKm3QCRwMUZkJKrGnqFMIaJKwHY7G695R361pXp04NVz9DHpfWG6tm1gRHr9cTERHBhg0bKp3fsGEDvXr1uuFzIiMjryu/fv16unbtir29/S3L3KxOBwcH3N3dK92EqM8qpozHnCW30AQtLlodhPRVj2UcjrAlFd1Tw01S3aoD57mcW4S/wZHBYbJruDUzexfVtGnT+Prrr5k7dy5xcXE8//zzJCUlMXnyZEBtXRk3blxF+cmTJ3PmzBmmTZtGXFwcc+fOZc6cOUyfPr2izHPPPcf69et55513OHbsGO+88w4bN25k6tSp5n45QliFvq28ae7lQnZhCcv3mWjKeMV6OJtNU58QllZSCAllX4ZD7651dYqiVLSaytRw62f2f50xY8Ywa9Ys3nzzTTp16sTWrVtZvXo1QUFBAKSkpFRaEyckJITVq1ezefNmOnXqxL/+9S/+97//8cADD1SU6dWrF0uWLOGbb76hQ4cOzJs3j6VLl9KjRw9zvxwhrIJWq2FcpPp/aN6OREyy2kP5OJzkXVBkorE9QljS6W1QlA2uftCkS62r25d0hcPnstDbaXmoW6AJAhTmZPZ1cKyRrIMjbEF2QTGRM38np7CEhRO706eVd+0qVBSY1R4yk+HRZdBqkGkCFcJSVk2Fvd9A1ydgxIe1ru7Z7/bxy8EURndtxrsPdqx9fKLGrGYdHCGE+bg52vNgRPmU8cTaV6jRQPN+6rGMwxH1ndF4df0bE8yeSs0sYO1hmRpen0iCI0Q9Vt5N9Xt8GmcumaBbqbybqnxarRD11bkYyLkADu7q/lO1tGjXGUqMCt2DPWnXxGCCAIW5SYIjRD3W3NuVfmVTxheYYpfxkLIWnAuHICet9vUJYSnHflHvW0WBXe1WGi4sKeW7XbJreH0jCY4Q9dyE3sEAfB+TXPsp467e4NtePT69tXZ1CWFJJpwe/suBFC7lFuHn7khUO5kaXl9IgiNEPdevlTchXi5kF5SwfP+52ldYPg7ntHRTiXoqPQEunQCtPbSs3WD5P+8abi9Tw+sN+ZcSop67dsr4fFNMGQ/qrd4n7axlZEJYSPwa9T6kDzjWbqbsvqQMDp3LLJsaLtv81CeS4AhhAx6MaIaLXseJtBz+OHGpdpUF9lTv049Drol2LBeiLpUnOG3uqnVV5buG39OxCY1dHWpdn6g7kuAIYQMqTRkve0O+bc6e4N1WPZZWHFHf5F2G5LK/29ZDalXVhawCVh9KAWCCDC6udyTBEcJGjCt7A/7t2IXaTxkPjFTvk6JrV48QdS1hPShGdbC8R+1WG160U50a3i24EeFNZWp4fSMJjhA2ooUpp4xLgiPqq/LF/doMrVU1hSWlLCqbGj6hV0htoxIWIAmOEDakYsr4nlpOGS8fh5NyQPalEvVHSSGc+F09bjOsVlWVTw33N8jU8PpKEhwhbEjFlPHa7jLuEQjuTcFYAuf2mi5AIcwpcXvZ5pq+4N/5tqu5dmr4Yz1lanh9Jf9qQtgQrVbD+Gt2GTcab3PKuEZztRXnjHRTiXri+Fr1vvVQ0N7+x9u+pCsVU8Mf7i67htdXkuAIYWMeiGiGq4MdJy/msv1ELaZ5yzgcUZ8oyjXTw2vXPfVN2ea1Izs1wdOldts8CMuRBEcIG2OyKePlCc7ZPVBayy0ghDC3C4chMxnsnK7uqXYbUjLzWSO7htsESXCEsEHlb8y/H0vjdPptDhL2CQMHAxTlqJtvCmHN4su6p5r3B73zbVezaGcSpUaF7iGya3h9JwmOEDYoxMuFO9t4A7AgOvH2KtFqIbCHeiwL/glrVzE9/Pa7pwqKS/lutzo1/HFpvan3JMERwkZN6K2u3fFDzFmyC4pvr5KKgcY7TBSVEGaQlQLn96nHtVi9eOWB81zOLaKJwZHBYTI1vL6TBEcIG9WnpRfNvV3IKSzhx723OWU8sJd6n7RTHcQphDVKWKfeN40AN7/bqkJRlIrBxeN6BWMnU8PrPfkXFMJGabWaimb2+bc7ZbxJZ9DpITcNLp8ybYBCmEr57KnWt989tev0ZeJSsnC0l13DbYUkOELYsPu7NMPN0Y7ES3lsik+reQX2juq3YpDp4sI6FeXBqc3qcS3G33zzx2lA/T/j4SxTw22BJDhC2DAXB7uKb6Plze81Vj4ORxIcYY1ObYaSAjAEgm+726oi+XIeG45eAGRwsS2RBEcIGzcuMhitBrafSOf4heyaV1A+DkdWNBbW6NrNNTWa26piQXQiRgX6tPKila+bCYMTliQJjhA2LsDTmagwdeDlbbXiBHQHNHD5JOTcRjeXEOZiNMLxsgHGt9k9lVtYwpI9yQA8XrZZrbANkuAI0QCUv3Gv2H+WjLyimj3ZyQN8w9VjmS4urMn5feoAeL0bBN1xW1Us33eW7IISQrxc6N/ax8QBCkuSBEeIBqB7iCdh/u4UFBtZvDu55hUElXdT/WHawISojfLZUy0Hgl3NBwYbjQrflG1nMj4yCK329rq4hHWSBEeIBkCj0VS04iyMTqSk1FizCioSHGnBEVakYnPNu27r6VsTLnLqYi5uDnY82FWmhtsaSXCEaCDu7tiExi56zmcWsO7IhZo9Oai3en/hCORdNn1wQtTUlTOQdgQ0Omg1+LaqKB+TNqprAK4OdiYMTlgDSXCEaCAc7XU82jMIgLlla35Um6s3eLUGFNmXSliH42Wbawb2BGfPGj/9RFo2W45fRKOB8b2CTBycsAaS4AjRgDzWMxB7nYa9Z64Qm5xRsyfLOBxhTWq5uebcstabwaG+BDV2MVFQwppIgiNEA+Lj5sjdHZsAMHd7DVtxymepSIIjLK0gExLL/g5vY/zNldwilu9T92d74o4QU0YmrIhZE5wrV64wduxYDAYDBoOBsWPHkpGRcdPyxcXFvPjii7Rv3x4XFxeaNGnCuHHjOH/+fKVy/fv3R6PRVLo99NBD5nwpQtiMJ8p2GV99KIWUzPzqPzEoUr1POQCFt7FgoBCmcuI3MBZD41bQuEWNn/7d7iQKio20a+JOj5Cad2+J+sGsCc4jjzxCbGwsa9euZe3atcTGxjJ27Nibls/Ly2Pfvn289tpr7Nu3j+XLl3P8+HHuueee68pOmjSJlJSUituXX35pzpcihM0Ib2qgR4gnJUaFBdFnqv9EQzPwCALFCMm7zBegEFUpH39zG91TRSVGFkQnAjDxjhA0t7n6sbB+Zhs2HhcXx9q1a9m5cyc9evQA4KuvviIyMpL4+HjatGlz3XMMBgMbNmyodO7jjz+me/fuJCUlERgYWHHe2dkZPz8/c4UvhE2beEcIu05f5rtdSfxtQEuc9dV8Kwi+A2LPqN0DLQeZN0ghbqS05JrVi2vePbXmcAoXsgrxdnNgRIcmJg5OWBOzteBER0djMBgqkhuAnj17YjAY2LGj+mtpZGZmotFo8PDwqHR+0aJFeHl50a5dO6ZPn052tjSZC1FdA0N9CfR0JjO/mOX7zlX/ibIejrC0s3ugIAMcPaBZtxo9VVEU5pSNPRvXMwi9nQxDtWVm+9dNTU3Fx+f6Za99fHxITU2tVh0FBQW89NJLPPLII7i7u1ecf/TRR1m8eDGbN2/mtddeY9myZdx///03raewsJCsrKxKNyEaMp326sJ/c/84jdGoVO+J5QnOub1QXIPxO0KYSsJ69b7lINDVrBNi75krHDybid5OyyM9Aqt+gqjXapzgvP7669cN8P3zLSYmBuCGfZuKolSrz7O4uJiHHnoIo9HIZ599VumxSZMmMWjQIMLDw3nooYf48ccf2bhxI/v27bthXTNnzqwY6GwwGAgIkBUrhRjVNQA3BztOXcxly/GL1XtSoxBwa6IO8Dy7x7wBCnEjCWXDGFoPqfFTy1tv7u/clMauDqaMSlihGic4zz77LHFxcbe8hYeH4+fnx4UL16+WevHiRXx9fW95jeLiYkaPHs3p06fZsGFDpdabG+nSpQv29vYkJCTc8PEZM2aQmZlZcUtOvo29eISwMa4Odozppib7c6o7ZVyjkW4qYTmZ5+DCIUADLQbW6KnJl/NYd0TtPXi8t0wNbwhqPMjYy8sLLy+vKstFRkaSmZnJ7t276d69OwC7du0iMzOTXr163fR55clNQkICmzZtonHjxlVe68iRIxQXF+Pv73/Dxx0cHHBwkGxdiD8b3yuYuX+cZvuJdI6lZtHW79ZfJgA1wTn8o6yHI+reibLWm2bdwKXqz4ZrzduRiFGBO1p60cbPzQzBCWtjtjE4oaGhDB06lEmTJrFz50527tzJpEmTGDFiRKUZVG3btmXFihUAlJSU8OCDDxITE8OiRYsoLS0lNTWV1NRUioqKADh58iRvvvkmMTExJCYmsnr1akaNGkXnzp3p3bu3uV6OEDYpwNOZoeHqbMQ526rZihNctuBf8h4oKTJTZELcwPGy8Tetomr0tKyCYpbuUVvun+wjrTcNhVmHkC9atIj27dsTFRVFVFQUHTp0YOHChZXKxMfHk5mZCcDZs2dZuXIlZ8+epVOnTvj7+1fcymde6fV6fvvtN4YMGUKbNm2YMmUKUVFRbNy4EZ1OZ86XI4RNerJPcwB+jj1PWnZB1U/wag3OjaEkH87vN3N0QpQpKYRTm9XjGm6uuXR3MjmFJbTycaVfa2/Txyasklm3T/X09OTbb7+9ZRlFuTp7Izg4uNLPNxIQEMCWLVtMEp8QAroENqJLoAf7kjJYGH2Gv0ddv0ZVJeXjcOJWwZntENjj1uWFMIUzO6A4F1z9wL9jtZ9WUmrkm7LNZZ/sIwv7NSSyCIAQgkllrTjf7jxDflFp1U8o35fq9DYzRiXENcqnh7capCbZ1bT6cCrnMwvwctVzb6emZgpOWCNJcIQQRLXzI8DTiSt5xSwr24Twllrcqd6f2QFFeeYNTgi4JsGp/vgbRVH4etspAMb2DMbRXoYxNCSS4Agh0Gk1FZtwzt1ejYX/vFqDIQBKC2U2lTC/Syfh0gnQ2kHzO6v9tD2J6sJ+DnZaHuspC/s1NJLgCCGAsoX/HO04lZ7L78fSbl1Yo4EWA9TjExvNH5xo2MoX9wuMBMdqLGVQprz15v4uzWRhvwZIEhwhBKAu/Fe+fP1XZR8Mt1S+2eaJ38wYlRBc7Z6qwerFiem5bIhTF5udeEewGYIS1k4SHCFEhQm9grHTath1+jKHzmbeunDzfqDRwaUEuJJYJ/GJBqgoFxK3q8c1GH8z94/TKArc2cablj6ysF9DJAmOEKKCv8GJER3UFcGrbMVxNECAukq5tOIIszm9VR3r5RGojv2qhiu5RfwQow6WL1/nSTQ8kuAIISqZ1Ff9QPj1UApnr1QxQ6pl2X5AJ383c1Siwbp29lQ1p4cv3HmG/OJS2jVxp1eLmm3pIGyHJDhCiEraNTFwR0svSo1K1Ztwlo/DObVFtm0QpqcoV1sHy//WqlBQXMr8HYkAPNW3uSzs14BJgiOEuM5TZa04S/ckk5lXfPOCfh3B2QuKsuHs7jqKTjQYl09BxhnQ2kNwn2o9Zdm+s1zKLaKphxPD2994A2bRMEiCI4S4Tp9WXoT6u5NXVMq3u87cvKBWe810cRmHI0ys/G8qsCc4uFZZvNSo8HXZprET7wjBTicfcQ2Z/OsLIa6j0Wh4qq+68N83fyRSUHyL7RvKx+HIejjC1E6WJTjlSXQVNhy9wOn0XAxO9ozpFmDGwER9IAmOEOKGRnRoQhODI+k5hazYf+7mBcs/fFIPQk4VCwQKUV0lRVf3OitPoqswe+tJAMb2DMLFwax7SYt6QBIcIcQN2eu0PHGH2orz1bZTN9++wdXn6u7OMptKmEryTnX3cBdv8G1fZfGYxMvsS8pAb6dlfK9g88cnrJ4kOEKIm3qoe6C6fcPFXDaWrQp7Qy2km0qYWPn4mxYD1bFeVfhyq7pu0wNdmuLtJtsyCElwhBC34Opgx6M9ggCYvfUWC/+VT+E9+TsYjXUQmbB55eNvqtE9dfJiDhvjLqDRyMJ+4ipJcIQQt/R472D0Oi0xZ64Qk3j5xoUCuoPeDfIuwfn9dRugsD05aZB6SD2uxu7hs7ecQlFgUKgvLbyrnm0lGgZJcIQQt+Tr7sj9XZoC8MWWkzcupLOHlmWDjY/9UkeRCZtVPpbLvyO4et+yaGpmAcv3q9sy/KV/C3NHJuoRSXCEEFVSV4SFjXFpxKdm37hQ27vV+2O/1l1gwjZdO/6mCnO2n6K4VKFHiCddAhuZOTBRn0iCI4SoUnNvV4aF+wG3aMVpHaWuOJseD+kJdRidsClG49UWnCrG32TkFfHdriRAWm/E9STBEUJUy+R+6gfIygPnSb58g004HQ0QUracftyqOoxM2JTUg5CXDnpXaNb9lkUXRJ8ht6iUUH93+rW+dVeWaHgkwRFCVEuHZh4Vm3B+ve0mM6rajlDvpZtK3K7y2VMhfcFOf9NieUUlfPOHui3DX/q3kE01xXUkwRFCVNtfy7oBluxJJj2n8PoCbYer9+diIOt8HUYmbMaJsu6pKrZn+H5PMlfyign0dOausu5TIa4lCY4QotoiWzSmYzMDhSVG5v2ReH0BNz9o1k09llYcUVOF2eoKxnDL8TfFpUa+KttU8+l+zWVTTXFD8lchhKg2jUZTMZhzQXQi2QXF1xeSbipxu05uAmMJeDZXbzex6sB5zmXk4+XqwANdmtVhgKI+kQRHCFEjUWF+NPd2IaugpGIGSyWhZdPFE7dB/pW6DU7Ub8fXqfeth920iNGoVMzkm3hHCI72urqITNRDkuAIIWpEq9VUzKj6evtpCopLKxdo3AK826rfxBM2WCBCUS8ZjZBQnuBE3bTY+qMXOH4hBzcHOx7tGVhHwYn6SBIcIUSNjezUlKYeTlzMLuT7mOTrC5R3U8l0cVFd5/dD7kV1y4/AXjcsoigKn2xS11ia0DsYd0f7uoxQ1DOS4Aghakxvp2VyP3WMxBebT1JU8qcNNkPLEpwTG6E4v46jE/XS8bXqfcsBN50evvn4RQ6fy8JZr+Px3iF1GJyojyTBEULcllFdA/Bxc+B8ZgEryvYCquDfCdybQXEenNpsifBEfVOe4LQeesOHFUXh49/U1pvHegbh6XLzNXKEAElwhBC3ydFex1N91VaczzafpKT0mlYcjebqmjhxsvmmqELWeXUFYzTQcvANi0SfvMS+pAz0dlqe7COtN6JqZk1wrly5wtixYzEYDBgMBsaOHUtGRsYtnzNhwgQ0Gk2lW8+ePSuVKSws5G9/+xteXl64uLhwzz33cPbs2ZvUKIQwl0d6BOLpoufMpTx+OZhS+cHybqr4X6H0BtPJhSiXsF69b9b1pruHf/z7CQAe7haAj5tjXUUm6jGzJjiPPPIIsbGxrF27lrVr1xIbG8vYsWOrfN7QoUNJSUmpuK1evbrS41OnTmXFihUsWbKE7du3k5OTw4gRIygtLb1JjUIIc3DW2zHxDvXb9CebTmA0KlcfDOwFzl7qVPHEbRaKUNQLFdPDh9zw4ZjEy0SfuoS9TsNT/WRTTVE9Zktw4uLiWLt2LV9//TWRkZFERkby1Vdf8csvvxAfH3/L5zo4OODn51dx8/T0rHgsMzOTOXPm8P777zNo0CA6d+7Mt99+y6FDh9i4caO5Xo4Q4ibGRQbh7mjHibQc1h1JvfqAzu7qmjhHfrJIbKIeKM6/Ok7rJuNvPtmktt480KUZTT2c6igwUd+ZLcGJjo7GYDDQo0ePinM9e/bEYDCwY8eOWz538+bN+Pj40Lp1ayZNmkRaWlrFY3v37qW4uJioqKvrJDRp0oTw8PAq6xVCmJ6boz0Tyma0fPz7CRTlmlacsHvV+2O/QGmJBaITVi9xuzoY3b0p+IZf9/Chs5lsjr+IVkPFKtpCVIfZEpzU1FR8fHyuO+/j40NqauoNnqEaNmwYixYt4vfff+f9999nz549DBgwgMLCwop69Xo9jRo1qvQ8X1/fm9ZbWFhIVlZWpZsQwnQe7xWMi17H0ZQsNsZd/UJCcB9w8oS8S3Bmu+UCFNarYvbUEHVw+p98VDZz6p6OTQhq7FKXkYl6rsYJzuuvv37dIOA/32JiYgBuuH29oii33NZ+zJgxDB8+nPDwcO6++27WrFnD8ePH+fXXW+9rc6t6Z86cWTHQ2WAwEBAQUINXLISoSiMXPeN6BQMwa+Pxq604Orurg42lm0r8maJcM/7m+u6pw+cy2Rh3Aa0Gnh3Qqo6DE/VdjROcZ599lri4uFvewsPD8fPz48KFC9c9/+LFi/j6+lb7ev7+/gQFBZGQoGbxfn5+FBUVceVK5T1u0tLSblrvjBkzyMzMrLglJ99g5VUhRK1M6tMcF72OI+ez2HD0mv/7YSPV+7hVYJSJAOIaaUchMxnsnCCk73UPz9p4HFBbb1r6uNZ1dKKes6vpE7y8vPDy8qqyXGRkJJmZmezevZvu3bsDsGvXLjIzM+nV68bLcN/IpUuXSE5Oxt/fH4CIiAjs7e3ZsGEDo0ePBiAlJYXDhw/z7rvv3rAOBwcHHBwcqn1NIUTNebroGd8rmM82n2TWxgQGh/mqraohfcGpEeSlw5k/bvhBJhqo8u6p5v3AvvLg4YNnM9gYl4ZWA38bKK03oubMNgYnNDSUoUOHMmnSJHbu3MnOnTuZNGkSI0aMoE2bNhXl2rZty4oVKwDIyclh+vTpREdHk5iYyObNm7n77rvx8vLivvvuA8BgMDBx4kT+/ve/89tvv7F//34ee+wx2rdvz6BBg8z1coQQ1TCpT3NcHew4mpLF+vJWHJ391UX/jv5sueCE9TmsvvffqHvqo41qq/29nZrSwltab0TNmXUdnEWLFtG+fXuioqKIioqiQ4cOLFy4sFKZ+Ph4MjMzAdDpdBw6dIh7772X1q1bM378eFq3bk10dDRubm4Vz/nwww8ZOXIko0ePpnfv3jg7O7Nq1Sp0Op05X44QogqNXPRMqBiLk3B1XZww9QsKR1dKN5VQpRyAC4dAp4d2Iys9dCA5g9+OlbXeDGhpmfhEvadRKs3pbBiysrIwGAxkZmbi7u5u6XCEsCkZeUXc8c4mcgpL+OKxLgwN94eSIvhvSyjIhAmrIbi3pcMUlrbmRdj1BbS7D0bNq/TQ49/sZlP8Re7v3JQPxnSySHjCOtXk81v2ohJCmJSHs57HewcD17Ti2OmhbdlsqqM/WSw2YSVKiuDg9+pxp8cqPRSbnMGm+IvotBoZeyNqRRIcIYTJPXlHc9wc7DiWmn11dePyRf+OrgSj8eZPFrbv+BrIvwxu/tDizkoPlc+cGtmpKSFesu6NuH2S4AghTM7gbM/jZXtUfbjxOKVGBZr3BwcD5KRC8k7LBigsa/8i9b7jQ6C9OnZy75nLbC5vvZGxN6KWJMERQpjFxDtCcHe04/iFHFYeOAd2DldnUx36wbLBCcvJToUTZfsGXtM9pSgK765V9yl8oEtTgqX1RtSSJDhCCLMwONkzuWzvoA83JFBUYoQO6tpVHF6ujsMQDc/BpaCUQkAP8LraSrP9RDq7Tl9Gr9Py3KDWFgxQ2ApJcIQQZjOhVzBerg4kXc5jaUyyusifmz8UZMCJDZYOT9Q1RbnaPdXpkWtOK7y3Tm29ebRnoOwYLkxCEhwhhNk46+0qxlJ8/FsC+SVA+wfVBw8utVxgwjLO7YX0eHVrhnb3V5xed+QCB89m4qzX8df+MvZGmIYkOEIIs3q4eyDNGjmRll3IguhE6DBGfSB+LeRnWDI0Uddiy1pvwu4BR3UNk1Kjwvvr1dabJ3qH4O0m2+oI05AERwhhVno7LVPLxlR8vuUkWYY24BMGpYWydUNDUpwPh5apx9d0T/0ce46EtBwMTvZM6tvcQsEJWyQJjhDC7O7r3JSWPq5k5BXz9bbTVwcbly/2JmxfzDdQmAmGQAhWN1wtKjHyYdm6N5P7tcDgZG/JCIWNkQRHCGF2Oq2G6VFqK87X209zuXnZon9ntkNGsgUjE3WiIAu2/Vc97vt30KofPUv3JJF8OR9vNwfG9wqyYIDCFkmCI4SoE0Pa+dGhmYG8olI+2pMHwX3UB2RNHNsX/QnkXYLGLSvWvskpLOGj39Qdw/82oCXOejtLRihskCQ4Qog6odFoeGloWwAW7UriYshI9YGDS9Xpw8I25VyEHZ+oxwNeA52ayMzecpL0nCKCGzvzULdACwYobJUkOEKIOtOrpRd3tvGmxKjw79OtQOcAF49B6iFLhybMZet7UJwLTTpX7Ed2IauAr7adBuDFoW3R28lHkTA9+asSQtSpGXeFotXAT8dyuBwwUD0pa+LYpiuJEDNXPR70Omg0AHyw/jj5xaVEBDViaLifxcITtk0SHCFEnWrt68borgEAfHmlq3ry4FIoLrBgVMIsNr0NxmJ1o9Xm/QGIT83mh73qwPKX72qLpizpEcLUJMERQtS5aYNb42SvY86FluQ7+UPuRTgkU8ZtSurhq8sADPxnxemZa+IwKjAs3I+IIE8LBScaAklwhBB1zsfdkUl9m1OCHXNLh6ond3wCRqNlAxOmoSiw4TVAgbCR0LQLAH+cSGdz/EXstBpeKBtwLoS5SIIjhLCIp/s2x8vVgc+zelNk56ruUSQbcNqGQz/Ayd9Bp4eB/wDAaFR4e3UcAI/2CCTEy8WSEYoGQBIcIYRFuDjY8fzgVuTgzHclA9STOz62bFCi9nLTYc2L6nG/F6BxCwCW7TvLkfNZuDnYMWVgKwsGKBoKSXCEEBYzpmsAbXzd+KJgMKXoIHEbnNtn6bBEbax7GfIvg0876D0VgOyCYt5Zq26o+eyAljR2lQ01hflJgiOEsBg7nZZ/3h1GKo1ZWdpLPRn9iWWDErcvYaM6I06jhXs+Bp26t9Qnv58gPaeQEC8XHu8dYuEgRUMhCY4QwqJ6tfRiaDs/ZpfcBYBy5Ce4csayQYmaK8yBX55Xj3v8BZpFAHA6PZe5f6iL+r02IlQW9RN1Rv7ShBAW98rwUE7qQthWGo5GKYVdX1g6JFFTv78FmUngEQgDXqk4/dYvRykuVejfxpsBbX0tGKBoaCTBEUJYXICnM0/1ac5XpcMBUPbOh/wrFo5KVNu5vVeT0hEfgl6dIbU5Po3fjqVhp9Xw6vAwCwYoGiJJcIQQVuGvd7bguEt3jhkD0BTnwh//s3RIojqMRlj9AqBA+9HQchAARSVG3vzlKAATegXT0sfVgkGKhkgSHCGEVXDW2zFjeCgflDwIgLLjY0iLs3BUokoHl8K5GNC7QtS/Kk4viE7k1MVcvFz1TBkk08JF3ZMERwhhNe7p2ITLAVFsKI1AYyxWB63K6sbWqzAbNpZtw9D3/8BN3TgzNbOAWRsTAPi/IW1wd7S3VISiAZMERwhhNTQaDW/eG84bpRPIVRwgKRpiv7V0WOJmtr4HORfAszn0/EvF6Td/OUJOYQmdAz0YFRFgwQBFQyYJjhDCqoQ1cWdor65Xu6rWvwY5Fy0clbjOpZMQ/Zl6PPQ/YKcu3rfpWBqrD6Wi02p4+772aLWyW7iwDElwhBBW5/nBrVnnMpIjxiA0BRmw/lVLhyT+bO0MMBZDy8HQeggA+UWlvPbzYQAm3hFCqL+7JSMUDZxZE5wrV64wduxYDAYDBoOBsWPHkpGRccvnaDSaG97ee++9ijL9+/e/7vGHHnrInC9FCFGHXBzseO3eDrxcPBGjooGDS+DUFkuHJcodXw8J60BrD0NnVpz++PcEzl7Jp4nBkedkvylhYWZNcB555BFiY2NZu3Yta9euJTY2lrFjx97yOSkpKZVuc+fORaPR8MADD1QqN2nSpErlvvzyS3O+FCFEHRvSzg/vtr1ZUDoYAOWXqVCUa9mgBJSWqPtNAfScDF5qInP8Qjazt54C4I17w3FxsLNUhEIAYLa/wLi4ONauXcvOnTvp0aMHAF999RWRkZHEx8fTpk2bGz7Pz8+v0s8///wzd955J82bN6903tnZ+bqyQgjb8vo9Ydz3wcMMUWLwv3xK7aoa8aGlw2rY9i+ESwng3Bj6vgCA0ajwyopDlBgVBof5MjhMViwWlme2Fpzo6GgMBkNFcgPQs2dPDAYDO3bsqFYdFy5c4Ndff2XixInXPbZo0SK8vLxo164d06dPJzs722SxCyGsQ7NGzjw5qCN/L56snoiZC/FrLRtUQ1aUB5v/ox73fQEc1TE2S/YksyfxCs56Ha/f086CAQpxldkSnNTUVHx8fK477+PjQ2pqarXqmD9/Pm5ubtx///2Vzj/66KMsXryYzZs389prr7Fs2bLrylyrsLCQrKysSjchRP3wxB0hZPj24qvyzTh/fgZy0iwcVQO16wvISVX3m+r6OADnMvJ5e7W6IOO0wa1p6uFkyQiFqFDjBOf111+/6UDg8ltMTAygDhj+M0VRbnj+RubOncujjz6Ko6NjpfOTJk1i0KBBhIeH89BDD/Hjjz+yceNG9u3bd8N6Zs6cWTHQ2WAwEBAg6zIIUV/Y67S8N6oDHxrHEGcMQJOXDj8/C4pi6dAalrzLsH2Wenznq2DngKIovLTsIDmFJXQNasTjvUMsGqIQ16pxgvPss88SFxd3y1t4eDh+fn5cuHDhuudfvHgRX9+q+2e3bdtGfHw8Tz75ZJVlu3Tpgr29PQkJCTd8fMaMGWRmZlbckpOTq36hQgir0a6JgUl3hjG1+BkKsVdn8MTMsXRYDcv2D6AwE3zDof0oAJbuSWZbQjoOdlrefbADOlnzRliRGg8y9vLywsvLq8pykZGRZGZmsnv3brp37w7Arl27yMzMpFevXlU+f86cOURERNCxY8cqyx45coTi4mL8/f1v+LiDgwMODg5V1iOEsF7P3NmS9Ucv8E7aQ/zDfiGsexWC+4J3a0uHZvsyz8Ku2erxoNdBq+VcRj5v/ap2TU2PakNzb9lMU1gXs43BCQ0NZejQoUyaNImdO3eyc+dOJk2axIgRIyrNoGrbti0rVqyo9NysrCx++OGHG7benDx5kjfffJOYmBgSExNZvXo1o0aNonPnzvTu3dtcL0cIYWF6Oy3/HdWBhcpQtpWGQ0k+/DJVuqrqwuaZUFoIQXdAy0EoisKM5YcqtmN44g7pmhLWx6wLFSxatIgpU6YQFRUFwD333MMnn3xSqUx8fDyZmZmVzi1ZsgRFUXj44Yevq1Ov1/Pbb7/x0UcfkZOTQ0BAAMOHD+ef//wnOp3OpPGXlpZSXFxs0jobAnt7e5P/WwgBalfVX+5szUu/TWKj7v9wOvMHHPoBOoy2dGi2K+0YxH6nHg96HTQaftiTzNbjF9HbaXnvwY7SNSWskkZRGt7Xn6ysLAwGA5mZmbi7X7+UuKIopKamVrnqsrg5Dw8P/Pz8qj2gXIjqKioxcs8n2xlwcSEv2H+P4uqL5tmYiinLwoQUBRbcC6e3QNsR8NAizlzKZfj/tpNTWMKMYW15ul8LS0cpGpCqPr+vJUtN3kB5cuPj44Ozs7N8SNeAoijk5eWRlqZO473ZuCghbpfeTssHozsx6tMMHjRupXlOqro2y9C3LR2a7TmyXE1u7Bwh6i2KS408tySWnMISugU3YqJ0TQkrJgnOn5SWllYkN40bN7Z0OPWSk5O6DkZaWho+Pj7SXSVMLqyJO9OGteeN1eOZr38HZdcXaDo/Br5hlg7NdhRmw7pX1OM7poFnCB+tiyc2OQM3Rzs+HNMJO53s1yysl/x1/kn5mBtnZ2cLR1K/lf/+ZAyTMJcnegejaTWItaXd0CilGH/9uww4NqXN/4HsFGgUAr2fY+epS3y6+QQAM+9vT7NG8h4prJskODch3VK1I78/YW4ajYb3HuzIJ/onyFf0aJN2wKEfLR2WbbhwFHZ+rh7f9R4ZxVqeXxqLosDors0Y0aGJZeMTohokwRFC1Fvebg5MHz2Ij0tGAlC4egYU5lg2qPpOUWD1dFBKoe0IlJaDeGnZIVIyCwjxcuGfd8teU6J+kARHCFGv9W/jQ0mPZ0g0+uJQcJGszR9ZOqT67eBSOPMH2DnB0P/w7c4zrD2Sir1Ow/8e6oyLgwzdFPWDJDg2pH///kydOtXSYQhR5/5+V3uWuI0HwG7nxxRkymactyUjGda9rB73+z/2Zrrw5i9HAXhhSFvaNzNYMDghakYSnAZEURRKSkosHYYQJudgp+PRJ54jjhCclXz2LHiFBrjEV+0U5cGSRyDvEvi1J639JP7y7T6KSxWGt/fnyT4yJVzUL5Lg2IgJEyawZcsWPvroo4pd3efNm4dGo2HdunV07doVBwcHtm3bxoQJExg5cmSl50+dOpX+/ftX/KwoCu+++y7NmzfHycmJjh078uOPMoBTWK+Axq6U3PkPALqnL+fnLbstHFE9oijw8zOQehCcvSh68FueWXKYtOxCWvm48u6DHWTigKh3pDO1GhRFIb+41CLXdrLXVeuN5aOPPuL48eOEh4fz5ptvAuompAAvvPAC//3vf2nevDkeHh7Vuu6rr77K8uXL+fzzz2nVqhVbt27lsccew9vbm379+t326xHCnNr3vY+z+z+lWUYMpb//m33N59MlsJGlw7J+295XF/XT2sGYhby9I5c9iVdwc7Djy7ERMu5G1EvyV1sN+cWlhP1jnUWuffTNITjrq/5nMhgM6PV6nJ2d8fPzA+DYsWMAvPnmmwwePLja18zNzeWDDz7g999/JzIyEoDmzZuzfft2vvzyS0lwhPXSaGj6wH9gziBGarby2IKf+N9zj+Dt5mDpyKxX/Br4/S31+K73WH4pkHk7DgDwwZhOsku4qLeki6oB6Nq1a43KHz16lIKCAgYPHoyrq2vFbcGCBZw8edJMUQphGpqAbhS3HoFOo/B44UKeWhhDgYVaYK3ehSOwbBKgQNeJxHiN5KXlhwCYMqAlg8N8LRufELUgLTjV4GSv4+ibQyx27dpycXGp9LNWq71uAOa1Kw4bjUYAfv31V5o2bVqpnIODfBMW1s9+8D9QElYTpdvLF8m7eW6JA589GiG7Xl8r5SAsHAlF2RDch5NdX+PJ2XsoKjEyKNSX5wa1tnSEQtSKJDjVoNFoqtVNZGl6vZ7S0qq/qXp7e3P48OFK52JjY7G3twcgLCwMBwcHkpKSpDtK1E/ebdB0ehT2L+Qf9t9y35GW/OuXo/zz7jAZLAtwdi98ex8UZEKTzqTf9TUT5u0nI6+YjgEefPxwZ0kGRb0nXVQ2JDg4mF27dpGYmEh6enpFS8yfDRgwgJiYGBYsWEBCQgL//Oc/KyU8bm5uTJ8+neeff5758+dz8uRJ9u/fz6effsr8+fPr6uUIUTt3vgJ6NzppTzBGt5l5OxL5ettpS0dleWeiYcG9anIT0IO8h5Yz8fsTJF/OJ9DTmTnju+Kklw1yRf0nCY4NmT59OjqdjrCwMLy9vUlKSrphuSFDhvDaa6/xwgsv0K1bN7Kzsxk3blylMv/617/4xz/+wcyZMwkNDWXIkCGsWrWKkBBZC0PUE+7+cKe6aN3rTt/jSRb/Xh3HLwfPWzgwCzq1Gb69v6JbquSRH5my4iQHzmbSyNmeeY93w8tVuqGFbdAoDXA1rKysLAwGA5mZmbi7u1d6rKCggNOnTxMSEoKjo6OFIqz/5PcorEJpCczuDxcOsb/xcO479yh6nZavx3elb2tvS0dXt078BosfhtJCaDkI46iFvLAygR/3nsXBTst3k3oQEeRp6SiFuKVbfX7/mbTgCCFsl84ORnwAQOdLv/Jsi4sUlRqZtCCGbQkXLRxcHbo2uWlzF8bRi3hplZrc6LQaPnqokyQ3wuZIgiOEsG0B3aGL2gU7regLhoY2prDEyJPzY9iekG7h4OrAiY3XJDfDMT44j5dXHef7mLNoNTBrTCeGhvtbOkohTE4SHCGE7Rv0Bjh5or0YxyctdjMo1IfCEiMT5+/hjxM2nOSc2AiLH1GTm7YjMD74Da+simfJnmS0GvhwTCfu7tjE0lEKYRaS4AghbJ+zJwxWtzCx2/IfPotyZmBbG09yTvxWKbkpfWAur6w6zuLdV5Obezs1rboeIeopSXCEEA1Dp0ch6A4ozkX/3YN8NsKLAW19KCg2MuGb3azYf9bSEZpO4h/qzuBlyU3+vV/zl8WHWLw7Ca0GPhgtyY2wfZLgCCEaBq0WxiwE71DIPo/Dovv4fGRThnfwp7hU4fmlB/jfbwnXrfJd75zbC9+NgZICaDWE9GFf8NDcfaw/egG9nZb/PdyZkZ0luRG2TxIcIUTD4ewJY1eARxBcOY3D4lF8fG8IT/dtDsAHG47z4rKDFJfeeJFMq5d6GBZeXefm1IBPuf/LGA4kZ+DhbM+iJ3swooOMuRENgyQ4QoiGxd0fxv0Err5w4TDaxaOZMSiQf40MR6uB72PO8vg3e7iSW2TpSGsm/YS6t1RBBjTrxs6en3L/V/tJupxHoKczy//Si27BMhVcNByS4AghGh7P5jD2J3D0gLO7Ye4QxjZL5+vxXXHW69h+Ip27/reNXacuWTrS6rlwFBbcA7kXUXzb82mTmTw8/zAZecV0CvBg+V970dzb1dJRClGnJMERtyU4OJhZs2ZZOgwhbp9vGDz6o5rkpB6CrwcyIOFtlk8IJcTLhZTMAh7+aiezNh6n1GjF43Li18CcwZB1juJGrXjS+DLvbU1DUWB012YsntRTtl8QDZIkOEKIhiugGzy7Bzo+DCiwdx5tf7yTtX3PMKqzH0YFZm1M4OGvdnI+I9/S0VamKLD9Q3URv6IcLnl3Z+CVGfyWrODqYMf/Hu7Muw92lI0zRYMlCU4DVlRUz8YYCGEOrj5w3xcwYbU6wyrvEg6rp/Be6hP81P0YjR2M7D59mcEfbOGLLScpLCm1dMRQXAArJsPG1wGFzW530yP5WZIKHOkY4MHqKX24RxbwEw2cJDg2pH///jz77LM8++yzeHh40LhxY1599dWKaa/BwcG89dZbTJgwAYPBwKRJkwDYsWMHffv2xcnJiYCAAKZMmUJubm5FvWlpadx99904OTkREhLCokWLLPL6hDCr4N4weRtEvQVOnnAlkU4H32S381T+7bUOXVEW/1lzjKGztrHpWJplYkw/ARv+CR91gINLMKLj9dInmHDxYdDa85f+Lfjh6UgCGztbJj4hrIidpQOoFxQFivMsc217Z9Boql18/vz5TJw4kV27dhETE8NTTz1FUFBQRTLz3nvv8dprr/Hqq68CcOjQIYYMGcK//vUv5syZw8WLFyuSpG+++QaACRMmkJyczO+//45er2fKlCmkpVnoDV4Ic9LZQ6+/QdcnYP+3sOMTdJlJPMp8Rrv+yCJjFB+nR/H4vFwGtPXhuYGt6BjgYbrrKwoU5aozofIzoCBTvWWfh0PLIGlHRdF0PJhS9Fd2GMO5o6UXr98TRksfN9PFIkQ9p1HMuKrVv//9b3799VdiY2PR6/VkZGRU+RxFUXjjjTeYPXs2V65coUePHnz66ae0a9euokxhYSHTp09n8eLF5OfnM3DgQD777DOaNWtWrbhutd16QUEBp0+fJiQkBEdHR/VkUS68baHm3pfPg96lWkX79+9PWloaR44cQVOWFL300kusXLmSo0ePEhwcTOfOnVmxYkXFc8aNG4eTkxNffvllxbnt27fTr18/cnNzSUpKok2bNuzcuZMePXoAcOzYMUJDQ/nwww+ZOnXqDWO54e9RiPqmtBgOL4c/ZkHaUQCKtQ58V3wnnxcPJ5XGdA1qxMQ7Qohq54dOW80vI8X5kLwLTm9TZ3HlXIT8y5B/BUpv3nVsRMsfms58W9iX34yd8WvkxqvDwxjSzrfi/7wQtuxWn99/ZtYWnKKiIkaNGkVkZCRz5syp1nPeffddPvjgA+bNm0fr1q156623GDx4MPHx8bi5qd9Opk6dyqpVq1iyZAmNGzfm73//OyNGjGDv3r3odA17QF3Pnj0rvdFFRkby/vvvU1qqjhvo2rVrpfJ79+7lxIkTlbqdFEXBaDRy+vRpjh8/jp2dXaXntW3bFg8PD/O+ECGsgc4eOo6B9qPg+BrY+l/sz+9jvG4tj+k2sMXYkZXJPZm+KIJGjTx5uHsgg0J9ae3rWjnhKC1WVxg+tflqUnOLRAatPTh5YHQwkKNxJqXQkV8zQ1ha3IcLeOJvcOSF3sGMiwzG0b5hv+cJcTNmTXDeeOMNAObNm1et8oqiMGvWLF555RXuv/9+QO1y8fX15bvvvuPpp58mMzOTOXPmsHDhQgYNGgTAt99+S0BAABs3bmTIkCGmfyH2zmpLiiXYm7Yv3cWlcmuQ0Wjk6aefZsqUKdeVDQwMJD4+HkC+HYqGTauFtsOhzV1wahNsfR/dme0M0O5jgH4fBdjze05n9m9syfcbtbg6OdDS10Col46ArP3oz+1EU5RTuU63JhDSB4J6Q6MgcPKkUG8gOd+RPecK+O3YRf44kU5+8dVBzR2aGXilT3OGhfthr5MhlELcilWNwTl9+jSpqalERUVVnHNwcKBfv37s2LGDp59+mr1791JcXFypTJMmTQgPD2fHjh3mSXA0mmp3E1nazp07r/u5VatWN23Z6tKlC0eOHKFly5Y3fDw0NJSSkhJiYmLo3r07APHx8dXqbhTC5mg00GKAeks7BkeWw6Efcbx8krt0u7lLt1stVwKcK7uVydS4c8KlC+cbdee0WwSXHZqBRkNJspHkA/mcTr/E2SvJ/HnJHX+DIwPa+jCyc1O6BjWSLxtCVJNVJTipqakA+Pr6Vjrv6+vLmTNnKsro9XoaNWp0XZny5/9ZYWEhhYWFFT9nZWWZMmyrkpyczLRp03j66afZt28fH3/8Me+///5Ny7/44ov07NmTZ555hkmTJuHi4kJcXBwbNmzg448/pk2bNgwdOpRJkyYxe/Zs7OzsmDp1Kk5OTnX4qoSwQj5twedl6D8DUg/CkZ8g6xwlpaWkZ+VxMSuP9NxidhYEs620HXFKIEq+FtIBSoEzN6zW1cGO1r6u3NnGhwGhPoT5u0tSI8RtqHGC8/rrr1d0Pd3Mnj17rhvrURN//s+sKEqV/8FvVWbmzJlVxmwrxo0bR35+Pt27d0en0/G3v/2Np5566qblO3TowJYtW3jllVfo06cPiqLQokULxowZU1Hmm2++4cknn6Rfv374+vry1ltv8dprr9XFyxHC+mk04N9RvaG+qfqV3QB6lxgZfSWPxPRcTl3MJSO/CA2aSk9v4uFEcy8XQrxd8HZ1kIRGCBOocYLz7LPP8tBDD92yTHBw8G0F4+enviWkpqbi7+9fcT4tLa2iVcfPz4+ioiKuXLlSqRUnLS2NXr163bDeGTNmMG3atIqfs7KyCAgIuK0YrZ29vT2zZs3i888/v+6xxMTEGz6nW7durF+//qZ1+vn58csvv1Q6N3bs2FrFKURDobfT0sLblRbergwMtXQ0QjQcNU5wvLy88PLyMkcshISE4Ofnx4YNG+jcuTOgzsTasmUL77zzDgARERHY29uzYcMGRo8eDUBKSgqHDx/m3XffvWG9Dg4OODjIXixCCCFEQ2HWMThJSUlcvnyZpKQkSktLiY2NBaBly5a4uqo727Zt25aZM2dy3333odFomDp1Km+//TatWrWiVatWvP322zg7O/PII48AYDAYmDhxIn//+99p3Lgxnp6eTJ8+nfbt21fMqhJCCCFEw2bWBOcf//gH8+fPr/i5vFVm06ZN9O/fH1Bn5GRmZlaUeeGFF8jPz+evf/1rxUJ/69evr1gDB+DDDz/Ezs6O0aNHVyz0N2/evAa/Bs7mzZstHYIQQghhFcy6krG1qvFKxqLG5PcohBDC1GqykrGsFCWEEEIImyMJzk0YjUZLh1Cvye9PCCGEJVnVQn/WQK/Xo9VqOX/+PN7e3uj1elmTogYURaGoqIiLFy+i1WrR6/WWDkkIIUQDJAnOn2i1WkJCQkhJSeH8eQvtP2UDnJ2dCQwMRKuVRkIhhBB1TxKcG9Dr9QQGBlJSUlKxC7eoPp1Oh52dnbR8CSGEsBhJcG5Co9Fgb2+Pvb29pUMRQgghRA1J/4EQQgghbI4kOEIIIYSwOZLgCCGEEMLmNMgxOOWLN2dlZVk4EiGEEEJUV/nndnU2YWiQCU52djYAAQEBFo5ECCGEEDWVnZ2NwWC4ZZkGuReV0Wjk/PnzuLm5mXwqc1ZWFgEBASQnJ1e5T0ZDJL+fm5Pfza3J7+fW5Pdza/L7ubn69LtRFIXs7GyaNGlS5TprDbIFR6vV0qxZM7New93d3er/UCxJfj83J7+bW5Pfz63J7+fW5Pdzc/Xld1NVy005GWQshBBCCJsjCY4QQgghbI4kOCbm4ODAP//5TxwcHCwdilWS38/Nye/m1uT3c2vy+7k1+f3cnK3+bhrkIGMhhBBC2DZpwRFCCCGEzZEERwghhBA2RxIcIYQQQtgcSXCEEEIIYXMkwTGhzz77jJCQEBwdHYmIiGDbtm2WDslqbN26lbvvvpsmTZqg0Wj46aefLB2S1Zg5cybdunXDzc0NHx8fRo4cSXx8vKXDshqff/45HTp0qFiELDIykjVr1lg6LKs0c+ZMNBoNU6dOtXQoVuH1119Ho9FUuvn5+Vk6LKty7tw5HnvsMRo3boyzszOdOnVi7969lg7LJCTBMZGlS5cydepUXnnlFfbv30+fPn0YNmwYSUlJlg7NKuTm5tKxY0c++eQTS4didbZs2cIzzzzDzp072bBhAyUlJURFRZGbm2vp0KxCs2bN+M9//kNMTAwxMTEMGDCAe++9lyNHjlg6NKuyZ88eZs+eTYcOHSwdilVp164dKSkpFbdDhw5ZOiSrceXKFXr37o29vT1r1qzh6NGjvP/++3h4eFg6NJOQaeIm0qNHD7p06cLnn39ecS40NJSRI0cyc+ZMC0ZmfTQaDStWrGDkyJGWDsUqXbx4ER8fH7Zs2ULfvn0tHY5V8vT05L333mPixImWDsUq5OTk0KVLFz777DPeeustOnXqxKxZsywdlsW9/vrr/PTTT8TGxlo6FKv00ksv8ccff9hsb4O04JhAUVERe/fuJSoqqtL5qKgoduzYYaGoRH2VmZkJqB/iorLS0lKWLFlCbm4ukZGRlg7HajzzzDMMHz6cQYMGWToUq5OQkECTJk0ICQnhoYce4tSpU5YOyWqsXLmSrl27MmrUKHx8fOjcuTNfffWVpcMyGUlwTCA9PZ3S0lJ8fX0rnff19SU1NdVCUYn6SFEUpk2bxh133EF4eLilw7Eahw4dwtXVFQcHByZPnsyKFSsICwuzdFhWYcmSJezbt09aim+gR48eLFiwgHXr1vHVV1+RmppKr169uHTpkqVDswqnTp3i888/p1WrVqxbt47JkyczZcoUFixYYOnQTKJB7iZuLhqNptLPiqJcd06IW3n22Wc5ePAg27dvt3QoVqVNmzbExsaSkZHBsmXLGD9+PFu2bGnwSU5ycjLPPfcc69evx9HR0dLhWJ1hw4ZVHLdv357IyEhatGjB/PnzmTZtmgUjsw5Go5GuXbvy9ttvA9C5c2eOHDnC559/zrhx4ywcXe1JC44JeHl5odPprmutSUtLu65VR4ib+dvf/sbKlSvZtGkTzZo1s3Q4VkWv19OyZUu6du3KzJkz6dixIx999JGlw7K4vXv3kpaWRkREBHZ2dtjZ2bFlyxb+97//YWdnR2lpqaVDtCouLi60b9+ehIQES4diFfz9/a/7khAaGmozk2MkwTEBvV5PREQEGzZsqHR+w4YN9OrVy0JRifpCURSeffZZli9fzu+//05ISIilQ7J6iqJQWFho6TAsbuDAgRw6dIjY2NiKW9euXXn00UeJjY1Fp9NZOkSrUlhYSFxcHP7+/pYOxSr07t37uiUpjh8/TlBQkIUiMi3pojKRadOmMXbsWLp27UpkZCSzZ88mKSmJyZMnWzo0q5CTk8OJEycqfj59+jSxsbF4enoSGBhowcgs75lnnuG7777j559/xs3NraIl0GAw4OTkZOHoLO/ll19m2LBhBAQEkJ2dzZIlS9i8eTNr1661dGgW5+bmdt1YLRcXFxo3bixjuIDp06dz9913ExgYSFpaGm+99RZZWVmMHz/e0qFZheeff55evXrx9ttvM3r0aHbv3s3s2bOZPXu2pUMzDUWYzKeffqoEBQUper1e6dKli7JlyxZLh2Q1Nm3apADX3caPH2/p0CzuRr8XQPnmm28sHZpVeOKJJyr+X3l7eysDBw5U1q9fb+mwrFa/fv2U5557ztJhWIUxY8Yo/v7+ir29vdKkSRPl/vvvV44cOWLpsKzKqlWrlPDwcMXBwUFp27atMnv2bEuHZDKyDo4QQgghbI6MwRFCCCGEzZEERwghhBA2RxIcIYQQQtgcSXCEEEIIYXMkwRFCCCGEzZEERwghhBA2RxIcIYQQQtgcSXCEEEIIYXMkwRFCCCGEzZEERwghhBA2RxIcIYQQQtgcSXCEEEIIYXP+H3NrQJLwwTVEAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = S4D(d_model=1, d_state=64, dropout=0.0).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def f(x):\n",
    "    return torch.sin(x)\n",
    "\n",
    "batch_size = 32\n",
    "seq_len = 100\n",
    "steps = 2000\n",
    "\n",
    "for _ in range(steps):\n",
    "    phi = torch.rand(batch_size, device=device) * 2 * math.pi\n",
    "    t = torch.linspace(0, 2*math.pi, seq_len, device=device)\n",
    "    x = f(t.unsqueeze(0) + phi.unsqueeze(1)).unsqueeze(1)\n",
    "    y = f(t.unsqueeze(0) + phi.unsqueeze(1)).unsqueeze(1)\n",
    "    pred, _ = model(x)\n",
    "    loss = loss_fn(pred, y)\n",
    "    print(loss)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "model.eval()\n",
    "phi_test = 1.0\n",
    "t = torch.linspace(0, 2*math.pi, seq_len)\n",
    "x_test = f(t + phi_test).unsqueeze(0).unsqueeze(0).to(device)\n",
    "y_true = f(t + phi_test)\n",
    "with torch.no_grad():\n",
    "    y_pred, _ = model(x_test)\n",
    "y_pred = y_pred.squeeze().cpu()\n",
    "\n",
    "plt.plot(t.numpy(), y_true.numpy(), label='true')\n",
    "plt.plot(t.numpy(), y_pred.numpy(), label='pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S4D is complexity $\\mathcal{O}(L \\log L)$ where $L$ is the sequence length because this is the complexity of the inverse fast fourier transform used to recover the filter. On the other hand attention is $\\mathcal{O}(L^2)$ for a similar task.\n",
    "\n",
    "S4D approximates a continuous-time state space model. Unlike S4, it is diagonal (D), and the learned parameters are the decay and oscillation of the A matrix in a standard SSM and the readout C of each channel. Then the convolution kernel K's FFT is used to produce the output, along with the inputs to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Lyra(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super().__init__()\n",
    "        self.pgc1 = ProjectedGatedConv(features, hidden_dim=16)\n",
    "        self.pgc2 = ProjectedGatedConv(features, hidden_dim=128)\n",
    "        self.prenorm = RMSNorm(features)\n",
    "        self.s4d = S4D(d_model=features, d_state=64, dropout=0.0, transposed=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pgc1(x)\n",
    "        x = self.pgc2(x)\n",
    "        residual = x\n",
    "        x = self.prenorm(x)\n",
    "        x_s4d, _ = self.s4d(x.transpose(1, 2))\n",
    "        x_s4d = x_s4d.transpose(1, 2)\n",
    "        return x_s4d + residual\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Lyra comprises two core components: the Projected Gated Convolution (PGC) block[43], followed by a state-space layer with depthwise convolution (S4D). In the standard implementation, which consists of approximately 55,000 parameters, Lyra includes two PGC blocks. The first PGC block operates\n",
    "with a hidden dimension of 16, while the second uses a hidden dimension of 128. These are followed\n",
    "by an S4D layer[38], which has a hidden dimension of 64 and is equipped with a residual connection\n",
    "and sequence prenormalization using Root Mean Square Layer Normalization (RMSNorm). The\n",
    "PGC blocks are designed to capture contextualized local dependencies in the input sequence, while the\n",
    "S4D layer parameterizes a long convolution to model long-range dependencies.\"\n",
    "\n",
    "https://arxiv.org/pdf/2503.16351"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyraMini(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super().__init__()\n",
    "        self.pgc1 = ProjectedGatedConv(features, hidden_dim=8)\n",
    "        self.pgc2 = ProjectedGatedConv(features, hidden_dim=16)\n",
    "        self.prenorm = RMSNorm(features)\n",
    "        self.s4d = S4D(d_model=features, d_state=16, dropout=0.0, transposed=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pgc1(x)\n",
    "        x = self.pgc2(x)\n",
    "        residual = x\n",
    "        x = self.prenorm(x)\n",
    "        x_s4d, _ = self.s4d(x.transpose(1, 2))\n",
    "        x_s4d = x_s4d.transpose(1, 2)\n",
    "        return x_s4d + residual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6084, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5591, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.5148, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.4459, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3574, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.3569, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.2198, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1737, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1646, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.1464, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0768, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0378, grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0625, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9224, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9412, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9190, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8634, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.8424, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7731, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7746, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7095, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.7099, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6905, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6393, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6011, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.6029, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5692, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5548, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.5320, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4898, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4720, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4479, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.4280, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3826, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3719, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3593, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3250, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3125, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.3024, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2719, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2750, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2485, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2259, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2211, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1970, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1942, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1771, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1625, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1710, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1628, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1425, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1406, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1266, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1212, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1240, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1200, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1126, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1048, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1068, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0943, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.1062, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0967, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0849, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0866, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0841, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0752, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0771, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0713, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0710, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0722, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0664, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0599, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0627, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0584, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0551, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0571, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0611, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0504, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0475, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0501, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0458, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0486, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0444, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0457, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0434, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0393, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0417, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0338, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0367, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0319, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0317, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0314, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0298, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0310, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0264, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0284, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0261, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0253, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0249, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0230, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0236, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0202, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0207, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0192, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0178, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0188, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0176, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0155, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0166, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0153, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0154, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0137, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0132, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0126, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0131, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0109, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0112, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0127, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0107, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0112, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0108, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0094, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0088, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0090, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0095, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0079, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0076, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0078, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0075, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0066, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0065, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0069, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0067, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0064, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0062, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0058, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0057, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0049, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0053, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0052, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0047, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0048, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0044, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0044, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0040, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0042, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LyraMini(features=100)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def f2(x):\n",
    "    return torch.cos(x) + .5*torch.sin(2*x)\n",
    "\n",
    "batch_size = 32\n",
    "seq_len = 100\n",
    "steps = 300\n",
    "\n",
    "for _ in range(steps):\n",
    "    phi = torch.rand(batch_size, device=device) * 2 * math.pi\n",
    "    t = torch.linspace(0, 2*math.pi, seq_len, device=device)\n",
    "    x = f2(t.unsqueeze(0) + phi.unsqueeze(1)).unsqueeze(1)\n",
    "    y = f2(t.unsqueeze(0) + phi.unsqueeze(1)).unsqueeze(1)\n",
    "    pred = model(x)\n",
    "    loss = loss_fn(pred, y)\n",
    "    print(loss)\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "model.eval()\n",
    "phi_test = 1.0\n",
    "t = torch.linspace(0, 2*math.pi, seq_len)\n",
    "x_test = f2(t + phi_test).unsqueeze(0).unsqueeze(0).to(device)\n",
    "y_true = f2(t + phi_test)\n",
    "with torch.no_grad():\n",
    "    y_pred = model(x_test)\n",
    "y_pred = y_pred.squeeze().cpu()\n",
    "\n",
    "plt.plot(t.numpy(), y_true.numpy(), label='true')\n",
    "plt.plot(t.numpy(), y_pred.numpy(), label='pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "md_sims",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
